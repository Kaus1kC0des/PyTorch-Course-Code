{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3619d604-8dd3-453b-9f23-f5c36e046e13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5745b5-07f5-426e-8866-f17e82ddaa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341a91d6-b779-4920-9366-342efd9f8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "#  y = m*x + c ---> m = weight ; c=bias\n",
    "y = weight*X + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4750c31c-6cd5-4670-887e-b6a7e232223b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0200],\n",
       "        [0.0400],\n",
       "        [0.0600],\n",
       "        [0.0800],\n",
       "        [0.1000],\n",
       "        [0.1200],\n",
       "        [0.1400],\n",
       "        [0.1600],\n",
       "        [0.1800],\n",
       "        [0.2000],\n",
       "        [0.2200],\n",
       "        [0.2400],\n",
       "        [0.2600],\n",
       "        [0.2800],\n",
       "        [0.3000],\n",
       "        [0.3200],\n",
       "        [0.3400],\n",
       "        [0.3600],\n",
       "        [0.3800],\n",
       "        [0.4000],\n",
       "        [0.4200],\n",
       "        [0.4400],\n",
       "        [0.4600],\n",
       "        [0.4800],\n",
       "        [0.5000],\n",
       "        [0.5200],\n",
       "        [0.5400],\n",
       "        [0.5600],\n",
       "        [0.5800],\n",
       "        [0.6000],\n",
       "        [0.6200],\n",
       "        [0.6400],\n",
       "        [0.6600],\n",
       "        [0.6800],\n",
       "        [0.7000],\n",
       "        [0.7200],\n",
       "        [0.7400],\n",
       "        [0.7600],\n",
       "        [0.7800],\n",
       "        [0.8000],\n",
       "        [0.8200],\n",
       "        [0.8400],\n",
       "        [0.8600],\n",
       "        [0.8800],\n",
       "        [0.9000],\n",
       "        [0.9200],\n",
       "        [0.9400],\n",
       "        [0.9600],\n",
       "        [0.9800]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6a93d4-eb4a-47c9-8b44-cf7e4e0d8178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3000],\n",
       "        [0.3140],\n",
       "        [0.3280],\n",
       "        [0.3420],\n",
       "        [0.3560],\n",
       "        [0.3700],\n",
       "        [0.3840],\n",
       "        [0.3980],\n",
       "        [0.4120],\n",
       "        [0.4260],\n",
       "        [0.4400],\n",
       "        [0.4540],\n",
       "        [0.4680],\n",
       "        [0.4820],\n",
       "        [0.4960],\n",
       "        [0.5100],\n",
       "        [0.5240],\n",
       "        [0.5380],\n",
       "        [0.5520],\n",
       "        [0.5660],\n",
       "        [0.5800],\n",
       "        [0.5940],\n",
       "        [0.6080],\n",
       "        [0.6220],\n",
       "        [0.6360],\n",
       "        [0.6500],\n",
       "        [0.6640],\n",
       "        [0.6780],\n",
       "        [0.6920],\n",
       "        [0.7060],\n",
       "        [0.7200],\n",
       "        [0.7340],\n",
       "        [0.7480],\n",
       "        [0.7620],\n",
       "        [0.7760],\n",
       "        [0.7900],\n",
       "        [0.8040],\n",
       "        [0.8180],\n",
       "        [0.8320],\n",
       "        [0.8460],\n",
       "        [0.8600],\n",
       "        [0.8740],\n",
       "        [0.8880],\n",
       "        [0.9020],\n",
       "        [0.9160],\n",
       "        [0.9300],\n",
       "        [0.9440],\n",
       "        [0.9580],\n",
       "        [0.9720],\n",
       "        [0.9860]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4498bff2-7c98-4211-a3d1-00377ead072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = int(0.8 * len(X))\n",
    "X_train = X[:train_set]\n",
    "y_train = y[:train_set]\n",
    "X_test = X[train_set:]\n",
    "y_test = y[train_set:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13767cfb-c32f-42d1-bf13-15faf36ada0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    train_data = X_train,\n",
    "    train_labels = y_train,\n",
    "    test_data = X_test,\n",
    "    test_labels = y_test,\n",
    "    predictions = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plt the predictions out of the data.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,7))\n",
    "\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data,train_labels,c=\"blue\",s=4,label=\"Training Data\")\n",
    "\n",
    "\n",
    "    # Plot the testing data in red\n",
    "    plt.scatter(test_data,test_labels,c=\"red\",s=4,label=\"Testing Data\")\n",
    "\n",
    "    # Plot the predictions if they exists\n",
    "    if predictions is not None:\n",
    "        predictions = predictions.detach().numpy()\n",
    "        plt.scatter(test_data,predictions,c=\"black\", s=4, label=\"Predictions\")\n",
    "    plt.legend(prop={\"size\": 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ca5109-078f-4a35-92c8-88cb8d2d3d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cc509-28f3-4af6-b670-39763d7316af",
   "metadata": {},
   "source": [
    "#### LinearRegresionModel\n",
    "\n",
    "We'll do it manually by inheriting a ```nn.Module``` from the `torch` library and use it to implement linear regression into our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae1cf23-a644-48cb-ac58-3a8a33ea3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weights = nn.Parameter(torch.randn(1,dtype=torch.float,requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.randn(1,dtype=torch.float,requires_grad=True))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (self.weights * x) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563f0b8d-f15c-4086-86f7-cace71774574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = LinearRegressionModel()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432ddcf2-736a-41be-ad55-e03af7df7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    preds = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9197b0f6-945f-4106-966a-65a1df240939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982],\n",
       "        [0.4049],\n",
       "        [0.4116],\n",
       "        [0.4184],\n",
       "        [0.4251],\n",
       "        [0.4318],\n",
       "        [0.4386],\n",
       "        [0.4453],\n",
       "        [0.4520],\n",
       "        [0.4588]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5662c05-df87-469b-ade1-d5bacab0b7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4618],\n",
       "        [0.4691],\n",
       "        [0.4764],\n",
       "        [0.4836],\n",
       "        [0.4909],\n",
       "        [0.4982],\n",
       "        [0.5054],\n",
       "        [0.5127],\n",
       "        [0.5200],\n",
       "        [0.5272]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test - preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd832519-dc4c-4f40-ac8a-5bf94e62ed7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "145e4b7a-a18b-4c5e-a4f2-ccceb9afa208",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b444a6-2d8d-464a-9d12-4cb23d39c9c0",
   "metadata": {},
   "source": [
    "Building a training loop.\n",
    "\n",
    "\n",
    "Things to do:\n",
    "0. Loop through the data\n",
    "1. Set the model in training mode\n",
    "2. Forward pass (This means our data moving through the `forward()` function a.k.a forward propagation)\n",
    "3. Make predictions on data and calculate the loss (compare predictions to ground truth), store it in ``loss``.\n",
    "4. Optimizer zero grad ``optimizer.zero_grad()`` to clear the weights and biases from the memory.\n",
    "5. Loss Backwards (backpropagate through the loss, to calcuate the gradients) ``loss.backward()``.\n",
    "6. Optimiser step (Use the optimizer to travel back through the network and update the parameters) ``optimizer.step()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0a57ea7-379e-46b7-ae52-f754f743538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.31288138031959534 | Testing Loss: 0.4698256850242615\n",
      "Epoch: 10 | Training Loss: 0.1976713240146637 | Testing Loss: 0.33382871747016907\n",
      "Epoch: 20 | Training Loss: 0.08908725529909134 | Testing Loss: 0.20348870754241943\n",
      "Epoch: 30 | Training Loss: 0.053148526698350906 | Testing Loss: 0.12975125014781952\n",
      "Epoch: 40 | Training Loss: 0.04543796554207802 | Testing Loss: 0.09794081002473831\n",
      "Epoch: 50 | Training Loss: 0.04167863354086876 | Testing Loss: 0.08290375024080276\n",
      "Epoch: 60 | Training Loss: 0.03818932920694351 | Testing Loss: 0.07198350876569748\n",
      "Epoch: 70 | Training Loss: 0.03476089984178543 | Testing Loss: 0.06314468383789062\n",
      "Epoch: 80 | Training Loss: 0.03132382780313492 | Testing Loss: 0.05430586263537407\n",
      "Epoch: 90 | Training Loss: 0.02788739837706089 | Testing Loss: 0.046160854399204254\n",
      "Epoch: 100 | Training Loss: 0.024458957836031914 | Testing Loss: 0.0373220220208168\n",
      "Epoch: 110 | Training Loss: 0.021020207554101944 | Testing Loss: 0.028483206406235695\n",
      "Epoch: 120 | Training Loss: 0.01758546568453312 | Testing Loss: 0.021132390946149826\n",
      "Epoch: 130 | Training Loss: 0.014155393466353416 | Testing Loss: 0.015439515933394432\n",
      "Epoch: 140 | Training Loss: 0.010716589167714119 | Testing Loss: 0.012343818321824074\n",
      "Epoch: 150 | Training Loss: 0.0072835334576666355 | Testing Loss: 0.012339085340499878\n",
      "Epoch: 160 | Training Loss: 0.0038517764769494534 | Testing Loss: 0.015704387798905373\n",
      "Epoch: 170 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 180 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 190 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 200 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 210 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 220 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 230 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 240 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 250 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 260 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 270 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 280 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n",
      "Epoch: 290 | Training Loss: 0.008932482451200485 | Testing Loss: 0.01817433163523674\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass the data and store the predictions in y_pred\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(y_pred,y_train)\n",
    "\n",
    "\n",
    "    # Put optimizer into zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backpropagate the losses\n",
    "    loss.backward()\n",
    "\n",
    "    # Step the optimizer to calculate the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Testing loop\n",
    "    with torch.inference_mode():\n",
    "        # Make forward pass on testing data\n",
    "        test_preds = model(y_test)\n",
    "\n",
    "        # Calculate test loss\n",
    "        test_loss = loss_fn(test_preds,y_test)\n",
    "\n",
    "    if epoch%10==0:\n",
    "        print(f\"Epoch: {epoch} | Training Loss: {loss} | Testing Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f741ddb9-85f0-4601-9796-efe1b6303bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87729545-26b7-4ecc-a54b-58073117a49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0085],\n",
       "        [-0.0085],\n",
       "        [-0.0085],\n",
       "        [-0.0085],\n",
       "        [-0.0085],\n",
       "        [-0.0084],\n",
       "        [-0.0084],\n",
       "        [-0.0084],\n",
       "        [-0.0084],\n",
       "        [-0.0084]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test - y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07dd7f1a-b561-45ab-836d-2b6a40ccb135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca53dd3f-f349-443c-ba87-7d8a5dae562f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd705ecd-819a-4019-8d57-80e4fe4539ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a277e45-92b9-4ccf-9ec2-532416ca5ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.2345], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2303], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edcd9471-5664-4992-af28-8c36c43f7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    preds_2 = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75a1e183-ce45-4a82-ae78-0c781d95fbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96e9745e-41c4-46aa-8e47-517d937ff068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421],\n",
       "        [0.4514],\n",
       "        [0.4607],\n",
       "        [0.4700],\n",
       "        [0.4793],\n",
       "        [0.4887],\n",
       "        [0.4980],\n",
       "        [0.5073],\n",
       "        [0.5166],\n",
       "        [0.5259]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(y_test - preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9c8dcb9-19e1-468e-bb8d-611a33adbfad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421],\n",
       "        [0.4514],\n",
       "        [0.4607],\n",
       "        [0.4700],\n",
       "        [0.4793],\n",
       "        [0.4887],\n",
       "        [0.4980],\n",
       "        [0.5073],\n",
       "        [0.5166],\n",
       "        [0.5259]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test - preds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "573644c3-c764-414b-ab78-6490639619f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the loss and optimiser functions\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81585b-d77f-4b90-a271-5386039a9f60",
   "metadata": {},
   "source": [
    "# Writing the training loop for practice\n",
    "\n",
    "0) Set the model in training mode\n",
    "1) Predict for the data with current parameters, perform a forward pass using the ``forward()`` method.\n",
    "2) Calculate the loss for the data\n",
    "3) Set optimizer's data accumulated to 0 using ``optimizer.zero_grad()``\n",
    "4) Backpropagate the loss using `loss.backward()`\n",
    "5) Step the optimizer i.e., set the optimizer to update the parameters of the model using ``optimizer.step()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "385c320c-d8e4-4cd3-b4e9-845149f73d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 || Training Loss: 0.25122663378715515 || Testing Loss: 0.4838607907295227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 || Training Loss: 0.14213839173316956 || Testing Loss: 0.3524583578109741\n",
      "Epoch: 2000 || Training Loss: 0.09166976064443588 || Testing Loss: 0.260356068611145\n",
      "Epoch: 3000 || Training Loss: 0.07439471781253815 || Testing Loss: 0.20573052763938904\n",
      "Epoch: 4000 || Training Loss: 0.06765337288379669 || Testing Loss: 0.1747949868440628\n",
      "Epoch: 5000 || Training Loss: 0.06344598531723022 || Testing Loss: 0.15566988289356232\n",
      "Epoch: 6000 || Training Loss: 0.059939898550510406 || Testing Loss: 0.14445261657238007\n",
      "Epoch: 7000 || Training Loss: 0.05644945055246353 || Testing Loss: 0.1334388703107834\n",
      "Epoch: 8000 || Training Loss: 0.052985675632953644 || Testing Loss: 0.12388703972101212\n",
      "Epoch: 9000 || Training Loss: 0.049549590796232224 || Testing Loss: 0.11585547029972076\n",
      "Epoch: 10000 || Training Loss: 0.04611348360776901 || Testing Loss: 0.10782048851251602\n",
      "Epoch: 11000 || Training Loss: 0.04267736151814461 || Testing Loss: 0.0997854694724083\n",
      "Epoch: 12000 || Training Loss: 0.039241231977939606 || Testing Loss: 0.09174732863903046\n",
      "Epoch: 13000 || Training Loss: 0.03580531105399132 || Testing Loss: 0.0837133377790451\n",
      "Epoch: 14000 || Training Loss: 0.03236943855881691 || Testing Loss: 0.07568272948265076\n",
      "Epoch: 15000 || Training Loss: 0.028933506458997726 || Testing Loss: 0.06764877587556839\n",
      "Epoch: 16000 || Training Loss: 0.02549757994711399 || Testing Loss: 0.05961477756500244\n",
      "Epoch: 17000 || Training Loss: 0.02206164039671421 || Testing Loss: 0.05158077925443649\n",
      "Epoch: 18000 || Training Loss: 0.018625713884830475 || Testing Loss: 0.043546803295612335\n",
      "Epoch: 19000 || Training Loss: 0.015189781785011292 || Testing Loss: 0.03550932928919792\n",
      "Epoch: 20000 || Training Loss: 0.011753858998417854 || Testing Loss: 0.02747536264359951\n",
      "Epoch: 21000 || Training Loss: 0.008317908272147179 || Testing Loss: 0.0194447822868824\n",
      "Epoch: 22000 || Training Loss: 0.004881975706666708 || Testing Loss: 0.011410790495574474\n",
      "Epoch: 23000 || Training Loss: 0.0014460466336458921 || Testing Loss: 0.0033767998684197664\n",
      "Epoch: 24000 || Training Loss: 3.474578261375427e-05 || Testing Loss: 7.107853889465332e-05\n",
      "Epoch: 25000 || Training Loss: 3.474578261375427e-05 || Testing Loss: 7.107853889465332e-05\n",
      "Epoch: 26000 || Training Loss: 3.474578261375427e-05 || Testing Loss: 7.107853889465332e-05\n",
      "Epoch: 27000 || Training Loss: 3.474578261375427e-05 || Testing Loss: 7.107853889465332e-05\n",
      "Epoch: 28000 || Training Loss: 3.474578261375427e-05 || Testing Loss: 7.107853889465332e-05\n",
      "Epoch: 29000 || Training Loss: 3.474578261375427e-05 || Testing Loss: 7.107853889465332e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 30000 \n",
    "\n",
    "test_loss_array = []\n",
    "train_loss_array = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAINING LOOP\n",
    "    # Set in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Make predictions\n",
    "    y_preds = model(X_train)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(y_preds,y_train)\n",
    "    train_loss_array.append(loss)\n",
    "\n",
    "    # Set the optimizer's gradients to 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform GD with the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # TESTING LOOP\n",
    "    with torch.inference_mode():\n",
    "        test_preds = model(X_test)\n",
    "        test_loss = loss_fn(test_preds,y_test)\n",
    "        test_loss_array.append(test_loss)\n",
    "\n",
    "    if epoch%1000 == 0:\n",
    "        print(f\"Epoch: {epoch} || Training Loss: {loss} || Testing Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "320c9ead-27c2-4e5c-8220-ec382381d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c4360c7-5479-4dec-b68a-fa94ec5ad048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4dd9aac-eed5-468d-bb9a-56d117bbc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = loss_fn(final_preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96a292a1-fa3e-4190-8fb4-0b5d89d3350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3592e-05, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7cecc6d-1cf4-4247-9178-e4caef298b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb5aaf0e-5319-4db1-8185-56d645a80eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_array = np.array(torch.Tensor(train_loss_array).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57e66015-5848-4aea-983e-5944af8dab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_array = np.array(torch.Tensor(test_loss_array).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50f554ee-0351-42f5-91ad-4ebcfefdf319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(train_loss_array,c=\"red\", label=\"Training Loss\")\n",
    "plt.plot(test_loss_array,c=\"green\",label=\"Testing Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b21d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"LinearRegressionModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f3fc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(\"LinearRegressionModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f10b400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3000]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bb53eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    loaded_preds = loaded_model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2f0403a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=loaded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c331b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Our own custom linear regression model for the linear regression problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer:torch.optim=None, loss_metric=nn.L1Loss) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the linear regression model.\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim, optional): The optimizer to use for training. If not provided,\n",
    "                the default is SGD with a learning rate of 0.001.\n",
    "            loss_metric (callable, optional): The loss function to use for training. Default is\n",
    "                the mean absolute error (L1Loss).\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(1,dtype=torch.float64,requires_grad=True)\n",
    "        )\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(1,dtype=torch.float64,requires_grad=True)\n",
    "        )\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.SGD(self.parameters(), lr=0.001)\n",
    "        self.loss_fn = loss_metric()\n",
    "\n",
    "    \n",
    "    def forward(self, x:torch.tensor)->torch.tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the linear regression model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): The input features.\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: The predicted output values.\n",
    "        \"\"\"\n",
    "        return (self.weights*x) + self.bias\n",
    "    \n",
    "    \n",
    "    def fit(\n",
    "            self, \n",
    "            X:torch.tensor, \n",
    "            y:torch.tensor, \n",
    "            epochs:int, \n",
    "            validate:bool = False, x_val:torch.tensor = None, y_val:torch.tensor = None\n",
    "            ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train the linear regression model on the given data.\n",
    "\n",
    "        Args:\n",
    "            X (torch.tensor): The input features for training.\n",
    "            y (torch.tensor): The target values for training.\n",
    "            epochs (int): The number of epochs to train for.\n",
    "            validate (bool, optional): Whether to compute validation loss during training. Default is False.\n",
    "            x_val (torch.tensor, optional): The input features for validation. Required if validate is True.\n",
    "            y_val (torch.tensor, optional): The target values for validation. Required if validate is True.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two lists:\n",
    "                - train_loss_array: A list of training losses for each epoch.\n",
    "                - test_loss_array: A list of validation losses for each epoch (if validate is True).\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        train_loss_array = []\n",
    "        test_loss_array = []\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Set the model in training mode\n",
    "            self.train()\n",
    "\n",
    "            # Make a forward pass and predict off the labels\n",
    "            train_preds = self.forward(X)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = self.loss_fn(train_preds, y)\n",
    "\n",
    "            # Zero grad the optimizer\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Step the optimizer\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if validate:\n",
    "                self.eval()\n",
    "                with torch.inference_mode():\n",
    "                    test_preds = self.forward(x_val)\n",
    "                    test_loss = self.loss_fn(test_preds, y_val)\n",
    "                    test_loss_array.append(test_loss)\n",
    "                    train_loss_array.append(loss)\n",
    "                print(f\"Epoch: {epoch+1} \\t|| Train Loss: {loss} \\t|| Test Loss: {test_loss}\")\n",
    "            else:\n",
    "                train_loss_array.append(loss)\n",
    "                print(f\"Epoch: {epoch+1} \\t|| Train Loss: {loss}\")\n",
    "        if validate:\n",
    "            return train_loss_array, test_loss_array\n",
    "        else:\n",
    "            return train_loss_array\n",
    "        \n",
    "\n",
    "\n",
    "    def plot_metrics(self, train_loss_array, test_loss_array=None):\n",
    "        \"\"\"\n",
    "        Plot the loss curve for the training and validation data.\n",
    "\n",
    "        Args:\n",
    "            train_loss_array (list): A list of training losses for each epoch.\n",
    "            test_loss_array (list, optional): A list of validation losses for each epoch. \n",
    "        \"\"\"\n",
    "        # Detach the tensors and convert them to lists\n",
    "        train_loss_array = [loss.detach().numpy() for loss in train_loss_array]\n",
    "        if test_loss_array is not None:\n",
    "            test_loss_array = [loss.detach().numpy() for loss in test_loss_array]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_loss_array, label='Training Loss')\n",
    "        if test_loss_array is not None:\n",
    "            plt.plot(test_loss_array, label='Validation Loss')\n",
    "        plt.title('Loss Metrics')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5affee62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([-1.1720], dtype=torch.float64, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3929], dtype=torch.float64, requires_grad=True)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1000)\n",
    "model = LinearRegressionModel()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4aa64de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t|| Train Loss: 1.422983400224267 \t|| Test Loss: 2.357660907309107\n",
      "Epoch: 2 \t|| Train Loss: 1.4218313002306304 \t|| Test Loss: 2.3563138073126493\n",
      "Epoch: 3 \t|| Train Loss: 1.4206792002369941 \t|| Test Loss: 2.354966707316191\n",
      "Epoch: 4 \t|| Train Loss: 1.4195271002433576 \t|| Test Loss: 2.3536196073197324\n",
      "Epoch: 5 \t|| Train Loss: 1.4183750002497213 \t|| Test Loss: 2.3522725073232738\n",
      "Epoch: 6 \t|| Train Loss: 1.4172229002560848 \t|| Test Loss: 2.350925407326816\n",
      "Epoch: 7 \t|| Train Loss: 1.4160708002624482 \t|| Test Loss: 2.3495783073303578\n",
      "Epoch: 8 \t|| Train Loss: 1.414918700268812 \t|| Test Loss: 2.348231207333899\n",
      "Epoch: 9 \t|| Train Loss: 1.4137666002751756 \t|| Test Loss: 2.3468841073374413\n",
      "Epoch: 10 \t|| Train Loss: 1.4126145002815391 \t|| Test Loss: 2.3455370073409827\n",
      "Epoch: 11 \t|| Train Loss: 1.4114624002879026 \t|| Test Loss: 2.344189907344524\n",
      "Epoch: 12 \t|| Train Loss: 1.4103103002942663 \t|| Test Loss: 2.342842807348066\n",
      "Epoch: 13 \t|| Train Loss: 1.4091582003006298 \t|| Test Loss: 2.341495707351608\n",
      "Epoch: 14 \t|| Train Loss: 1.4080061003069932 \t|| Test Loss: 2.3401486073551494\n",
      "Epoch: 15 \t|| Train Loss: 1.406854000313357 \t|| Test Loss: 2.338801507358691\n",
      "Epoch: 16 \t|| Train Loss: 1.4057019003197204 \t|| Test Loss: 2.337454407362233\n",
      "Epoch: 17 \t|| Train Loss: 1.4045498003260841 \t|| Test Loss: 2.3361073073657748\n",
      "Epoch: 18 \t|| Train Loss: 1.4033977003324476 \t|| Test Loss: 2.3347602073693166\n",
      "Epoch: 19 \t|| Train Loss: 1.4022456003388113 \t|| Test Loss: 2.3334131073728583\n",
      "Epoch: 20 \t|| Train Loss: 1.4010935003451748 \t|| Test Loss: 2.3320660073764\n",
      "Epoch: 21 \t|| Train Loss: 1.3999414003515385 \t|| Test Loss: 2.3307189073799415\n",
      "Epoch: 22 \t|| Train Loss: 1.3987893003579022 \t|| Test Loss: 2.3293718073834833\n",
      "Epoch: 23 \t|| Train Loss: 1.3976372003642656 \t|| Test Loss: 2.328024707387025\n",
      "Epoch: 24 \t|| Train Loss: 1.3964851003706291 \t|| Test Loss: 2.3266776073905673\n",
      "Epoch: 25 \t|| Train Loss: 1.3953330003769926 \t|| Test Loss: 2.3253305073941086\n",
      "Epoch: 26 \t|| Train Loss: 1.3941809003833563 \t|| Test Loss: 2.32398340739765\n",
      "Epoch: 27 \t|| Train Loss: 1.39302880038972 \t|| Test Loss: 2.3226363074011918\n",
      "Epoch: 28 \t|| Train Loss: 1.3918767003960835 \t|| Test Loss: 2.321289207404733\n",
      "Epoch: 29 \t|| Train Loss: 1.390724600402447 \t|| Test Loss: 2.3199421074082758\n",
      "Epoch: 30 \t|| Train Loss: 1.3895725004088106 \t|| Test Loss: 2.318595007411817\n",
      "Epoch: 31 \t|| Train Loss: 1.3884204004151741 \t|| Test Loss: 2.3172479074153585\n",
      "Epoch: 32 \t|| Train Loss: 1.3872683004215376 \t|| Test Loss: 2.3159008074189003\n",
      "Epoch: 33 \t|| Train Loss: 1.3861162004279013 \t|| Test Loss: 2.314553707422442\n",
      "Epoch: 34 \t|| Train Loss: 1.384964100434265 \t|| Test Loss: 2.313206607425984\n",
      "Epoch: 35 \t|| Train Loss: 1.3838120004406282 \t|| Test Loss: 2.311859507429526\n",
      "Epoch: 36 \t|| Train Loss: 1.382659900446992 \t|| Test Loss: 2.310512407433067\n",
      "Epoch: 37 \t|| Train Loss: 1.3815078004533556 \t|| Test Loss: 2.3091653074366087\n",
      "Epoch: 38 \t|| Train Loss: 1.3803557004597191 \t|| Test Loss: 2.3078182074401505\n",
      "Epoch: 39 \t|| Train Loss: 1.379203600466083 \t|| Test Loss: 2.3064711074436923\n",
      "Epoch: 40 \t|| Train Loss: 1.3780515004724463 \t|| Test Loss: 2.305124007447234\n",
      "Epoch: 41 \t|| Train Loss: 1.3768994004788098 \t|| Test Loss: 2.303776907450776\n",
      "Epoch: 42 \t|| Train Loss: 1.3757473004851735 \t|| Test Loss: 2.3024298074543177\n",
      "Epoch: 43 \t|| Train Loss: 1.374595200491537 \t|| Test Loss: 2.3010827074578595\n",
      "Epoch: 44 \t|| Train Loss: 1.3734431004979006 \t|| Test Loss: 2.299735607461401\n",
      "Epoch: 45 \t|| Train Loss: 1.3722910005042643 \t|| Test Loss: 2.298388507464943\n",
      "Epoch: 46 \t|| Train Loss: 1.3711389005106276 \t|| Test Loss: 2.297041407468484\n",
      "Epoch: 47 \t|| Train Loss: 1.3699868005169915 \t|| Test Loss: 2.2956943074720257\n",
      "Epoch: 48 \t|| Train Loss: 1.368834700523355 \t|| Test Loss: 2.294347207475568\n",
      "Epoch: 49 \t|| Train Loss: 1.3676826005297185 \t|| Test Loss: 2.2930001074791093\n",
      "Epoch: 50 \t|| Train Loss: 1.3665305005360822 \t|| Test Loss: 2.291653007482651\n",
      "Epoch: 51 \t|| Train Loss: 1.3653784005424456 \t|| Test Loss: 2.290305907486193\n",
      "Epoch: 52 \t|| Train Loss: 1.3642263005488093 \t|| Test Loss: 2.2889588074897347\n",
      "Epoch: 53 \t|| Train Loss: 1.3630742005551728 \t|| Test Loss: 2.2876117074932765\n",
      "Epoch: 54 \t|| Train Loss: 1.3619221005615363 \t|| Test Loss: 2.286264607496818\n",
      "Epoch: 55 \t|| Train Loss: 1.3607700005679 \t|| Test Loss: 2.2849175075003596\n",
      "Epoch: 56 \t|| Train Loss: 1.3596179005742637 \t|| Test Loss: 2.283570407503901\n",
      "Epoch: 57 \t|| Train Loss: 1.358465800580627 \t|| Test Loss: 2.2822233075074427\n",
      "Epoch: 58 \t|| Train Loss: 1.3573137005869906 \t|| Test Loss: 2.2808762075109845\n",
      "Epoch: 59 \t|| Train Loss: 1.356161600593354 \t|| Test Loss: 2.2795291075145263\n",
      "Epoch: 60 \t|| Train Loss: 1.3550095005997178 \t|| Test Loss: 2.278182007518068\n",
      "Epoch: 61 \t|| Train Loss: 1.3538574006060815 \t|| Test Loss: 2.27683490752161\n",
      "Epoch: 62 \t|| Train Loss: 1.352705300612445 \t|| Test Loss: 2.2754878075251517\n",
      "Epoch: 63 \t|| Train Loss: 1.3515532006188085 \t|| Test Loss: 2.274140707528693\n",
      "Epoch: 64 \t|| Train Loss: 1.350401100625172 \t|| Test Loss: 2.2727936075322352\n",
      "Epoch: 65 \t|| Train Loss: 1.3492490006315356 \t|| Test Loss: 2.271446507535777\n",
      "Epoch: 66 \t|| Train Loss: 1.3480969006378993 \t|| Test Loss: 2.270099407539319\n",
      "Epoch: 67 \t|| Train Loss: 1.346944800644263 \t|| Test Loss: 2.26875230754286\n",
      "Epoch: 68 \t|| Train Loss: 1.3457927006506263 \t|| Test Loss: 2.267405207546402\n",
      "Epoch: 69 \t|| Train Loss: 1.3446406006569902 \t|| Test Loss: 2.2660581075499437\n",
      "Epoch: 70 \t|| Train Loss: 1.3434885006633537 \t|| Test Loss: 2.2647110075534855\n",
      "Epoch: 71 \t|| Train Loss: 1.342336400669717 \t|| Test Loss: 2.2633639075570273\n",
      "Epoch: 72 \t|| Train Loss: 1.3411843006760809 \t|| Test Loss: 2.2620168075605687\n",
      "Epoch: 73 \t|| Train Loss: 1.3400322006824443 \t|| Test Loss: 2.2606697075641105\n",
      "Epoch: 74 \t|| Train Loss: 1.3388801006888078 \t|| Test Loss: 2.259322607567652\n",
      "Epoch: 75 \t|| Train Loss: 1.3377280006951713 \t|| Test Loss: 2.257975507571194\n",
      "Epoch: 76 \t|| Train Loss: 1.336575900701535 \t|| Test Loss: 2.256628407574736\n",
      "Epoch: 77 \t|| Train Loss: 1.3354238007078985 \t|| Test Loss: 2.255281307578277\n",
      "Epoch: 78 \t|| Train Loss: 1.3342717007142622 \t|| Test Loss: 2.253934207581819\n",
      "Epoch: 79 \t|| Train Loss: 1.3331196007206259 \t|| Test Loss: 2.2525871075853607\n",
      "Epoch: 80 \t|| Train Loss: 1.3319675007269893 \t|| Test Loss: 2.2512400075889025\n",
      "Epoch: 81 \t|| Train Loss: 1.330815400733353 \t|| Test Loss: 2.2498929075924443\n",
      "Epoch: 82 \t|| Train Loss: 1.3296633007397163 \t|| Test Loss: 2.2485458075959857\n",
      "Epoch: 83 \t|| Train Loss: 1.32851120074608 \t|| Test Loss: 2.2471987075995274\n",
      "Epoch: 84 \t|| Train Loss: 1.3273591007524435 \t|| Test Loss: 2.2458516076030697\n",
      "Epoch: 85 \t|| Train Loss: 1.3262070007588072 \t|| Test Loss: 2.244504507606611\n",
      "Epoch: 86 \t|| Train Loss: 1.3250549007651706 \t|| Test Loss: 2.243157407610153\n",
      "Epoch: 87 \t|| Train Loss: 1.3239028007715343 \t|| Test Loss: 2.2418103076136946\n",
      "Epoch: 88 \t|| Train Loss: 1.3227507007778978 \t|| Test Loss: 2.2404632076172364\n",
      "Epoch: 89 \t|| Train Loss: 1.3215986007842615 \t|| Test Loss: 2.2391161076207777\n",
      "Epoch: 90 \t|| Train Loss: 1.320446500790625 \t|| Test Loss: 2.23776900762432\n",
      "Epoch: 91 \t|| Train Loss: 1.3192944007969887 \t|| Test Loss: 2.236421907627861\n",
      "Epoch: 92 \t|| Train Loss: 1.3181423008033522 \t|| Test Loss: 2.235074807631403\n",
      "Epoch: 93 \t|| Train Loss: 1.3169902008097156 \t|| Test Loss: 2.233727707634945\n",
      "Epoch: 94 \t|| Train Loss: 1.3158381008160793 \t|| Test Loss: 2.2323806076384867\n",
      "Epoch: 95 \t|| Train Loss: 1.314686000822443 \t|| Test Loss: 2.231033507642028\n",
      "Epoch: 96 \t|| Train Loss: 1.3135339008288065 \t|| Test Loss: 2.22968640764557\n",
      "Epoch: 97 \t|| Train Loss: 1.3123818008351698 \t|| Test Loss: 2.2283393076491116\n",
      "Epoch: 98 \t|| Train Loss: 1.3112297008415337 \t|| Test Loss: 2.2269922076526534\n",
      "Epoch: 99 \t|| Train Loss: 1.310077600847897 \t|| Test Loss: 2.2256451076561943\n",
      "Epoch: 100 \t|| Train Loss: 1.3089255008542608 \t|| Test Loss: 2.2242980076597365\n",
      "Epoch: 101 \t|| Train Loss: 1.3077734008606243 \t|| Test Loss: 2.2229509076632783\n",
      "Epoch: 102 \t|| Train Loss: 1.306621300866988 \t|| Test Loss: 2.2216038076668196\n",
      "Epoch: 103 \t|| Train Loss: 1.3054692008733515 \t|| Test Loss: 2.220256707670362\n",
      "Epoch: 104 \t|| Train Loss: 1.304317100879715 \t|| Test Loss: 2.218909607673903\n",
      "Epoch: 105 \t|| Train Loss: 1.3031650008860785 \t|| Test Loss: 2.217562507677445\n",
      "Epoch: 106 \t|| Train Loss: 1.3020129008924424 \t|| Test Loss: 2.216215407680987\n",
      "Epoch: 107 \t|| Train Loss: 1.3008608008988056 \t|| Test Loss: 2.2148683076845286\n",
      "Epoch: 108 \t|| Train Loss: 1.2997087009051693 \t|| Test Loss: 2.2135212076880704\n",
      "Epoch: 109 \t|| Train Loss: 1.298556600911533 \t|| Test Loss: 2.212174107691612\n",
      "Epoch: 110 \t|| Train Loss: 1.2974045009178963 \t|| Test Loss: 2.210827007695154\n",
      "Epoch: 111 \t|| Train Loss: 1.29625240092426 \t|| Test Loss: 2.2094799076986957\n",
      "Epoch: 112 \t|| Train Loss: 1.2951003009306237 \t|| Test Loss: 2.208132807702237\n",
      "Epoch: 113 \t|| Train Loss: 1.2939482009369871 \t|| Test Loss: 2.206785707705779\n",
      "Epoch: 114 \t|| Train Loss: 1.292796100943351 \t|| Test Loss: 2.205438607709321\n",
      "Epoch: 115 \t|| Train Loss: 1.2916440009497143 \t|| Test Loss: 2.2040915077128624\n",
      "Epoch: 116 \t|| Train Loss: 1.290491900956078 \t|| Test Loss: 2.2027444077164042\n",
      "Epoch: 117 \t|| Train Loss: 1.2893398009624415 \t|| Test Loss: 2.2013973077199456\n",
      "Epoch: 118 \t|| Train Loss: 1.288187700968805 \t|| Test Loss: 2.200050207723488\n",
      "Epoch: 119 \t|| Train Loss: 1.2870356009751684 \t|| Test Loss: 2.1987031077270296\n",
      "Epoch: 120 \t|| Train Loss: 1.285883500981532 \t|| Test Loss: 2.1973560077305705\n",
      "Epoch: 121 \t|| Train Loss: 1.2847314009878956 \t|| Test Loss: 2.1960089077341127\n",
      "Epoch: 122 \t|| Train Loss: 1.2835793009942593 \t|| Test Loss: 2.194661807737654\n",
      "Epoch: 123 \t|| Train Loss: 1.282427201000623 \t|| Test Loss: 2.193314707741196\n",
      "Epoch: 124 \t|| Train Loss: 1.2812751010069863 \t|| Test Loss: 2.1919676077447376\n",
      "Epoch: 125 \t|| Train Loss: 1.2801230010133502 \t|| Test Loss: 2.190620507748279\n",
      "Epoch: 126 \t|| Train Loss: 1.2789709010197137 \t|| Test Loss: 2.189273407751821\n",
      "Epoch: 127 \t|| Train Loss: 1.2778188010260774 \t|| Test Loss: 2.1879263077553626\n",
      "Epoch: 128 \t|| Train Loss: 1.2766667010324408 \t|| Test Loss: 2.1865792077589044\n",
      "Epoch: 129 \t|| Train Loss: 1.2755146010388043 \t|| Test Loss: 2.185232107762446\n",
      "Epoch: 130 \t|| Train Loss: 1.274362501045168 \t|| Test Loss: 2.183885007765988\n",
      "Epoch: 131 \t|| Train Loss: 1.2732104010515315 \t|| Test Loss: 2.1825379077695297\n",
      "Epoch: 132 \t|| Train Loss: 1.2720583010578952 \t|| Test Loss: 2.1811908077730715\n",
      "Epoch: 133 \t|| Train Loss: 1.2709062010642587 \t|| Test Loss: 2.1798437077766133\n",
      "Epoch: 134 \t|| Train Loss: 1.2697541010706224 \t|| Test Loss: 2.178496607780155\n",
      "Epoch: 135 \t|| Train Loss: 1.2686020010769858 \t|| Test Loss: 2.177149507783697\n",
      "Epoch: 136 \t|| Train Loss: 1.2674499010833495 \t|| Test Loss: 2.1758024077872387\n",
      "Epoch: 137 \t|| Train Loss: 1.266297801089713 \t|| Test Loss: 2.1744553077907796\n",
      "Epoch: 138 \t|| Train Loss: 1.2651457010960765 \t|| Test Loss: 2.1731082077943213\n",
      "Epoch: 139 \t|| Train Loss: 1.2639936011024402 \t|| Test Loss: 2.1717611077978636\n",
      "Epoch: 140 \t|| Train Loss: 1.2628415011088037 \t|| Test Loss: 2.170414007801405\n",
      "Epoch: 141 \t|| Train Loss: 1.2616894011151674 \t|| Test Loss: 2.1690669078049467\n",
      "Epoch: 142 \t|| Train Loss: 1.2605373011215308 \t|| Test Loss: 2.1677198078084885\n",
      "Epoch: 143 \t|| Train Loss: 1.2593852011278943 \t|| Test Loss: 2.1663727078120303\n",
      "Epoch: 144 \t|| Train Loss: 1.2582331011342578 \t|| Test Loss: 2.165025607815572\n",
      "Epoch: 145 \t|| Train Loss: 1.2570810011406217 \t|| Test Loss: 2.1636785078191134\n",
      "Epoch: 146 \t|| Train Loss: 1.2559289011469852 \t|| Test Loss: 2.1623314078226556\n",
      "Epoch: 147 \t|| Train Loss: 1.2547768011533487 \t|| Test Loss: 2.1609843078261965\n",
      "Epoch: 148 \t|| Train Loss: 1.2536247011597124 \t|| Test Loss: 2.1596372078297383\n",
      "Epoch: 149 \t|| Train Loss: 1.2524726011660758 \t|| Test Loss: 2.15829010783328\n",
      "Epoch: 150 \t|| Train Loss: 1.2513205011724393 \t|| Test Loss: 2.1569430078368224\n",
      "Epoch: 151 \t|| Train Loss: 1.250168401178803 \t|| Test Loss: 2.1555959078403637\n",
      "Epoch: 152 \t|| Train Loss: 1.2490163011851667 \t|| Test Loss: 2.1542488078439055\n",
      "Epoch: 153 \t|| Train Loss: 1.2478642011915304 \t|| Test Loss: 2.1529017078474473\n",
      "Epoch: 154 \t|| Train Loss: 1.2467121011978937 \t|| Test Loss: 2.151554607850989\n",
      "Epoch: 155 \t|| Train Loss: 1.2455600012042574 \t|| Test Loss: 2.150207507854531\n",
      "Epoch: 156 \t|| Train Loss: 1.2444079012106208 \t|| Test Loss: 2.1488604078580726\n",
      "Epoch: 157 \t|| Train Loss: 1.2432558012169845 \t|| Test Loss: 2.1475133078616144\n",
      "Epoch: 158 \t|| Train Loss: 1.242103701223348 \t|| Test Loss: 2.1461662078651558\n",
      "Epoch: 159 \t|| Train Loss: 1.2409516012297115 \t|| Test Loss: 2.1448191078686976\n",
      "Epoch: 160 \t|| Train Loss: 1.239799501236075 \t|| Test Loss: 2.1434720078722393\n",
      "Epoch: 161 \t|| Train Loss: 1.2386474012424389 \t|| Test Loss: 2.142124907875781\n",
      "Epoch: 162 \t|| Train Loss: 1.2374953012488024 \t|| Test Loss: 2.140777807879323\n",
      "Epoch: 163 \t|| Train Loss: 1.2363432012551656 \t|| Test Loss: 2.1394307078828643\n",
      "Epoch: 164 \t|| Train Loss: 1.2351911012615298 \t|| Test Loss: 2.138083607886406\n",
      "Epoch: 165 \t|| Train Loss: 1.2340390012678928 \t|| Test Loss: 2.1367365078899483\n",
      "Epoch: 166 \t|| Train Loss: 1.2328869012742567 \t|| Test Loss: 2.135389407893489\n",
      "Epoch: 167 \t|| Train Loss: 1.2317348012806202 \t|| Test Loss: 2.1340423078970314\n",
      "Epoch: 168 \t|| Train Loss: 1.2305827012869837 \t|| Test Loss: 2.1326952079005728\n",
      "Epoch: 169 \t|| Train Loss: 1.2294306012933471 \t|| Test Loss: 2.1313481079041146\n",
      "Epoch: 170 \t|| Train Loss: 1.228278501299711 \t|| Test Loss: 2.130001007907656\n",
      "Epoch: 171 \t|| Train Loss: 1.2271264013060743 \t|| Test Loss: 2.128653907911198\n",
      "Epoch: 172 \t|| Train Loss: 1.2259743013124378 \t|| Test Loss: 2.12730680791474\n",
      "Epoch: 173 \t|| Train Loss: 1.2248222013188017 \t|| Test Loss: 2.1259597079182813\n",
      "Epoch: 174 \t|| Train Loss: 1.2236701013251652 \t|| Test Loss: 2.124612607921823\n",
      "Epoch: 175 \t|| Train Loss: 1.2225180013315287 \t|| Test Loss: 2.123265507925365\n",
      "Epoch: 176 \t|| Train Loss: 1.2213659013378924 \t|| Test Loss: 2.1219184079289066\n",
      "Epoch: 177 \t|| Train Loss: 1.2202138013442556 \t|| Test Loss: 2.1205713079324484\n",
      "Epoch: 178 \t|| Train Loss: 1.2190617013506195 \t|| Test Loss: 2.11922420793599\n",
      "Epoch: 179 \t|| Train Loss: 1.217909601356983 \t|| Test Loss: 2.117877107939532\n",
      "Epoch: 180 \t|| Train Loss: 1.2167575013633467 \t|| Test Loss: 2.1165300079430733\n",
      "Epoch: 181 \t|| Train Loss: 1.2156054013697104 \t|| Test Loss: 2.115182907946615\n",
      "Epoch: 182 \t|| Train Loss: 1.2144533013760737 \t|| Test Loss: 2.1138358079501574\n",
      "Epoch: 183 \t|| Train Loss: 1.2133012013824371 \t|| Test Loss: 2.1124887079536983\n",
      "Epoch: 184 \t|| Train Loss: 1.2121491013888008 \t|| Test Loss: 2.1111416079572405\n",
      "Epoch: 185 \t|| Train Loss: 1.2109970013951645 \t|| Test Loss: 2.109794507960782\n",
      "Epoch: 186 \t|| Train Loss: 1.2098449014015282 \t|| Test Loss: 2.1084474079643236\n",
      "Epoch: 187 \t|| Train Loss: 1.2086928014078917 \t|| Test Loss: 2.1071003079678654\n",
      "Epoch: 188 \t|| Train Loss: 1.2075407014142552 \t|| Test Loss: 2.105753207971407\n",
      "Epoch: 189 \t|| Train Loss: 1.2063886014206187 \t|| Test Loss: 2.104406107974949\n",
      "Epoch: 190 \t|| Train Loss: 1.2052365014269824 \t|| Test Loss: 2.1030590079784908\n",
      "Epoch: 191 \t|| Train Loss: 1.2040844014333458 \t|| Test Loss: 2.101711907982032\n",
      "Epoch: 192 \t|| Train Loss: 1.2029323014397098 \t|| Test Loss: 2.100364807985574\n",
      "Epoch: 193 \t|| Train Loss: 1.201780201446073 \t|| Test Loss: 2.0990177079891152\n",
      "Epoch: 194 \t|| Train Loss: 1.2006281014524367 \t|| Test Loss: 2.097670607992657\n",
      "Epoch: 195 \t|| Train Loss: 1.1994760014588004 \t|| Test Loss: 2.096323507996199\n",
      "Epoch: 196 \t|| Train Loss: 1.1983239014651639 \t|| Test Loss: 2.0949764079997406\n",
      "Epoch: 197 \t|| Train Loss: 1.1971718014715276 \t|| Test Loss: 2.0936293080032824\n",
      "Epoch: 198 \t|| Train Loss: 1.196019701477891 \t|| Test Loss: 2.092282208006824\n",
      "Epoch: 199 \t|| Train Loss: 1.1948676014842543 \t|| Test Loss: 2.090935108010366\n",
      "Epoch: 200 \t|| Train Loss: 1.1937155014906182 \t|| Test Loss: 2.0895880080139078\n",
      "Epoch: 201 \t|| Train Loss: 1.1925634014969817 \t|| Test Loss: 2.0882409080174495\n",
      "Epoch: 202 \t|| Train Loss: 1.1914113015033452 \t|| Test Loss: 2.0868938080209913\n",
      "Epoch: 203 \t|| Train Loss: 1.1902592015097089 \t|| Test Loss: 2.085546708024533\n",
      "Epoch: 204 \t|| Train Loss: 1.1891071015160726 \t|| Test Loss: 2.0841996080280745\n",
      "Epoch: 205 \t|| Train Loss: 1.1879550015224363 \t|| Test Loss: 2.0828525080316163\n",
      "Epoch: 206 \t|| Train Loss: 1.1868029015287997 \t|| Test Loss: 2.081505408035158\n",
      "Epoch: 207 \t|| Train Loss: 1.185650801535163 \t|| Test Loss: 2.0801583080387\n",
      "Epoch: 208 \t|| Train Loss: 1.1844987015415265 \t|| Test Loss: 2.0788112080422416\n",
      "Epoch: 209 \t|| Train Loss: 1.1833466015478902 \t|| Test Loss: 2.077464108045783\n",
      "Epoch: 210 \t|| Train Loss: 1.1821945015542539 \t|| Test Loss: 2.076117008049325\n",
      "Epoch: 211 \t|| Train Loss: 1.1810424015606173 \t|| Test Loss: 2.074769908052866\n",
      "Epoch: 212 \t|| Train Loss: 1.179890301566981 \t|| Test Loss: 2.073422808056408\n",
      "Epoch: 213 \t|| Train Loss: 1.1787382015733445 \t|| Test Loss: 2.0720757080599497\n",
      "Epoch: 214 \t|| Train Loss: 1.1775861015797082 \t|| Test Loss: 2.0707286080634915\n",
      "Epoch: 215 \t|| Train Loss: 1.1764340015860717 \t|| Test Loss: 2.069381508067033\n",
      "Epoch: 216 \t|| Train Loss: 1.1752819015924352 \t|| Test Loss: 2.0680344080705746\n",
      "Epoch: 217 \t|| Train Loss: 1.174129801598799 \t|| Test Loss: 2.0666873080741164\n",
      "Epoch: 218 \t|| Train Loss: 1.1729777016051623 \t|| Test Loss: 2.0653402080776586\n",
      "Epoch: 219 \t|| Train Loss: 1.171825601611526 \t|| Test Loss: 2.0639931080812\n",
      "Epoch: 220 \t|| Train Loss: 1.1706735016178895 \t|| Test Loss: 2.0626460080847417\n",
      "Epoch: 221 \t|| Train Loss: 1.1695214016242532 \t|| Test Loss: 2.0612989080882835\n",
      "Epoch: 222 \t|| Train Loss: 1.1683693016306167 \t|| Test Loss: 2.0599518080918253\n",
      "Epoch: 223 \t|| Train Loss: 1.1672172016369804 \t|| Test Loss: 2.058604708095367\n",
      "Epoch: 224 \t|| Train Loss: 1.1660651016433439 \t|| Test Loss: 2.057257608098909\n",
      "Epoch: 225 \t|| Train Loss: 1.1649130016497076 \t|| Test Loss: 2.0559105081024507\n",
      "Epoch: 226 \t|| Train Loss: 1.163760901656071 \t|| Test Loss: 2.054563408105992\n",
      "Epoch: 227 \t|| Train Loss: 1.1626088016624347 \t|| Test Loss: 2.0532163081095343\n",
      "Epoch: 228 \t|| Train Loss: 1.1614567016687982 \t|| Test Loss: 2.051869208113075\n",
      "Epoch: 229 \t|| Train Loss: 1.1603046016751617 \t|| Test Loss: 2.0505221081166174\n",
      "Epoch: 230 \t|| Train Loss: 1.1591525016815254 \t|| Test Loss: 2.049175008120159\n",
      "Epoch: 231 \t|| Train Loss: 1.1580004016878889 \t|| Test Loss: 2.047827908123701\n",
      "Epoch: 232 \t|| Train Loss: 1.1568483016942523 \t|| Test Loss: 2.0464808081272423\n",
      "Epoch: 233 \t|| Train Loss: 1.155696201700616 \t|| Test Loss: 2.0451337081307837\n",
      "Epoch: 234 \t|| Train Loss: 1.1545441017069795 \t|| Test Loss: 2.043786608134326\n",
      "Epoch: 235 \t|| Train Loss: 1.153392001713343 \t|| Test Loss: 2.0424395081378677\n",
      "Epoch: 236 \t|| Train Loss: 1.1522399017197067 \t|| Test Loss: 2.0410924081414095\n",
      "Epoch: 237 \t|| Train Loss: 1.1510878017260704 \t|| Test Loss: 2.039745308144951\n",
      "Epoch: 238 \t|| Train Loss: 1.1499357017324339 \t|| Test Loss: 2.038398208148492\n",
      "Epoch: 239 \t|| Train Loss: 1.1487836017387976 \t|| Test Loss: 2.037051108152034\n",
      "Epoch: 240 \t|| Train Loss: 1.147631501745161 \t|| Test Loss: 2.035704008155576\n",
      "Epoch: 241 \t|| Train Loss: 1.1464794017515245 \t|| Test Loss: 2.0343569081591175\n",
      "Epoch: 242 \t|| Train Loss: 1.1453273017578882 \t|| Test Loss: 2.0330098081626593\n",
      "Epoch: 243 \t|| Train Loss: 1.1441752017642517 \t|| Test Loss: 2.031662708166201\n",
      "Epoch: 244 \t|| Train Loss: 1.1430231017706152 \t|| Test Loss: 2.030315608169743\n",
      "Epoch: 245 \t|| Train Loss: 1.141871001776979 \t|| Test Loss: 2.0289685081732847\n",
      "Epoch: 246 \t|| Train Loss: 1.1407189017833426 \t|| Test Loss: 2.0276214081768265\n",
      "Epoch: 247 \t|| Train Loss: 1.139566801789706 \t|| Test Loss: 2.026274308180368\n",
      "Epoch: 248 \t|| Train Loss: 1.1384147017960697 \t|| Test Loss: 2.02492720818391\n",
      "Epoch: 249 \t|| Train Loss: 1.1372626018024332 \t|| Test Loss: 2.0235801081874514\n",
      "Epoch: 250 \t|| Train Loss: 1.136110501808797 \t|| Test Loss: 2.022233008190993\n",
      "Epoch: 251 \t|| Train Loss: 1.1349584018151604 \t|| Test Loss: 2.0208859081945354\n",
      "Epoch: 252 \t|| Train Loss: 1.1338063018215239 \t|| Test Loss: 2.0195388081980767\n",
      "Epoch: 253 \t|| Train Loss: 1.1326542018278876 \t|| Test Loss: 2.0181917082016185\n",
      "Epoch: 254 \t|| Train Loss: 1.131502101834251 \t|| Test Loss: 2.01684460820516\n",
      "Epoch: 255 \t|| Train Loss: 1.1303500018406147 \t|| Test Loss: 2.0154975082087017\n",
      "Epoch: 256 \t|| Train Loss: 1.1291979018469782 \t|| Test Loss: 2.014150408212243\n",
      "Epoch: 257 \t|| Train Loss: 1.1280458018533417 \t|| Test Loss: 2.0128033082157852\n",
      "Epoch: 258 \t|| Train Loss: 1.1268937018597054 \t|| Test Loss: 2.0114562082193266\n",
      "Epoch: 259 \t|| Train Loss: 1.125741601866069 \t|| Test Loss: 2.0101091082228684\n",
      "Epoch: 260 \t|| Train Loss: 1.1245895018724326 \t|| Test Loss: 2.00876200822641\n",
      "Epoch: 261 \t|| Train Loss: 1.1234374018787963 \t|| Test Loss: 2.007414908229952\n",
      "Epoch: 262 \t|| Train Loss: 1.1222853018851597 \t|| Test Loss: 2.0060678082334937\n",
      "Epoch: 263 \t|| Train Loss: 1.1211332018915232 \t|| Test Loss: 2.0047207082370355\n",
      "Epoch: 264 \t|| Train Loss: 1.1199811018978867 \t|| Test Loss: 2.0033736082405773\n",
      "Epoch: 265 \t|| Train Loss: 1.1188290019042504 \t|| Test Loss: 2.0020265082441187\n",
      "Epoch: 266 \t|| Train Loss: 1.1176769019106139 \t|| Test Loss: 2.0006794082476604\n",
      "Epoch: 267 \t|| Train Loss: 1.1165248019169778 \t|| Test Loss: 1.999332308251202\n",
      "Epoch: 268 \t|| Train Loss: 1.115372701923341 \t|| Test Loss: 1.9979852082547442\n",
      "Epoch: 269 \t|| Train Loss: 1.114220601929705 \t|| Test Loss: 1.9966381082582856\n",
      "Epoch: 270 \t|| Train Loss: 1.1130685019360684 \t|| Test Loss: 1.9952910082618274\n",
      "Epoch: 271 \t|| Train Loss: 1.1119164019424317 \t|| Test Loss: 1.9939439082653692\n",
      "Epoch: 272 \t|| Train Loss: 1.1107643019487956 \t|| Test Loss: 1.992596808268911\n",
      "Epoch: 273 \t|| Train Loss: 1.109612201955159 \t|| Test Loss: 1.9912497082724525\n",
      "Epoch: 274 \t|| Train Loss: 1.1084601019615223 \t|| Test Loss: 1.9899026082759939\n",
      "Epoch: 275 \t|| Train Loss: 1.107308001967886 \t|| Test Loss: 1.9885555082795356\n",
      "Epoch: 276 \t|| Train Loss: 1.1061559019742497 \t|| Test Loss: 1.9872084082830774\n",
      "Epoch: 277 \t|| Train Loss: 1.1050038019806134 \t|| Test Loss: 1.9858613082866192\n",
      "Epoch: 278 \t|| Train Loss: 1.1038517019869771 \t|| Test Loss: 1.984514208290161\n",
      "Epoch: 279 \t|| Train Loss: 1.1026996019933404 \t|| Test Loss: 1.9831671082937024\n",
      "Epoch: 280 \t|| Train Loss: 1.1015475019997039 \t|| Test Loss: 1.9818200082972441\n",
      "Epoch: 281 \t|| Train Loss: 1.1003954020060678 \t|| Test Loss: 1.980472908300786\n",
      "Epoch: 282 \t|| Train Loss: 1.0992433020124313 \t|| Test Loss: 1.9791258083043277\n",
      "Epoch: 283 \t|| Train Loss: 1.0980912020187947 \t|| Test Loss: 1.9777787083078695\n",
      "Epoch: 284 \t|| Train Loss: 1.0969391020251584 \t|| Test Loss: 1.9764316083114113\n",
      "Epoch: 285 \t|| Train Loss: 1.095787002031522 \t|| Test Loss: 1.9750845083149529\n",
      "Epoch: 286 \t|| Train Loss: 1.0946349020378854 \t|| Test Loss: 1.9737374083184946\n",
      "Epoch: 287 \t|| Train Loss: 1.093482802044249 \t|| Test Loss: 1.9723903083220364\n",
      "Epoch: 288 \t|| Train Loss: 1.0923307020506126 \t|| Test Loss: 1.9710432083255782\n",
      "Epoch: 289 \t|| Train Loss: 1.091178602056976 \t|| Test Loss: 1.96969610832912\n",
      "Epoch: 290 \t|| Train Loss: 1.0900265020633397 \t|| Test Loss: 1.9683490083326614\n",
      "Epoch: 291 \t|| Train Loss: 1.0888744020697032 \t|| Test Loss: 1.9670019083362031\n",
      "Epoch: 292 \t|| Train Loss: 1.087722302076067 \t|| Test Loss: 1.965654808339745\n",
      "Epoch: 293 \t|| Train Loss: 1.0865702020824304 \t|| Test Loss: 1.9643077083432867\n",
      "Epoch: 294 \t|| Train Loss: 1.085418102088794 \t|| Test Loss: 1.9629606083468285\n",
      "Epoch: 295 \t|| Train Loss: 1.0842660020951578 \t|| Test Loss: 1.96161350835037\n",
      "Epoch: 296 \t|| Train Loss: 1.083113902101521 \t|| Test Loss: 1.9602664083539119\n",
      "Epoch: 297 \t|| Train Loss: 1.0819618021078847 \t|| Test Loss: 1.9589193083574536\n",
      "Epoch: 298 \t|| Train Loss: 1.0808097021142484 \t|| Test Loss: 1.9575722083609954\n",
      "Epoch: 299 \t|| Train Loss: 1.079657602120612 \t|| Test Loss: 1.9562251083645368\n",
      "Epoch: 300 \t|| Train Loss: 1.0785055021269756 \t|| Test Loss: 1.9548780083680786\n",
      "Epoch: 301 \t|| Train Loss: 1.077353402133339 \t|| Test Loss: 1.9535309083716204\n",
      "Epoch: 302 \t|| Train Loss: 1.0762013021397028 \t|| Test Loss: 1.9521838083751617\n",
      "Epoch: 303 \t|| Train Loss: 1.0750492021460663 \t|| Test Loss: 1.950836708378704\n",
      "Epoch: 304 \t|| Train Loss: 1.0738971021524297 \t|| Test Loss: 1.9494896083822453\n",
      "Epoch: 305 \t|| Train Loss: 1.0727450021587934 \t|| Test Loss: 1.948142508385787\n",
      "Epoch: 306 \t|| Train Loss: 1.071592902165157 \t|| Test Loss: 1.9467954083893289\n",
      "Epoch: 307 \t|| Train Loss: 1.0704408021715204 \t|| Test Loss: 1.9454483083928706\n",
      "Epoch: 308 \t|| Train Loss: 1.069288702177884 \t|| Test Loss: 1.9441012083964124\n",
      "Epoch: 309 \t|| Train Loss: 1.0681366021842478 \t|| Test Loss: 1.942754108399954\n",
      "Epoch: 310 \t|| Train Loss: 1.0669845021906113 \t|| Test Loss: 1.9414070084034958\n",
      "Epoch: 311 \t|| Train Loss: 1.065832402196975 \t|| Test Loss: 1.9400599084070371\n",
      "Epoch: 312 \t|| Train Loss: 1.0646803022033384 \t|| Test Loss: 1.938712808410579\n",
      "Epoch: 313 \t|| Train Loss: 1.063528202209702 \t|| Test Loss: 1.9373657084141207\n",
      "Epoch: 314 \t|| Train Loss: 1.0623761022160658 \t|| Test Loss: 1.9360186084176625\n",
      "Epoch: 315 \t|| Train Loss: 1.061224002222429 \t|| Test Loss: 1.9346715084212043\n",
      "Epoch: 316 \t|| Train Loss: 1.0600719022287926 \t|| Test Loss: 1.933324408424746\n",
      "Epoch: 317 \t|| Train Loss: 1.058919802235156 \t|| Test Loss: 1.9319773084282876\n",
      "Epoch: 318 \t|| Train Loss: 1.0577677022415197 \t|| Test Loss: 1.9306302084318294\n",
      "Epoch: 319 \t|| Train Loss: 1.0566156022478834 \t|| Test Loss: 1.9292831084353714\n",
      "Epoch: 320 \t|| Train Loss: 1.0554635022542471 \t|| Test Loss: 1.9279360084389126\n",
      "Epoch: 321 \t|| Train Loss: 1.0543114022606104 \t|| Test Loss: 1.9265889084424543\n",
      "Epoch: 322 \t|| Train Loss: 1.0531593022669743 \t|| Test Loss: 1.9252418084459966\n",
      "Epoch: 323 \t|| Train Loss: 1.0520072022733378 \t|| Test Loss: 1.923894708449538\n",
      "Epoch: 324 \t|| Train Loss: 1.0508551022797012 \t|| Test Loss: 1.9225476084530797\n",
      "Epoch: 325 \t|| Train Loss: 1.049703002286065 \t|| Test Loss: 1.9212005084566215\n",
      "Epoch: 326 \t|| Train Loss: 1.0485509022924284 \t|| Test Loss: 1.9198534084601633\n",
      "Epoch: 327 \t|| Train Loss: 1.047398802298792 \t|| Test Loss: 1.9185063084637046\n",
      "Epoch: 328 \t|| Train Loss: 1.0462467023051556 \t|| Test Loss: 1.9171592084672464\n",
      "Epoch: 329 \t|| Train Loss: 1.045094602311519 \t|| Test Loss: 1.9158121084707882\n",
      "Epoch: 330 \t|| Train Loss: 1.0439425023178828 \t|| Test Loss: 1.9144650084743298\n",
      "Epoch: 331 \t|| Train Loss: 1.0427904023242462 \t|| Test Loss: 1.9131179084778716\n",
      "Epoch: 332 \t|| Train Loss: 1.0416383023306097 \t|| Test Loss: 1.9117708084814133\n",
      "Epoch: 333 \t|| Train Loss: 1.0404862023369736 \t|| Test Loss: 1.9104237084849551\n",
      "Epoch: 334 \t|| Train Loss: 1.0393341023433371 \t|| Test Loss: 1.909076608488497\n",
      "Epoch: 335 \t|| Train Loss: 1.0381820023497004 \t|| Test Loss: 1.9077295084920387\n",
      "Epoch: 336 \t|| Train Loss: 1.037029902356064 \t|| Test Loss: 1.90638240849558\n",
      "Epoch: 337 \t|| Train Loss: 1.0358778023624278 \t|| Test Loss: 1.9050353084991216\n",
      "Epoch: 338 \t|| Train Loss: 1.0347257023687915 \t|| Test Loss: 1.9036882085026636\n",
      "Epoch: 339 \t|| Train Loss: 1.033573602375155 \t|| Test Loss: 1.9023411085062054\n",
      "Epoch: 340 \t|| Train Loss: 1.0324215023815184 \t|| Test Loss: 1.900994008509747\n",
      "Epoch: 341 \t|| Train Loss: 1.031269402387882 \t|| Test Loss: 1.8996469085132888\n",
      "Epoch: 342 \t|| Train Loss: 1.0301173023942458 \t|| Test Loss: 1.8982998085168306\n",
      "Epoch: 343 \t|| Train Loss: 1.028965202400609 \t|| Test Loss: 1.8969527085203723\n",
      "Epoch: 344 \t|| Train Loss: 1.0278131024069728 \t|| Test Loss: 1.8956056085239141\n",
      "Epoch: 345 \t|| Train Loss: 1.0266610024133365 \t|| Test Loss: 1.8942585085274555\n",
      "Epoch: 346 \t|| Train Loss: 1.0255089024197 \t|| Test Loss: 1.8929114085309973\n",
      "Epoch: 347 \t|| Train Loss: 1.0243568024260634 \t|| Test Loss: 1.8915643085345386\n",
      "Epoch: 348 \t|| Train Loss: 1.0232047024324271 \t|| Test Loss: 1.8902172085380804\n",
      "Epoch: 349 \t|| Train Loss: 1.0220526024387904 \t|| Test Loss: 1.8888701085416222\n",
      "Epoch: 350 \t|| Train Loss: 1.0209005024451543 \t|| Test Loss: 1.887523008545164\n",
      "Epoch: 351 \t|| Train Loss: 1.0197484024515178 \t|| Test Loss: 1.8861759085487058\n",
      "Epoch: 352 \t|| Train Loss: 1.0185963024578815 \t|| Test Loss: 1.8848288085522475\n",
      "Epoch: 353 \t|| Train Loss: 1.017444202464245 \t|| Test Loss: 1.8834817085557893\n",
      "Epoch: 354 \t|| Train Loss: 1.0162921024706084 \t|| Test Loss: 1.8821346085593311\n",
      "Epoch: 355 \t|| Train Loss: 1.0151400024769721 \t|| Test Loss: 1.8807875085628727\n",
      "Epoch: 356 \t|| Train Loss: 1.0139879024833356 \t|| Test Loss: 1.8794404085664145\n",
      "Epoch: 357 \t|| Train Loss: 1.012835802489699 \t|| Test Loss: 1.8780933085699563\n",
      "Epoch: 358 \t|| Train Loss: 1.0116837024960628 \t|| Test Loss: 1.8767462085734976\n",
      "Epoch: 359 \t|| Train Loss: 1.0105316025024265 \t|| Test Loss: 1.8753991085770394\n",
      "Epoch: 360 \t|| Train Loss: 1.00937950250879 \t|| Test Loss: 1.8740520085805812\n",
      "Epoch: 361 \t|| Train Loss: 1.0082274025151534 \t|| Test Loss: 1.872704908584123\n",
      "Epoch: 362 \t|| Train Loss: 1.0070753025215171 \t|| Test Loss: 1.8713578085876645\n",
      "Epoch: 363 \t|| Train Loss: 1.0059232025278806 \t|| Test Loss: 1.8700107085912065\n",
      "Epoch: 364 \t|| Train Loss: 1.0047711025342443 \t|| Test Loss: 1.8686636085947481\n",
      "Epoch: 365 \t|| Train Loss: 1.0036190025406078 \t|| Test Loss: 1.8673165085982895\n",
      "Epoch: 366 \t|| Train Loss: 1.0024669025469712 \t|| Test Loss: 1.8659694086018317\n",
      "Epoch: 367 \t|| Train Loss: 1.0013148025533352 \t|| Test Loss: 1.8646223086053735\n",
      "Epoch: 368 \t|| Train Loss: 1.0001627025596984 \t|| Test Loss: 1.8632752086089153\n",
      "Epoch: 369 \t|| Train Loss: 0.9990106025660621 \t|| Test Loss: 1.8619281086124566\n",
      "Epoch: 370 \t|| Train Loss: 0.9978585025724257 \t|| Test Loss: 1.8605810086159984\n",
      "Epoch: 371 \t|| Train Loss: 0.9967064025787893 \t|| Test Loss: 1.8592339086195397\n",
      "Epoch: 372 \t|| Train Loss: 0.9955543025851528 \t|| Test Loss: 1.857886808623082\n",
      "Epoch: 373 \t|| Train Loss: 0.9944022025915163 \t|| Test Loss: 1.8565397086266233\n",
      "Epoch: 374 \t|| Train Loss: 0.9932501025978799 \t|| Test Loss: 1.855192608630165\n",
      "Epoch: 375 \t|| Train Loss: 0.9920980026042436 \t|| Test Loss: 1.853845508633707\n",
      "Epoch: 376 \t|| Train Loss: 0.9909459026106072 \t|| Test Loss: 1.8524984086372485\n",
      "Epoch: 377 \t|| Train Loss: 0.9897938026169706 \t|| Test Loss: 1.8511513086407902\n",
      "Epoch: 378 \t|| Train Loss: 0.9886417026233343 \t|| Test Loss: 1.849804208644332\n",
      "Epoch: 379 \t|| Train Loss: 0.9874896026296976 \t|| Test Loss: 1.8484571086478738\n",
      "Epoch: 380 \t|| Train Loss: 0.9863375026360615 \t|| Test Loss: 1.8471100086514152\n",
      "Epoch: 381 \t|| Train Loss: 0.9851854026424249 \t|| Test Loss: 1.845762908654957\n",
      "Epoch: 382 \t|| Train Loss: 0.9840333026487886 \t|| Test Loss: 1.8444158086584987\n",
      "Epoch: 383 \t|| Train Loss: 0.9828812026551519 \t|| Test Loss: 1.8430687086620405\n",
      "Epoch: 384 \t|| Train Loss: 0.9817291026615157 \t|| Test Loss: 1.841721608665582\n",
      "Epoch: 385 \t|| Train Loss: 0.9805770026678793 \t|| Test Loss: 1.840374508669124\n",
      "Epoch: 386 \t|| Train Loss: 0.9794249026742428 \t|| Test Loss: 1.8390274086726657\n",
      "Epoch: 387 \t|| Train Loss: 0.9782728026806066 \t|| Test Loss: 1.8376803086762075\n",
      "Epoch: 388 \t|| Train Loss: 0.9771207026869699 \t|| Test Loss: 1.8363332086797493\n",
      "Epoch: 389 \t|| Train Loss: 0.9759686026933334 \t|| Test Loss: 1.834986108683291\n",
      "Epoch: 390 \t|| Train Loss: 0.9748165026996972 \t|| Test Loss: 1.8336390086868324\n",
      "Epoch: 391 \t|| Train Loss: 0.9736644027060606 \t|| Test Loss: 1.8322919086903742\n",
      "Epoch: 392 \t|| Train Loss: 0.9725123027124244 \t|| Test Loss: 1.830944808693916\n",
      "Epoch: 393 \t|| Train Loss: 0.9713602027187879 \t|| Test Loss: 1.8295977086974577\n",
      "Epoch: 394 \t|| Train Loss: 0.9702081027251515 \t|| Test Loss: 1.828250608700999\n",
      "Epoch: 395 \t|| Train Loss: 0.9690560027315149 \t|| Test Loss: 1.8269035087045409\n",
      "Epoch: 396 \t|| Train Loss: 0.9679039027378786 \t|| Test Loss: 1.8255564087080827\n",
      "Epoch: 397 \t|| Train Loss: 0.9667518027442423 \t|| Test Loss: 1.8242093087116245\n",
      "Epoch: 398 \t|| Train Loss: 0.9655997027506057 \t|| Test Loss: 1.822862208715166\n",
      "Epoch: 399 \t|| Train Loss: 0.9644476027569692 \t|| Test Loss: 1.8215151087187078\n",
      "Epoch: 400 \t|| Train Loss: 0.963295502763333 \t|| Test Loss: 1.8201680087222492\n",
      "Epoch: 401 \t|| Train Loss: 0.9621434027696963 \t|| Test Loss: 1.8188209087257914\n",
      "Epoch: 402 \t|| Train Loss: 0.9609913027760602 \t|| Test Loss: 1.8174738087293327\n",
      "Epoch: 403 \t|| Train Loss: 0.9598392027824236 \t|| Test Loss: 1.816126708732875\n",
      "Epoch: 404 \t|| Train Loss: 0.958687102788787 \t|| Test Loss: 1.8147796087364163\n",
      "Epoch: 405 \t|| Train Loss: 0.9575350027951506 \t|| Test Loss: 1.813432508739958\n",
      "Epoch: 406 \t|| Train Loss: 0.9563829028015144 \t|| Test Loss: 1.8120854087434999\n",
      "Epoch: 407 \t|| Train Loss: 0.9552308028078779 \t|| Test Loss: 1.8107383087470417\n",
      "Epoch: 408 \t|| Train Loss: 0.9540787028142415 \t|| Test Loss: 1.8093912087505832\n",
      "Epoch: 409 \t|| Train Loss: 0.952926602820605 \t|| Test Loss: 1.808044108754125\n",
      "Epoch: 410 \t|| Train Loss: 0.9517745028269686 \t|| Test Loss: 1.8066970087576668\n",
      "Epoch: 411 \t|| Train Loss: 0.9506224028333323 \t|| Test Loss: 1.8053499087612086\n",
      "Epoch: 412 \t|| Train Loss: 0.9494703028396957 \t|| Test Loss: 1.80400280876475\n",
      "Epoch: 413 \t|| Train Loss: 0.9483182028460593 \t|| Test Loss: 1.8026557087682917\n",
      "Epoch: 414 \t|| Train Loss: 0.9471661028524228 \t|| Test Loss: 1.8013086087718335\n",
      "Epoch: 415 \t|| Train Loss: 0.9460140028587866 \t|| Test Loss: 1.7999615087753753\n",
      "Epoch: 416 \t|| Train Loss: 0.9448619028651501 \t|| Test Loss: 1.798614408778917\n",
      "Epoch: 417 \t|| Train Loss: 0.9437098028715136 \t|| Test Loss: 1.7972673087824589\n",
      "Epoch: 418 \t|| Train Loss: 0.9425577028778772 \t|| Test Loss: 1.7959202087860002\n",
      "Epoch: 419 \t|| Train Loss: 0.9414056028842408 \t|| Test Loss: 1.794573108789542\n",
      "Epoch: 420 \t|| Train Loss: 0.9402535028906044 \t|| Test Loss: 1.7932260087930838\n",
      "Epoch: 421 \t|| Train Loss: 0.9391014028969679 \t|| Test Loss: 1.7918789087966254\n",
      "Epoch: 422 \t|| Train Loss: 0.9379493029033314 \t|| Test Loss: 1.7905318088001672\n",
      "Epoch: 423 \t|| Train Loss: 0.936797202909695 \t|| Test Loss: 1.789184708803709\n",
      "Epoch: 424 \t|| Train Loss: 0.9356451029160586 \t|| Test Loss: 1.7878376088072507\n",
      "Epoch: 425 \t|| Train Loss: 0.9344930029224223 \t|| Test Loss: 1.786490508810792\n",
      "Epoch: 426 \t|| Train Loss: 0.9333409029287857 \t|| Test Loss: 1.7851434088143343\n",
      "Epoch: 427 \t|| Train Loss: 0.9321888029351495 \t|| Test Loss: 1.7837963088178757\n",
      "Epoch: 428 \t|| Train Loss: 0.931036702941513 \t|| Test Loss: 1.7824492088214174\n",
      "Epoch: 429 \t|| Train Loss: 0.9298846029478766 \t|| Test Loss: 1.7811021088249592\n",
      "Epoch: 430 \t|| Train Loss: 0.9287325029542401 \t|| Test Loss: 1.779755008828501\n",
      "Epoch: 431 \t|| Train Loss: 0.9275804029606036 \t|| Test Loss: 1.7784079088320428\n",
      "Epoch: 432 \t|| Train Loss: 0.9264283029669672 \t|| Test Loss: 1.7770608088355844\n",
      "Epoch: 433 \t|| Train Loss: 0.9252762029733308 \t|| Test Loss: 1.7757137088391262\n",
      "Epoch: 434 \t|| Train Loss: 0.9241241029796944 \t|| Test Loss: 1.7743666088426675\n",
      "Epoch: 435 \t|| Train Loss: 0.922972002986058 \t|| Test Loss: 1.7730195088462097\n",
      "Epoch: 436 \t|| Train Loss: 0.9218199029924217 \t|| Test Loss: 1.771672408849751\n",
      "Epoch: 437 \t|| Train Loss: 0.9206678029987853 \t|| Test Loss: 1.7703253088532929\n",
      "Epoch: 438 \t|| Train Loss: 0.9195157030051486 \t|| Test Loss: 1.7689782088568347\n",
      "Epoch: 439 \t|| Train Loss: 0.9183636030115123 \t|| Test Loss: 1.767631108860376\n",
      "Epoch: 440 \t|| Train Loss: 0.9172115030178759 \t|| Test Loss: 1.7662840088639182\n",
      "Epoch: 441 \t|| Train Loss: 0.9160594030242393 \t|| Test Loss: 1.7649369088674596\n",
      "Epoch: 442 \t|| Train Loss: 0.9149073030306031 \t|| Test Loss: 1.7635898088710014\n",
      "Epoch: 443 \t|| Train Loss: 0.9137552030369666 \t|| Test Loss: 1.762242708874543\n",
      "Epoch: 444 \t|| Train Loss: 0.9126031030433299 \t|| Test Loss: 1.7608956088780847\n",
      "Epoch: 445 \t|| Train Loss: 0.9114510030496934 \t|| Test Loss: 1.759548508881626\n",
      "Epoch: 446 \t|| Train Loss: 0.910298903056057 \t|| Test Loss: 1.7582014088851678\n",
      "Epoch: 447 \t|| Train Loss: 0.9091468030624206 \t|| Test Loss: 1.7568543088887094\n",
      "Epoch: 448 \t|| Train Loss: 0.907994703068784 \t|| Test Loss: 1.7555072088922512\n",
      "Epoch: 449 \t|| Train Loss: 0.9068426030751479 \t|| Test Loss: 1.7541601088957925\n",
      "Epoch: 450 \t|| Train Loss: 0.9056905030815112 \t|| Test Loss: 1.7528130088993343\n",
      "Epoch: 451 \t|| Train Loss: 0.9045384030878747 \t|| Test Loss: 1.7514659089028757\n",
      "Epoch: 452 \t|| Train Loss: 0.9033863030942382 \t|| Test Loss: 1.7501188089064172\n",
      "Epoch: 453 \t|| Train Loss: 0.9022342031006018 \t|| Test Loss: 1.748771708909959\n",
      "Epoch: 454 \t|| Train Loss: 0.9010821031069653 \t|| Test Loss: 1.7474246089135008\n",
      "Epoch: 455 \t|| Train Loss: 0.8999300031133288 \t|| Test Loss: 1.7460775089170422\n",
      "Epoch: 456 \t|| Train Loss: 0.8987779031196924 \t|| Test Loss: 1.744730408920584\n",
      "Epoch: 457 \t|| Train Loss: 0.897625803126056 \t|| Test Loss: 1.7433833089241255\n",
      "Epoch: 458 \t|| Train Loss: 0.8964737031324195 \t|| Test Loss: 1.7420362089276673\n",
      "Epoch: 459 \t|| Train Loss: 0.8953216031387828 \t|| Test Loss: 1.7406891089312087\n",
      "Epoch: 460 \t|| Train Loss: 0.8941695031451464 \t|| Test Loss: 1.73934200893475\n",
      "Epoch: 461 \t|| Train Loss: 0.8930174031515099 \t|| Test Loss: 1.7379949089382918\n",
      "Epoch: 462 \t|| Train Loss: 0.8918653031578735 \t|| Test Loss: 1.7366478089418336\n",
      "Epoch: 463 \t|| Train Loss: 0.8907132031642371 \t|| Test Loss: 1.7353007089453751\n",
      "Epoch: 464 \t|| Train Loss: 0.8895611031706008 \t|| Test Loss: 1.733953608948917\n",
      "Epoch: 465 \t|| Train Loss: 0.8884090031769644 \t|| Test Loss: 1.7326065089524587\n",
      "Epoch: 466 \t|| Train Loss: 0.8872569031833277 \t|| Test Loss: 1.731259408956\n",
      "Epoch: 467 \t|| Train Loss: 0.8861048031896914 \t|| Test Loss: 1.7299123089595416\n",
      "Epoch: 468 \t|| Train Loss: 0.8849527031960548 \t|| Test Loss: 1.728565208963083\n",
      "Epoch: 469 \t|| Train Loss: 0.8838006032024183 \t|| Test Loss: 1.7272181089666248\n",
      "Epoch: 470 \t|| Train Loss: 0.8826485032087821 \t|| Test Loss: 1.7258710089701665\n",
      "Epoch: 471 \t|| Train Loss: 0.8814964032151454 \t|| Test Loss: 1.7245239089737083\n",
      "Epoch: 472 \t|| Train Loss: 0.8803443032215089 \t|| Test Loss: 1.7231768089772497\n",
      "Epoch: 473 \t|| Train Loss: 0.8791922032278723 \t|| Test Loss: 1.7218297089807912\n",
      "Epoch: 474 \t|| Train Loss: 0.8780401032342358 \t|| Test Loss: 1.720482608984333\n",
      "Epoch: 475 \t|| Train Loss: 0.8768880032405996 \t|| Test Loss: 1.7191355089878744\n",
      "Epoch: 476 \t|| Train Loss: 0.8757359032469629 \t|| Test Loss: 1.7177884089914162\n",
      "Epoch: 477 \t|| Train Loss: 0.8745838032533266 \t|| Test Loss: 1.7164413089949577\n",
      "Epoch: 478 \t|| Train Loss: 0.8734317032596902 \t|| Test Loss: 1.7150942089984995\n",
      "Epoch: 479 \t|| Train Loss: 0.8722796032660536 \t|| Test Loss: 1.7137471090020409\n",
      "Epoch: 480 \t|| Train Loss: 0.8711275032724173 \t|| Test Loss: 1.7124000090055822\n",
      "Epoch: 481 \t|| Train Loss: 0.8699754032787805 \t|| Test Loss: 1.711052909009124\n",
      "Epoch: 482 \t|| Train Loss: 0.8688233032851441 \t|| Test Loss: 1.7097058090126658\n",
      "Epoch: 483 \t|| Train Loss: 0.8676712032915077 \t|| Test Loss: 1.7083587090162076\n",
      "Epoch: 484 \t|| Train Loss: 0.8665191032978712 \t|| Test Loss: 1.7070116090197491\n",
      "Epoch: 485 \t|| Train Loss: 0.8653670033042349 \t|| Test Loss: 1.705664509023291\n",
      "Epoch: 486 \t|| Train Loss: 0.8642149033105984 \t|| Test Loss: 1.7043174090268323\n",
      "Epoch: 487 \t|| Train Loss: 0.8630628033169618 \t|| Test Loss: 1.702970309030374\n",
      "Epoch: 488 \t|| Train Loss: 0.8619107033233254 \t|| Test Loss: 1.7016232090339156\n",
      "Epoch: 489 \t|| Train Loss: 0.8607586033296888 \t|| Test Loss: 1.700276109037457\n",
      "Epoch: 490 \t|| Train Loss: 0.8596065033360525 \t|| Test Loss: 1.6989290090409987\n",
      "Epoch: 491 \t|| Train Loss: 0.8584544033424159 \t|| Test Loss: 1.69758190904454\n",
      "Epoch: 492 \t|| Train Loss: 0.8573023033487794 \t|| Test Loss: 1.6962348090480819\n",
      "Epoch: 493 \t|| Train Loss: 0.8561502033551431 \t|| Test Loss: 1.6948877090516234\n",
      "Epoch: 494 \t|| Train Loss: 0.8549981033615065 \t|| Test Loss: 1.6935406090551652\n",
      "Epoch: 495 \t|| Train Loss: 0.85384600336787 \t|| Test Loss: 1.6921935090587066\n",
      "Epoch: 496 \t|| Train Loss: 0.8526939033742336 \t|| Test Loss: 1.6908464090622481\n",
      "Epoch: 497 \t|| Train Loss: 0.8515418033805972 \t|| Test Loss: 1.68949930906579\n",
      "Epoch: 498 \t|| Train Loss: 0.8503897033869606 \t|| Test Loss: 1.6881522090693317\n",
      "Epoch: 499 \t|| Train Loss: 0.8492376033933242 \t|| Test Loss: 1.686805109072873\n",
      "Epoch: 500 \t|| Train Loss: 0.8480855033996878 \t|| Test Loss: 1.6854580090764149\n",
      "Epoch: 501 \t|| Train Loss: 0.8469334034060513 \t|| Test Loss: 1.6841109090799562\n",
      "Epoch: 502 \t|| Train Loss: 0.8457813034124149 \t|| Test Loss: 1.682763809083498\n",
      "Epoch: 503 \t|| Train Loss: 0.8446292034187785 \t|| Test Loss: 1.6814167090870398\n",
      "Epoch: 504 \t|| Train Loss: 0.8434771034251419 \t|| Test Loss: 1.6800696090905813\n",
      "Epoch: 505 \t|| Train Loss: 0.8423250034315053 \t|| Test Loss: 1.6787225090941227\n",
      "Epoch: 506 \t|| Train Loss: 0.841172903437869 \t|| Test Loss: 1.6773754090976645\n",
      "Epoch: 507 \t|| Train Loss: 0.8400208034442324 \t|| Test Loss: 1.6760283091012063\n",
      "Epoch: 508 \t|| Train Loss: 0.8388687034505959 \t|| Test Loss: 1.6746812091047478\n",
      "Epoch: 509 \t|| Train Loss: 0.8377166034569594 \t|| Test Loss: 1.6733341091082896\n",
      "Epoch: 510 \t|| Train Loss: 0.836564503463323 \t|| Test Loss: 1.671987009111831\n",
      "Epoch: 511 \t|| Train Loss: 0.8354124034696865 \t|| Test Loss: 1.6706399091153723\n",
      "Epoch: 512 \t|| Train Loss: 0.8342603034760501 \t|| Test Loss: 1.669292809118914\n",
      "Epoch: 513 \t|| Train Loss: 0.8331082034824137 \t|| Test Loss: 1.6679457091224557\n",
      "Epoch: 514 \t|| Train Loss: 0.8319561034887771 \t|| Test Loss: 1.6665986091259974\n",
      "Epoch: 515 \t|| Train Loss: 0.8308040034951407 \t|| Test Loss: 1.6652515091295388\n",
      "Epoch: 516 \t|| Train Loss: 0.8296519035015042 \t|| Test Loss: 1.6639044091330806\n",
      "Epoch: 517 \t|| Train Loss: 0.8284998035078678 \t|| Test Loss: 1.6625573091366221\n",
      "Epoch: 518 \t|| Train Loss: 0.8273477035142311 \t|| Test Loss: 1.661210209140164\n",
      "Epoch: 519 \t|| Train Loss: 0.8261956035205948 \t|| Test Loss: 1.6598631091437053\n",
      "Epoch: 520 \t|| Train Loss: 0.8250435035269582 \t|| Test Loss: 1.658516009147247\n",
      "Epoch: 521 \t|| Train Loss: 0.8238914035333218 \t|| Test Loss: 1.6571689091507884\n",
      "Epoch: 522 \t|| Train Loss: 0.8227393035396855 \t|| Test Loss: 1.6558218091543302\n",
      "Epoch: 523 \t|| Train Loss: 0.8215872035460491 \t|| Test Loss: 1.654474709157872\n",
      "Epoch: 524 \t|| Train Loss: 0.8204351035524124 \t|| Test Loss: 1.6531276091614138\n",
      "Epoch: 525 \t|| Train Loss: 0.8192830035587761 \t|| Test Loss: 1.6517805091649553\n",
      "Epoch: 526 \t|| Train Loss: 0.8181309035651395 \t|| Test Loss: 1.6504334091684967\n",
      "Epoch: 527 \t|| Train Loss: 0.8169788035715031 \t|| Test Loss: 1.6490863091720385\n",
      "Epoch: 528 \t|| Train Loss: 0.8158267035778666 \t|| Test Loss: 1.64773920917558\n",
      "Epoch: 529 \t|| Train Loss: 0.81467460358423 \t|| Test Loss: 1.6463921091791218\n",
      "Epoch: 530 \t|| Train Loss: 0.8135225035905936 \t|| Test Loss: 1.6450450091826632\n",
      "Epoch: 531 \t|| Train Loss: 0.812370403596957 \t|| Test Loss: 1.6436979091862045\n",
      "Epoch: 532 \t|| Train Loss: 0.8112183036033208 \t|| Test Loss: 1.6423508091897463\n",
      "Epoch: 533 \t|| Train Loss: 0.8100662036096843 \t|| Test Loss: 1.6410037091932879\n",
      "Epoch: 534 \t|| Train Loss: 0.8089141036160479 \t|| Test Loss: 1.6396566091968297\n",
      "Epoch: 535 \t|| Train Loss: 0.8077620036224114 \t|| Test Loss: 1.638309509200371\n",
      "Epoch: 536 \t|| Train Loss: 0.8066099036287749 \t|| Test Loss: 1.6369624092039128\n",
      "Epoch: 537 \t|| Train Loss: 0.8054578036351383 \t|| Test Loss: 1.6356153092074543\n",
      "Epoch: 538 \t|| Train Loss: 0.804305703641502 \t|| Test Loss: 1.6342682092109961\n",
      "Epoch: 539 \t|| Train Loss: 0.8031536036478654 \t|| Test Loss: 1.6329211092145375\n",
      "Epoch: 540 \t|| Train Loss: 0.802001503654229 \t|| Test Loss: 1.6315740092180793\n",
      "Epoch: 541 \t|| Train Loss: 0.8008494036605924 \t|| Test Loss: 1.630226909221621\n",
      "Epoch: 542 \t|| Train Loss: 0.7996973036669561 \t|| Test Loss: 1.6288798092251624\n",
      "Epoch: 543 \t|| Train Loss: 0.7985452036733195 \t|| Test Loss: 1.627532709228704\n",
      "Epoch: 544 \t|| Train Loss: 0.7973931036796831 \t|| Test Loss: 1.6261856092322453\n",
      "Epoch: 545 \t|| Train Loss: 0.7962410036860467 \t|| Test Loss: 1.6248385092357875\n",
      "Epoch: 546 \t|| Train Loss: 0.7950889036924101 \t|| Test Loss: 1.6234914092393289\n",
      "Epoch: 547 \t|| Train Loss: 0.7939368036987737 \t|| Test Loss: 1.6221443092428707\n",
      "Epoch: 548 \t|| Train Loss: 0.7927847037051372 \t|| Test Loss: 1.6207972092464122\n",
      "Epoch: 549 \t|| Train Loss: 0.7916326037115008 \t|| Test Loss: 1.6194501092499536\n",
      "Epoch: 550 \t|| Train Loss: 0.7904805037178642 \t|| Test Loss: 1.6181030092534954\n",
      "Epoch: 551 \t|| Train Loss: 0.7893284037242279 \t|| Test Loss: 1.6167559092570367\n",
      "Epoch: 552 \t|| Train Loss: 0.7881763037305914 \t|| Test Loss: 1.6154088092605785\n",
      "Epoch: 553 \t|| Train Loss: 0.7870242037369548 \t|| Test Loss: 1.61406170926412\n",
      "Epoch: 554 \t|| Train Loss: 0.7858721037433184 \t|| Test Loss: 1.6127146092676619\n",
      "Epoch: 555 \t|| Train Loss: 0.784720003749682 \t|| Test Loss: 1.6113675092712032\n",
      "Epoch: 556 \t|| Train Loss: 0.7835679037560455 \t|| Test Loss: 1.610020409274745\n",
      "Epoch: 557 \t|| Train Loss: 0.7824158037624089 \t|| Test Loss: 1.6086733092782866\n",
      "Epoch: 558 \t|| Train Loss: 0.7812637037687725 \t|| Test Loss: 1.6073262092818283\n",
      "Epoch: 559 \t|| Train Loss: 0.7801116037751361 \t|| Test Loss: 1.6059791092853701\n",
      "Epoch: 560 \t|| Train Loss: 0.7789595037814996 \t|| Test Loss: 1.6046320092889115\n",
      "Epoch: 561 \t|| Train Loss: 0.7778074037878631 \t|| Test Loss: 1.6032849092924528\n",
      "Epoch: 562 \t|| Train Loss: 0.7766553037942266 \t|| Test Loss: 1.6019378092959946\n",
      "Epoch: 563 \t|| Train Loss: 0.7755032038005902 \t|| Test Loss: 1.6005907092995362\n",
      "Epoch: 564 \t|| Train Loss: 0.7743511038069537 \t|| Test Loss: 1.599243609303078\n",
      "Epoch: 565 \t|| Train Loss: 0.7731990038133173 \t|| Test Loss: 1.5978965093066195\n",
      "Epoch: 566 \t|| Train Loss: 0.7720469038196808 \t|| Test Loss: 1.596549409310161\n",
      "Epoch: 567 \t|| Train Loss: 0.7708948038260443 \t|| Test Loss: 1.5952023093137029\n",
      "Epoch: 568 \t|| Train Loss: 0.7697427038324078 \t|| Test Loss: 1.5938552093172444\n",
      "Epoch: 569 \t|| Train Loss: 0.7685906038387713 \t|| Test Loss: 1.5925081093207858\n",
      "Epoch: 570 \t|| Train Loss: 0.7674385038451349 \t|| Test Loss: 1.5911610093243276\n",
      "Epoch: 571 \t|| Train Loss: 0.7662864038514984 \t|| Test Loss: 1.5898139093278691\n",
      "Epoch: 572 \t|| Train Loss: 0.765134303857862 \t|| Test Loss: 1.5884668093314107\n",
      "Epoch: 573 \t|| Train Loss: 0.7639822038642254 \t|| Test Loss: 1.5871197093349523\n",
      "Epoch: 574 \t|| Train Loss: 0.7628301038705889 \t|| Test Loss: 1.585772609338494\n",
      "Epoch: 575 \t|| Train Loss: 0.7616780038769525 \t|| Test Loss: 1.5844255093420356\n",
      "Epoch: 576 \t|| Train Loss: 0.760525903883316 \t|| Test Loss: 1.5830784093455772\n",
      "Epoch: 577 \t|| Train Loss: 0.7593738038896796 \t|| Test Loss: 1.5817313093491188\n",
      "Epoch: 578 \t|| Train Loss: 0.7582217038960432 \t|| Test Loss: 1.5803842093526606\n",
      "Epoch: 579 \t|| Train Loss: 0.7570696039024067 \t|| Test Loss: 1.579037109356202\n",
      "Epoch: 580 \t|| Train Loss: 0.7559175039087702 \t|| Test Loss: 1.5776900093597437\n",
      "Epoch: 581 \t|| Train Loss: 0.7547654039151338 \t|| Test Loss: 1.576342909363285\n",
      "Epoch: 582 \t|| Train Loss: 0.7536133039214972 \t|| Test Loss: 1.5749958093668268\n",
      "Epoch: 583 \t|| Train Loss: 0.7524612039278608 \t|| Test Loss: 1.5736487093703684\n",
      "Epoch: 584 \t|| Train Loss: 0.7513091039342242 \t|| Test Loss: 1.57230160937391\n",
      "Epoch: 585 \t|| Train Loss: 0.7501570039405878 \t|| Test Loss: 1.5709545093774515\n",
      "Epoch: 586 \t|| Train Loss: 0.7490049039469513 \t|| Test Loss: 1.5696074093809933\n",
      "Epoch: 587 \t|| Train Loss: 0.747852803953315 \t|| Test Loss: 1.5682603093845349\n",
      "Epoch: 588 \t|| Train Loss: 0.7467007039596785 \t|| Test Loss: 1.5669132093880767\n",
      "Epoch: 589 \t|| Train Loss: 0.7455486039660418 \t|| Test Loss: 1.565566109391618\n",
      "Epoch: 590 \t|| Train Loss: 0.7443965039724055 \t|| Test Loss: 1.5642190093951598\n",
      "Epoch: 591 \t|| Train Loss: 0.743244403978769 \t|| Test Loss: 1.5628719093987014\n",
      "Epoch: 592 \t|| Train Loss: 0.7420923039851326 \t|| Test Loss: 1.561524809402243\n",
      "Epoch: 593 \t|| Train Loss: 0.7409402039914961 \t|| Test Loss: 1.5601777094057847\n",
      "Epoch: 594 \t|| Train Loss: 0.7397881039978597 \t|| Test Loss: 1.5588306094093263\n",
      "Epoch: 595 \t|| Train Loss: 0.7386360040042231 \t|| Test Loss: 1.557483509412868\n",
      "Epoch: 596 \t|| Train Loss: 0.7374839040105867 \t|| Test Loss: 1.5561364094164094\n",
      "Epoch: 597 \t|| Train Loss: 0.7363318040169501 \t|| Test Loss: 1.554789309419951\n",
      "Epoch: 598 \t|| Train Loss: 0.7351797040233138 \t|| Test Loss: 1.5534422094234928\n",
      "Epoch: 599 \t|| Train Loss: 0.7340276040296774 \t|| Test Loss: 1.552095109427034\n",
      "Epoch: 600 \t|| Train Loss: 0.7328755040360407 \t|| Test Loss: 1.550748009430576\n",
      "Epoch: 601 \t|| Train Loss: 0.7317234040424043 \t|| Test Loss: 1.5494009094341172\n",
      "Epoch: 602 \t|| Train Loss: 0.7305713040487678 \t|| Test Loss: 1.5480538094376588\n",
      "Epoch: 603 \t|| Train Loss: 0.7294192040551313 \t|| Test Loss: 1.5467067094412008\n",
      "Epoch: 604 \t|| Train Loss: 0.7282671040614949 \t|| Test Loss: 1.5453596094447424\n",
      "Epoch: 605 \t|| Train Loss: 0.7271150040678585 \t|| Test Loss: 1.5440125094482842\n",
      "Epoch: 606 \t|| Train Loss: 0.7259629040742219 \t|| Test Loss: 1.5426654094518253\n",
      "Epoch: 607 \t|| Train Loss: 0.7248108040805855 \t|| Test Loss: 1.541318309455367\n",
      "Epoch: 608 \t|| Train Loss: 0.7236587040869491 \t|| Test Loss: 1.5399712094589089\n",
      "Epoch: 609 \t|| Train Loss: 0.7225066040933126 \t|| Test Loss: 1.5386241094624504\n",
      "Epoch: 610 \t|| Train Loss: 0.7213545040996762 \t|| Test Loss: 1.537277009465992\n",
      "Epoch: 611 \t|| Train Loss: 0.7202024041060396 \t|| Test Loss: 1.5359299094695333\n",
      "Epoch: 612 \t|| Train Loss: 0.7190503041124032 \t|| Test Loss: 1.5345828094730751\n",
      "Epoch: 613 \t|| Train Loss: 0.7178982041187667 \t|| Test Loss: 1.533235709476617\n",
      "Epoch: 614 \t|| Train Loss: 0.7167461041251302 \t|| Test Loss: 1.5318886094801585\n",
      "Epoch: 615 \t|| Train Loss: 0.7155940041314938 \t|| Test Loss: 1.5305415094836998\n",
      "Epoch: 616 \t|| Train Loss: 0.7144419041378571 \t|| Test Loss: 1.5291944094872416\n",
      "Epoch: 617 \t|| Train Loss: 0.7132898041442208 \t|| Test Loss: 1.5278473094907832\n",
      "Epoch: 618 \t|| Train Loss: 0.7121377041505843 \t|| Test Loss: 1.526500209494325\n",
      "Epoch: 619 \t|| Train Loss: 0.7109856041569478 \t|| Test Loss: 1.5251531094978665\n",
      "Epoch: 620 \t|| Train Loss: 0.7098335041633114 \t|| Test Loss: 1.5238060095014079\n",
      "Epoch: 621 \t|| Train Loss: 0.7086814041696747 \t|| Test Loss: 1.5224589095049494\n",
      "Epoch: 622 \t|| Train Loss: 0.7075293041760384 \t|| Test Loss: 1.5211118095084912\n",
      "Epoch: 623 \t|| Train Loss: 0.706377204182402 \t|| Test Loss: 1.5197647095120328\n",
      "Epoch: 624 \t|| Train Loss: 0.7052251041887655 \t|| Test Loss: 1.5184176095155741\n",
      "Epoch: 625 \t|| Train Loss: 0.704073004195129 \t|| Test Loss: 1.517070509519116\n",
      "Epoch: 626 \t|| Train Loss: 0.7029209042014927 \t|| Test Loss: 1.5157234095226577\n",
      "Epoch: 627 \t|| Train Loss: 0.701768804207856 \t|| Test Loss: 1.5143763095261993\n",
      "Epoch: 628 \t|| Train Loss: 0.7006167042142197 \t|| Test Loss: 1.513029209529741\n",
      "Epoch: 629 \t|| Train Loss: 0.6994646042205831 \t|| Test Loss: 1.5116821095332826\n",
      "Epoch: 630 \t|| Train Loss: 0.6983125042269467 \t|| Test Loss: 1.5103350095368242\n",
      "Epoch: 631 \t|| Train Loss: 0.6971604042333104 \t|| Test Loss: 1.5089879095403658\n",
      "Epoch: 632 \t|| Train Loss: 0.6960083042396739 \t|| Test Loss: 1.5076408095439073\n",
      "Epoch: 633 \t|| Train Loss: 0.6948562042460373 \t|| Test Loss: 1.5062937095474491\n",
      "Epoch: 634 \t|| Train Loss: 0.6937041042524009 \t|| Test Loss: 1.5049466095509907\n",
      "Epoch: 635 \t|| Train Loss: 0.6925520042587643 \t|| Test Loss: 1.5035995095545323\n",
      "Epoch: 636 \t|| Train Loss: 0.691399904265128 \t|| Test Loss: 1.5022524095580738\n",
      "Epoch: 637 \t|| Train Loss: 0.6902478042714915 \t|| Test Loss: 1.5009053095616154\n",
      "Epoch: 638 \t|| Train Loss: 0.6890957042778549 \t|| Test Loss: 1.4995582095651572\n",
      "Epoch: 639 \t|| Train Loss: 0.6879436042842185 \t|| Test Loss: 1.4982111095686985\n",
      "Epoch: 640 \t|| Train Loss: 0.6867915042905819 \t|| Test Loss: 1.4968640095722403\n",
      "Epoch: 641 \t|| Train Loss: 0.6856394042969456 \t|| Test Loss: 1.4955169095757819\n",
      "Epoch: 642 \t|| Train Loss: 0.6844873043033091 \t|| Test Loss: 1.4941698095793234\n",
      "Epoch: 643 \t|| Train Loss: 0.6833352043096725 \t|| Test Loss: 1.4928227095828652\n",
      "Epoch: 644 \t|| Train Loss: 0.6821831043160362 \t|| Test Loss: 1.4914756095864066\n",
      "Epoch: 645 \t|| Train Loss: 0.6810310043223997 \t|| Test Loss: 1.4901285095899486\n",
      "Epoch: 646 \t|| Train Loss: 0.6798789043287633 \t|| Test Loss: 1.48878140959349\n",
      "Epoch: 647 \t|| Train Loss: 0.6787268043351268 \t|| Test Loss: 1.4874343095970315\n",
      "Epoch: 648 \t|| Train Loss: 0.6775747043414903 \t|| Test Loss: 1.4860872096005733\n",
      "Epoch: 649 \t|| Train Loss: 0.6764226043478538 \t|| Test Loss: 1.484740109604115\n",
      "Epoch: 650 \t|| Train Loss: 0.6752705043542173 \t|| Test Loss: 1.4833930096076566\n",
      "Epoch: 651 \t|| Train Loss: 0.6741184043605809 \t|| Test Loss: 1.482045909611198\n",
      "Epoch: 652 \t|| Train Loss: 0.6729663043669444 \t|| Test Loss: 1.4806988096147395\n",
      "Epoch: 653 \t|| Train Loss: 0.671814204373308 \t|| Test Loss: 1.4793517096182813\n",
      "Epoch: 654 \t|| Train Loss: 0.6706621043796714 \t|| Test Loss: 1.478004609621823\n",
      "Epoch: 655 \t|| Train Loss: 0.669510004386035 \t|| Test Loss: 1.4766575096253647\n",
      "Epoch: 656 \t|| Train Loss: 0.6683579043923984 \t|| Test Loss: 1.475310409628906\n",
      "Epoch: 657 \t|| Train Loss: 0.6672058043987621 \t|| Test Loss: 1.4739633096324478\n",
      "Epoch: 658 \t|| Train Loss: 0.6660537044051256 \t|| Test Loss: 1.4726162096359894\n",
      "Epoch: 659 \t|| Train Loss: 0.664901604411489 \t|| Test Loss: 1.471269109639531\n",
      "Epoch: 660 \t|| Train Loss: 0.6637495044178527 \t|| Test Loss: 1.4699220096430725\n",
      "Epoch: 661 \t|| Train Loss: 0.6625974044242162 \t|| Test Loss: 1.4685749096466139\n",
      "Epoch: 662 \t|| Train Loss: 0.6614453044305797 \t|| Test Loss: 1.4672278096501559\n",
      "Epoch: 663 \t|| Train Loss: 0.6602932044369433 \t|| Test Loss: 1.4658807096536972\n",
      "Epoch: 664 \t|| Train Loss: 0.6591411044433068 \t|| Test Loss: 1.464533609657239\n",
      "Epoch: 665 \t|| Train Loss: 0.6579890044496702 \t|| Test Loss: 1.4631865096607803\n",
      "Epoch: 666 \t|| Train Loss: 0.6568369044560338 \t|| Test Loss: 1.4618394096643221\n",
      "Epoch: 667 \t|| Train Loss: 0.6556848044623973 \t|| Test Loss: 1.4604923096678637\n",
      "Epoch: 668 \t|| Train Loss: 0.6545327044687609 \t|| Test Loss: 1.4591452096714055\n",
      "Epoch: 669 \t|| Train Loss: 0.6533806044751245 \t|| Test Loss: 1.4577981096749473\n",
      "Epoch: 670 \t|| Train Loss: 0.652228504481488 \t|| Test Loss: 1.4564510096784886\n",
      "Epoch: 671 \t|| Train Loss: 0.6510764044878515 \t|| Test Loss: 1.4551039096820304\n",
      "Epoch: 672 \t|| Train Loss: 0.6499243044942151 \t|| Test Loss: 1.453756809685572\n",
      "Epoch: 673 \t|| Train Loss: 0.6487722045005786 \t|| Test Loss: 1.4524097096891135\n",
      "Epoch: 674 \t|| Train Loss: 0.6476201045069422 \t|| Test Loss: 1.451062609692655\n",
      "Epoch: 675 \t|| Train Loss: 0.6464680045133055 \t|| Test Loss: 1.4497155096961967\n",
      "Epoch: 676 \t|| Train Loss: 0.6453159045196691 \t|| Test Loss: 1.4483684096997385\n",
      "Epoch: 677 \t|| Train Loss: 0.6441638045260326 \t|| Test Loss: 1.44702130970328\n",
      "Epoch: 678 \t|| Train Loss: 0.6430117045323963 \t|| Test Loss: 1.4456742097068216\n",
      "Epoch: 679 \t|| Train Loss: 0.6418596045387598 \t|| Test Loss: 1.4443271097103632\n",
      "Epoch: 680 \t|| Train Loss: 0.6407075045451232 \t|| Test Loss: 1.4429800097139045\n",
      "Epoch: 681 \t|| Train Loss: 0.6395554045514869 \t|| Test Loss: 1.4416329097174465\n",
      "Epoch: 682 \t|| Train Loss: 0.6384033045578503 \t|| Test Loss: 1.4402858097209879\n",
      "Epoch: 683 \t|| Train Loss: 0.6372512045642139 \t|| Test Loss: 1.4389387097245294\n",
      "Epoch: 684 \t|| Train Loss: 0.6360991045705774 \t|| Test Loss: 1.4375916097280712\n",
      "Epoch: 685 \t|| Train Loss: 0.634947004576941 \t|| Test Loss: 1.4362445097316128\n",
      "Epoch: 686 \t|| Train Loss: 0.6337949045833045 \t|| Test Loss: 1.4348974097351543\n",
      "Epoch: 687 \t|| Train Loss: 0.6326428045896679 \t|| Test Loss: 1.4335503097386961\n",
      "Epoch: 688 \t|| Train Loss: 0.6314907045960314 \t|| Test Loss: 1.4322032097422377\n",
      "Epoch: 689 \t|| Train Loss: 0.6303386046023951 \t|| Test Loss: 1.4308561097457795\n",
      "Epoch: 690 \t|| Train Loss: 0.6291865046087586 \t|| Test Loss: 1.4295090097493208\n",
      "Epoch: 691 \t|| Train Loss: 0.6280344046151222 \t|| Test Loss: 1.4281619097528626\n",
      "Epoch: 692 \t|| Train Loss: 0.6268823046214856 \t|| Test Loss: 1.426814809756404\n",
      "Epoch: 693 \t|| Train Loss: 0.6257302046278491 \t|| Test Loss: 1.4254677097599457\n",
      "Epoch: 694 \t|| Train Loss: 0.6245838918812275 \t|| Test Loss: 1.4241706097634874\n",
      "Epoch: 695 \t|| Train Loss: 0.623529291887591 \t|| Test Loss: 1.4228735097670289\n",
      "Epoch: 696 \t|| Train Loss: 0.6224746918939547 \t|| Test Loss: 1.4215764097705705\n",
      "Epoch: 697 \t|| Train Loss: 0.6214200919003181 \t|| Test Loss: 1.420279309774112\n",
      "Epoch: 698 \t|| Train Loss: 0.6203654919066817 \t|| Test Loss: 1.4189822097776539\n",
      "Epoch: 699 \t|| Train Loss: 0.6193108919130452 \t|| Test Loss: 1.4176851097811953\n",
      "Epoch: 700 \t|| Train Loss: 0.6182562919194087 \t|| Test Loss: 1.4163880097847372\n",
      "Epoch: 701 \t|| Train Loss: 0.6172016919257721 \t|| Test Loss: 1.4150909097882787\n",
      "Epoch: 702 \t|| Train Loss: 0.6161470919321357 \t|| Test Loss: 1.4137938097918201\n",
      "Epoch: 703 \t|| Train Loss: 0.6150924919384992 \t|| Test Loss: 1.4124967097953618\n",
      "Epoch: 704 \t|| Train Loss: 0.6140378919448628 \t|| Test Loss: 1.4111996097989035\n",
      "Epoch: 705 \t|| Train Loss: 0.6129832919512264 \t|| Test Loss: 1.409902509802445\n",
      "Epoch: 706 \t|| Train Loss: 0.6119286919575899 \t|| Test Loss: 1.4086054098059866\n",
      "Epoch: 707 \t|| Train Loss: 0.6108740919639535 \t|| Test Loss: 1.4073083098095283\n",
      "Epoch: 708 \t|| Train Loss: 0.6098194919703168 \t|| Test Loss: 1.40601120981307\n",
      "Epoch: 709 \t|| Train Loss: 0.6087648919766805 \t|| Test Loss: 1.4047141098166116\n",
      "Epoch: 710 \t|| Train Loss: 0.6077102919830439 \t|| Test Loss: 1.403417009820153\n",
      "Epoch: 711 \t|| Train Loss: 0.6066556919894074 \t|| Test Loss: 1.4021199098236945\n",
      "Epoch: 712 \t|| Train Loss: 0.605601091995771 \t|| Test Loss: 1.4008228098272364\n",
      "Epoch: 713 \t|| Train Loss: 0.6045464920021346 \t|| Test Loss: 1.3995257098307776\n",
      "Epoch: 714 \t|| Train Loss: 0.6034918920084981 \t|| Test Loss: 1.3982286098343195\n",
      "Epoch: 715 \t|| Train Loss: 0.6024372920148616 \t|| Test Loss: 1.396931509837861\n",
      "Epoch: 716 \t|| Train Loss: 0.6013826920212251 \t|| Test Loss: 1.3956344098414024\n",
      "Epoch: 717 \t|| Train Loss: 0.6003280920275886 \t|| Test Loss: 1.394337309844944\n",
      "Epoch: 718 \t|| Train Loss: 0.5992734920339522 \t|| Test Loss: 1.3930402098484858\n",
      "Epoch: 719 \t|| Train Loss: 0.5982188920403158 \t|| Test Loss: 1.3917431098520274\n",
      "Epoch: 720 \t|| Train Loss: 0.5971642920466792 \t|| Test Loss: 1.390446009855569\n",
      "Epoch: 721 \t|| Train Loss: 0.5961096920530429 \t|| Test Loss: 1.3891489098591108\n",
      "Epoch: 722 \t|| Train Loss: 0.5950550920594063 \t|| Test Loss: 1.3878518098626522\n",
      "Epoch: 723 \t|| Train Loss: 0.5940004920657698 \t|| Test Loss: 1.3865547098661941\n",
      "Epoch: 724 \t|| Train Loss: 0.5929458920721333 \t|| Test Loss: 1.3852576098697356\n",
      "Epoch: 725 \t|| Train Loss: 0.5918912920784969 \t|| Test Loss: 1.3839605098732772\n",
      "Epoch: 726 \t|| Train Loss: 0.5908366920848604 \t|| Test Loss: 1.3826634098768187\n",
      "Epoch: 727 \t|| Train Loss: 0.5897820920912238 \t|| Test Loss: 1.3813663098803601\n",
      "Epoch: 728 \t|| Train Loss: 0.5887597602782348 \t|| Test Loss: 1.3801200998838916\n",
      "Epoch: 729 \t|| Train Loss: 0.5877984392845647 \t|| Test Loss: 1.3788738898874229\n",
      "Epoch: 730 \t|| Train Loss: 0.5868371182908945 \t|| Test Loss: 1.3776276798909541\n",
      "Epoch: 731 \t|| Train Loss: 0.5858757972972244 \t|| Test Loss: 1.3763814698944856\n",
      "Epoch: 732 \t|| Train Loss: 0.5849144763035541 \t|| Test Loss: 1.3751352598980169\n",
      "Epoch: 733 \t|| Train Loss: 0.583953155309884 \t|| Test Loss: 1.3738890499015484\n",
      "Epoch: 734 \t|| Train Loss: 0.5829918343162138 \t|| Test Loss: 1.3726428399050796\n",
      "Epoch: 735 \t|| Train Loss: 0.5820305133225437 \t|| Test Loss: 1.3713966299086107\n",
      "Epoch: 736 \t|| Train Loss: 0.5810691923288736 \t|| Test Loss: 1.370150419912142\n",
      "Epoch: 737 \t|| Train Loss: 0.5801078713352034 \t|| Test Loss: 1.3689042099156734\n",
      "Epoch: 738 \t|| Train Loss: 0.5791465503415332 \t|| Test Loss: 1.367657999919205\n",
      "Epoch: 739 \t|| Train Loss: 0.5781852293478631 \t|| Test Loss: 1.366411789922736\n",
      "Epoch: 740 \t|| Train Loss: 0.5772239083541929 \t|| Test Loss: 1.3651655799262674\n",
      "Epoch: 741 \t|| Train Loss: 0.5762625873605227 \t|| Test Loss: 1.3639193699297985\n",
      "Epoch: 742 \t|| Train Loss: 0.5753012663668525 \t|| Test Loss: 1.3626731599333297\n",
      "Epoch: 743 \t|| Train Loss: 0.5743399453731823 \t|| Test Loss: 1.361426949936861\n",
      "Epoch: 744 \t|| Train Loss: 0.5733786243795123 \t|| Test Loss: 1.3601807399403925\n",
      "Epoch: 745 \t|| Train Loss: 0.5724173033858421 \t|| Test Loss: 1.3589345299439237\n",
      "Epoch: 746 \t|| Train Loss: 0.571455982392172 \t|| Test Loss: 1.3576883199474552\n",
      "Epoch: 747 \t|| Train Loss: 0.5704946613985017 \t|| Test Loss: 1.3564421099509865\n",
      "Epoch: 748 \t|| Train Loss: 0.5695333404048315 \t|| Test Loss: 1.3551958999545175\n",
      "Epoch: 749 \t|| Train Loss: 0.5685720194111614 \t|| Test Loss: 1.353949689958049\n",
      "Epoch: 750 \t|| Train Loss: 0.5676106984174912 \t|| Test Loss: 1.3527034799615802\n",
      "Epoch: 751 \t|| Train Loss: 0.5666493774238212 \t|| Test Loss: 1.3514572699651117\n",
      "Epoch: 752 \t|| Train Loss: 0.5656880564301509 \t|| Test Loss: 1.3502110599686428\n",
      "Epoch: 753 \t|| Train Loss: 0.5647267354364809 \t|| Test Loss: 1.3489648499721745\n",
      "Epoch: 754 \t|| Train Loss: 0.5637654144428106 \t|| Test Loss: 1.3477186399757053\n",
      "Epoch: 755 \t|| Train Loss: 0.5628040934491405 \t|| Test Loss: 1.346472429979237\n",
      "Epoch: 756 \t|| Train Loss: 0.5618427724554703 \t|| Test Loss: 1.3452262199827678\n",
      "Epoch: 757 \t|| Train Loss: 0.5608814514618001 \t|| Test Loss: 1.3439800099862995\n",
      "Epoch: 758 \t|| Train Loss: 0.5599201304681299 \t|| Test Loss: 1.3427337999898308\n",
      "Epoch: 759 \t|| Train Loss: 0.5589588094744598 \t|| Test Loss: 1.341487589993362\n",
      "Epoch: 760 \t|| Train Loss: 0.5579974884807897 \t|| Test Loss: 1.3402413799968933\n",
      "Epoch: 761 \t|| Train Loss: 0.5570361674871195 \t|| Test Loss: 1.3389951700004246\n",
      "Epoch: 762 \t|| Train Loss: 0.5560750476065847 \t|| Test Loss: 1.3378007400039351\n",
      "Epoch: 763 \t|| Train Loss: 0.5552027786128473 \t|| Test Loss: 1.3366063100074457\n",
      "Epoch: 764 \t|| Train Loss: 0.55433050961911 \t|| Test Loss: 1.335411880010956\n",
      "Epoch: 765 \t|| Train Loss: 0.5534582406253727 \t|| Test Loss: 1.3342174500144668\n",
      "Epoch: 766 \t|| Train Loss: 0.5525859716316354 \t|| Test Loss: 1.333023020017977\n",
      "Epoch: 767 \t|| Train Loss: 0.5517137026378981 \t|| Test Loss: 1.331828590021488\n",
      "Epoch: 768 \t|| Train Loss: 0.5508414336441606 \t|| Test Loss: 1.330634160024998\n",
      "Epoch: 769 \t|| Train Loss: 0.5499691646504233 \t|| Test Loss: 1.3294397300285086\n",
      "Epoch: 770 \t|| Train Loss: 0.549096895656686 \t|| Test Loss: 1.3282453000320191\n",
      "Epoch: 771 \t|| Train Loss: 0.5482246266629487 \t|| Test Loss: 1.3270508700355295\n",
      "Epoch: 772 \t|| Train Loss: 0.5473523576692113 \t|| Test Loss: 1.32585644003904\n",
      "Epoch: 773 \t|| Train Loss: 0.5464800886754739 \t|| Test Loss: 1.3246620100425506\n",
      "Epoch: 774 \t|| Train Loss: 0.5456078196817365 \t|| Test Loss: 1.3234675800460611\n",
      "Epoch: 775 \t|| Train Loss: 0.5447355506879993 \t|| Test Loss: 1.3222731500495715\n",
      "Epoch: 776 \t|| Train Loss: 0.5438632816942619 \t|| Test Loss: 1.321078720053082\n",
      "Epoch: 777 \t|| Train Loss: 0.5429910127005246 \t|| Test Loss: 1.3198842900565926\n",
      "Epoch: 778 \t|| Train Loss: 0.5421187437067874 \t|| Test Loss: 1.3186898600601031\n",
      "Epoch: 779 \t|| Train Loss: 0.5412464747130499 \t|| Test Loss: 1.3174954300636137\n",
      "Epoch: 780 \t|| Train Loss: 0.5403742057193126 \t|| Test Loss: 1.3163010000671238\n",
      "Epoch: 781 \t|| Train Loss: 0.5395019367255751 \t|| Test Loss: 1.3151065700706348\n",
      "Epoch: 782 \t|| Train Loss: 0.5386296677318378 \t|| Test Loss: 1.3139121400741451\n",
      "Epoch: 783 \t|| Train Loss: 0.5377573987381005 \t|| Test Loss: 1.3127177100776557\n",
      "Epoch: 784 \t|| Train Loss: 0.5368851297443631 \t|| Test Loss: 1.311523280081166\n",
      "Epoch: 785 \t|| Train Loss: 0.5360128607506258 \t|| Test Loss: 1.3103288500846766\n",
      "Epoch: 786 \t|| Train Loss: 0.5351405917568884 \t|| Test Loss: 1.3091344200881871\n",
      "Epoch: 787 \t|| Train Loss: 0.5342683227631511 \t|| Test Loss: 1.3079399900916977\n",
      "Epoch: 788 \t|| Train Loss: 0.5333960537694138 \t|| Test Loss: 1.306745560095208\n",
      "Epoch: 789 \t|| Train Loss: 0.5325237847756764 \t|| Test Loss: 1.3055511300987184\n",
      "Epoch: 790 \t|| Train Loss: 0.531651515781939 \t|| Test Loss: 1.3043567001022291\n",
      "Epoch: 791 \t|| Train Loss: 0.5307792467882018 \t|| Test Loss: 1.3031622701057395\n",
      "Epoch: 792 \t|| Train Loss: 0.5299069777944644 \t|| Test Loss: 1.3019678401092503\n",
      "Epoch: 793 \t|| Train Loss: 0.529034708800727 \t|| Test Loss: 1.3007734101127606\n",
      "Epoch: 794 \t|| Train Loss: 0.5281624398069897 \t|| Test Loss: 1.2995789801162712\n",
      "Epoch: 795 \t|| Train Loss: 0.5272901708132524 \t|| Test Loss: 1.2983845501197817\n",
      "Epoch: 796 \t|| Train Loss: 0.5264179018195151 \t|| Test Loss: 1.2971901201232923\n",
      "Epoch: 797 \t|| Train Loss: 0.5255456328257777 \t|| Test Loss: 1.2959956901268026\n",
      "Epoch: 798 \t|| Train Loss: 0.5246733638320403 \t|| Test Loss: 1.294801260130313\n",
      "Epoch: 799 \t|| Train Loss: 0.5238414598826404 \t|| Test Loss: 1.2936595001337927\n",
      "Epoch: 800 \t|| Train Loss: 0.523054003888803 \t|| Test Loss: 1.292517740137272\n",
      "Epoch: 801 \t|| Train Loss: 0.5222665478949656 \t|| Test Loss: 1.2913759801407516\n",
      "Epoch: 802 \t|| Train Loss: 0.5214790919011283 \t|| Test Loss: 1.2902342201442312\n",
      "Epoch: 803 \t|| Train Loss: 0.5206916359072908 \t|| Test Loss: 1.2890924601477107\n",
      "Epoch: 804 \t|| Train Loss: 0.5199041799134536 \t|| Test Loss: 1.28795070015119\n",
      "Epoch: 805 \t|| Train Loss: 0.5191167239196162 \t|| Test Loss: 1.2868089401546696\n",
      "Epoch: 806 \t|| Train Loss: 0.5183292679257788 \t|| Test Loss: 1.2856671801581492\n",
      "Epoch: 807 \t|| Train Loss: 0.5175418119319415 \t|| Test Loss: 1.2845254201616287\n",
      "Epoch: 808 \t|| Train Loss: 0.516754355938104 \t|| Test Loss: 1.2833836601651083\n",
      "Epoch: 809 \t|| Train Loss: 0.5159668999442667 \t|| Test Loss: 1.2822419001685876\n",
      "Epoch: 810 \t|| Train Loss: 0.5151794439504294 \t|| Test Loss: 1.2811001401720672\n",
      "Epoch: 811 \t|| Train Loss: 0.5143919879565919 \t|| Test Loss: 1.2799583801755465\n",
      "Epoch: 812 \t|| Train Loss: 0.5136045319627546 \t|| Test Loss: 1.278816620179026\n",
      "Epoch: 813 \t|| Train Loss: 0.5128170759689172 \t|| Test Loss: 1.2776748601825054\n",
      "Epoch: 814 \t|| Train Loss: 0.5120296199750799 \t|| Test Loss: 1.2765331001859852\n",
      "Epoch: 815 \t|| Train Loss: 0.5112421639812426 \t|| Test Loss: 1.2753913401894648\n",
      "Epoch: 816 \t|| Train Loss: 0.5104547079874052 \t|| Test Loss: 1.274249580192944\n",
      "Epoch: 817 \t|| Train Loss: 0.5096672519935679 \t|| Test Loss: 1.2731078201964237\n",
      "Epoch: 818 \t|| Train Loss: 0.5088797959997304 \t|| Test Loss: 1.2719660601999032\n",
      "Epoch: 819 \t|| Train Loss: 0.5080923400058931 \t|| Test Loss: 1.2708243002033826\n",
      "Epoch: 820 \t|| Train Loss: 0.5073048840120558 \t|| Test Loss: 1.2696825402068623\n",
      "Epoch: 821 \t|| Train Loss: 0.5065174280182184 \t|| Test Loss: 1.268540780210342\n",
      "Epoch: 822 \t|| Train Loss: 0.505729972024381 \t|| Test Loss: 1.267399020213821\n",
      "Epoch: 823 \t|| Train Loss: 0.5049425160305436 \t|| Test Loss: 1.2662572602173008\n",
      "Epoch: 824 \t|| Train Loss: 0.5041550600367063 \t|| Test Loss: 1.2651155002207801\n",
      "Epoch: 825 \t|| Train Loss: 0.503367604042869 \t|| Test Loss: 1.26397374022426\n",
      "Epoch: 826 \t|| Train Loss: 0.5025801480490315 \t|| Test Loss: 1.262831980227739\n",
      "Epoch: 827 \t|| Train Loss: 0.5017926920551942 \t|| Test Loss: 1.2616902202312186\n",
      "Epoch: 828 \t|| Train Loss: 0.5010052360613567 \t|| Test Loss: 1.2605484602346984\n",
      "Epoch: 829 \t|| Train Loss: 0.5002177800675195 \t|| Test Loss: 1.2594067002381777\n",
      "Epoch: 830 \t|| Train Loss: 0.4994303240736821 \t|| Test Loss: 1.258264940241657\n",
      "Epoch: 831 \t|| Train Loss: 0.4986428680798448 \t|| Test Loss: 1.2571231802451366\n",
      "Epoch: 832 \t|| Train Loss: 0.49785541208600737 \t|| Test Loss: 1.2559814202486161\n",
      "Epoch: 833 \t|| Train Loss: 0.4970679560921701 \t|| Test Loss: 1.2548396602520957\n",
      "Epoch: 834 \t|| Train Loss: 0.4962805000983327 \t|| Test Loss: 1.2536979002555753\n",
      "Epoch: 835 \t|| Train Loss: 0.49549304410449535 \t|| Test Loss: 1.2525561402590548\n",
      "Epoch: 836 \t|| Train Loss: 0.4947218110849783 \t|| Test Loss: 1.2514679402624929\n",
      "Epoch: 837 \t|| Train Loss: 0.4940149110910088 \t|| Test Loss: 1.2503797402659307\n",
      "Epoch: 838 \t|| Train Loss: 0.49330801109703926 \t|| Test Loss: 1.2492915402693687\n",
      "Epoch: 839 \t|| Train Loss: 0.4926011111030698 \t|| Test Loss: 1.248203340272807\n",
      "Epoch: 840 \t|| Train Loss: 0.49189421110910025 \t|| Test Loss: 1.2471151402762448\n",
      "Epoch: 841 \t|| Train Loss: 0.49118731111513075 \t|| Test Loss: 1.246026940279683\n",
      "Epoch: 842 \t|| Train Loss: 0.49048041112116125 \t|| Test Loss: 1.2449387402831211\n",
      "Epoch: 843 \t|| Train Loss: 0.4897735111271918 \t|| Test Loss: 1.2438505402865592\n",
      "Epoch: 844 \t|| Train Loss: 0.4890666111332223 \t|| Test Loss: 1.2427623402899972\n",
      "Epoch: 845 \t|| Train Loss: 0.4883597111392528 \t|| Test Loss: 1.2416741402934353\n",
      "Epoch: 846 \t|| Train Loss: 0.4876528111452833 \t|| Test Loss: 1.2405859402968733\n",
      "Epoch: 847 \t|| Train Loss: 0.4869459111513138 \t|| Test Loss: 1.2394977403003113\n",
      "Epoch: 848 \t|| Train Loss: 0.48623901115734436 \t|| Test Loss: 1.2384095403037496\n",
      "Epoch: 849 \t|| Train Loss: 0.48553211116337475 \t|| Test Loss: 1.2373213403071877\n",
      "Epoch: 850 \t|| Train Loss: 0.48482521116940536 \t|| Test Loss: 1.2362331403106257\n",
      "Epoch: 851 \t|| Train Loss: 0.48411831117543586 \t|| Test Loss: 1.2351449403140637\n",
      "Epoch: 852 \t|| Train Loss: 0.48341141118146636 \t|| Test Loss: 1.2340567403175018\n",
      "Epoch: 853 \t|| Train Loss: 0.48270451118749697 \t|| Test Loss: 1.2329685403209398\n",
      "Epoch: 854 \t|| Train Loss: 0.4819976111935273 \t|| Test Loss: 1.2318803403243779\n",
      "Epoch: 855 \t|| Train Loss: 0.4812907111995579 \t|| Test Loss: 1.2307921403278161\n",
      "Epoch: 856 \t|| Train Loss: 0.4805838112055884 \t|| Test Loss: 1.229703940331254\n",
      "Epoch: 857 \t|| Train Loss: 0.47987691121161885 \t|| Test Loss: 1.2286157403346922\n",
      "Epoch: 858 \t|| Train Loss: 0.47917001121764946 \t|| Test Loss: 1.2275275403381303\n",
      "Epoch: 859 \t|| Train Loss: 0.47846311122367996 \t|| Test Loss: 1.226439340341568\n",
      "Epoch: 860 \t|| Train Loss: 0.47775621122971035 \t|| Test Loss: 1.2253511403450061\n",
      "Epoch: 861 \t|| Train Loss: 0.4770493112357409 \t|| Test Loss: 1.2242629403484444\n",
      "Epoch: 862 \t|| Train Loss: 0.47634241124177146 \t|| Test Loss: 1.2231747403518825\n",
      "Epoch: 863 \t|| Train Loss: 0.4756355112478019 \t|| Test Loss: 1.2220865403553203\n",
      "Epoch: 864 \t|| Train Loss: 0.4749286112538324 \t|| Test Loss: 1.2209983403587583\n",
      "Epoch: 865 \t|| Train Loss: 0.4742217112598629 \t|| Test Loss: 1.2199101403621966\n",
      "Epoch: 866 \t|| Train Loss: 0.47351481126589345 \t|| Test Loss: 1.2188219403656348\n",
      "Epoch: 867 \t|| Train Loss: 0.47280791127192395 \t|| Test Loss: 1.217733740369073\n",
      "Epoch: 868 \t|| Train Loss: 0.47210101127795445 \t|| Test Loss: 1.2166455403725107\n",
      "Epoch: 869 \t|| Train Loss: 0.47139411128398495 \t|| Test Loss: 1.2155573403759488\n",
      "Epoch: 870 \t|| Train Loss: 0.4706872112900154 \t|| Test Loss: 1.214469140379387\n",
      "Epoch: 871 \t|| Train Loss: 0.469980311296046 \t|| Test Loss: 1.2133809403828248\n",
      "Epoch: 872 \t|| Train Loss: 0.4692734113020765 \t|| Test Loss: 1.212292740386263\n",
      "Epoch: 873 \t|| Train Loss: 0.468566511308107 \t|| Test Loss: 1.2112045403897012\n",
      "Epoch: 874 \t|| Train Loss: 0.46785961131413745 \t|| Test Loss: 1.2101163403931392\n",
      "Epoch: 875 \t|| Train Loss: 0.46715876806814893 \t|| Test Loss: 1.209082590396691\n",
      "Epoch: 876 \t|| Train Loss: 0.466528143074156 \t|| Test Loss: 1.2080488404002434\n",
      "Epoch: 877 \t|| Train Loss: 0.4658975180801631 \t|| Test Loss: 1.2070150904037953\n",
      "Epoch: 878 \t|| Train Loss: 0.4652668930861701 \t|| Test Loss: 1.2059813404073474\n",
      "Epoch: 879 \t|| Train Loss: 0.46463626809217723 \t|| Test Loss: 1.2049475904108995\n",
      "Epoch: 880 \t|| Train Loss: 0.46400564309818415 \t|| Test Loss: 1.2039138404144516\n",
      "Epoch: 881 \t|| Train Loss: 0.4633750181041913 \t|| Test Loss: 1.2028800904180037\n",
      "Epoch: 882 \t|| Train Loss: 0.46274439311019827 \t|| Test Loss: 1.2018463404215556\n",
      "Epoch: 883 \t|| Train Loss: 0.4621137681162053 \t|| Test Loss: 1.2008125904251077\n",
      "Epoch: 884 \t|| Train Loss: 0.46148314312221234 \t|| Test Loss: 1.1997788404286598\n",
      "Epoch: 885 \t|| Train Loss: 0.46085251812821937 \t|| Test Loss: 1.198745090432212\n",
      "Epoch: 886 \t|| Train Loss: 0.46022189313422646 \t|| Test Loss: 1.197711340435764\n",
      "Epoch: 887 \t|| Train Loss: 0.45959126814023354 \t|| Test Loss: 1.1966775904393159\n",
      "Epoch: 888 \t|| Train Loss: 0.4589606431462405 \t|| Test Loss: 1.1956438404428682\n",
      "Epoch: 889 \t|| Train Loss: 0.4583300181522475 \t|| Test Loss: 1.1946100904464203\n",
      "Epoch: 890 \t|| Train Loss: 0.4576993931582546 \t|| Test Loss: 1.1935763404499722\n",
      "Epoch: 891 \t|| Train Loss: 0.45706876816426156 \t|| Test Loss: 1.1925425904535243\n",
      "Epoch: 892 \t|| Train Loss: 0.4564381431702687 \t|| Test Loss: 1.1915088404570764\n",
      "Epoch: 893 \t|| Train Loss: 0.45580751817627574 \t|| Test Loss: 1.1904750904606285\n",
      "Epoch: 894 \t|| Train Loss: 0.45517689318228277 \t|| Test Loss: 1.1894413404641804\n",
      "Epoch: 895 \t|| Train Loss: 0.45454626818828975 \t|| Test Loss: 1.1884075904677327\n",
      "Epoch: 896 \t|| Train Loss: 0.4539156431942969 \t|| Test Loss: 1.1873738404712848\n",
      "Epoch: 897 \t|| Train Loss: 0.4532850182003038 \t|| Test Loss: 1.1863400904748367\n",
      "Epoch: 898 \t|| Train Loss: 0.45265439320631085 \t|| Test Loss: 1.1853063404783888\n",
      "Epoch: 899 \t|| Train Loss: 0.45202376821231793 \t|| Test Loss: 1.1842725904819411\n",
      "Epoch: 900 \t|| Train Loss: 0.451393143218325 \t|| Test Loss: 1.183238840485493\n",
      "Epoch: 901 \t|| Train Loss: 0.450762518224332 \t|| Test Loss: 1.1822050904890449\n",
      "Epoch: 902 \t|| Train Loss: 0.45013189323033903 \t|| Test Loss: 1.1811713404925972\n",
      "Epoch: 903 \t|| Train Loss: 0.44950126823634606 \t|| Test Loss: 1.1801375904961493\n",
      "Epoch: 904 \t|| Train Loss: 0.44887064324235304 \t|| Test Loss: 1.179103840499701\n",
      "Epoch: 905 \t|| Train Loss: 0.4482400182483602 \t|| Test Loss: 1.1780700905032533\n",
      "Epoch: 906 \t|| Train Loss: 0.4476093932543672 \t|| Test Loss: 1.1770363405068056\n",
      "Epoch: 907 \t|| Train Loss: 0.44697876826037425 \t|| Test Loss: 1.1760025905103575\n",
      "Epoch: 908 \t|| Train Loss: 0.4463481432663813 \t|| Test Loss: 1.1749688405139094\n",
      "Epoch: 909 \t|| Train Loss: 0.44571751827238837 \t|| Test Loss: 1.1739350905174617\n",
      "Epoch: 910 \t|| Train Loss: 0.4450868932783953 \t|| Test Loss: 1.1729013405210136\n",
      "Epoch: 911 \t|| Train Loss: 0.44445626828440243 \t|| Test Loss: 1.1718675905245657\n",
      "Epoch: 912 \t|| Train Loss: 0.4438256432904094 \t|| Test Loss: 1.1708338405281178\n",
      "Epoch: 913 \t|| Train Loss: 0.4431950182964165 \t|| Test Loss: 1.16980009053167\n",
      "Epoch: 914 \t|| Train Loss: 0.4425643933024235 \t|| Test Loss: 1.1687663405352218\n",
      "Epoch: 915 \t|| Train Loss: 0.4419337683084306 \t|| Test Loss: 1.1677325905387739\n",
      "Epoch: 916 \t|| Train Loss: 0.441304504144534 \t|| Test Loss: 1.166754180542264\n",
      "Epoch: 917 \t|| Train Loss: 0.440745843150346 \t|| Test Loss: 1.1657757705457539\n",
      "Epoch: 918 \t|| Train Loss: 0.44018718215615793 \t|| Test Loss: 1.1647973605492439\n",
      "Epoch: 919 \t|| Train Loss: 0.43962852116196993 \t|| Test Loss: 1.1638189505527339\n",
      "Epoch: 920 \t|| Train Loss: 0.43906986016778193 \t|| Test Loss: 1.1628405405562239\n",
      "Epoch: 921 \t|| Train Loss: 0.4385111991735938 \t|| Test Loss: 1.1618621305597139\n",
      "Epoch: 922 \t|| Train Loss: 0.4379525381794059 \t|| Test Loss: 1.1608837205632039\n",
      "Epoch: 923 \t|| Train Loss: 0.43739387718521777 \t|| Test Loss: 1.1599053105666939\n",
      "Epoch: 924 \t|| Train Loss: 0.4368352161910297 \t|| Test Loss: 1.1589269005701837\n",
      "Epoch: 925 \t|| Train Loss: 0.4362765551968417 \t|| Test Loss: 1.1579484905736737\n",
      "Epoch: 926 \t|| Train Loss: 0.4357178942026537 \t|| Test Loss: 1.1569700805771637\n",
      "Epoch: 927 \t|| Train Loss: 0.43515923320846567 \t|| Test Loss: 1.1559916705806537\n",
      "Epoch: 928 \t|| Train Loss: 0.43460057221427767 \t|| Test Loss: 1.1550132605841437\n",
      "Epoch: 929 \t|| Train Loss: 0.4340419112200896 \t|| Test Loss: 1.1540348505876339\n",
      "Epoch: 930 \t|| Train Loss: 0.4334832502259015 \t|| Test Loss: 1.1530564405911237\n",
      "Epoch: 931 \t|| Train Loss: 0.4329245892317136 \t|| Test Loss: 1.1520780305946137\n",
      "Epoch: 932 \t|| Train Loss: 0.4323659282375255 \t|| Test Loss: 1.1510996205981034\n",
      "Epoch: 933 \t|| Train Loss: 0.43180726724333746 \t|| Test Loss: 1.1501212106015937\n",
      "Epoch: 934 \t|| Train Loss: 0.43124860624914946 \t|| Test Loss: 1.1491428006050834\n",
      "Epoch: 935 \t|| Train Loss: 0.4306899452549615 \t|| Test Loss: 1.1481643906085734\n",
      "Epoch: 936 \t|| Train Loss: 0.4301312842607734 \t|| Test Loss: 1.1471859806120634\n",
      "Epoch: 937 \t|| Train Loss: 0.4295726232665854 \t|| Test Loss: 1.1462075706155535\n",
      "Epoch: 938 \t|| Train Loss: 0.42901396227239735 \t|| Test Loss: 1.1452291606190435\n",
      "Epoch: 939 \t|| Train Loss: 0.42845530127820924 \t|| Test Loss: 1.1442507506225335\n",
      "Epoch: 940 \t|| Train Loss: 0.4278966402840213 \t|| Test Loss: 1.1432723406260235\n",
      "Epoch: 941 \t|| Train Loss: 0.4273379792898332 \t|| Test Loss: 1.1422939306295135\n",
      "Epoch: 942 \t|| Train Loss: 0.4267793182956453 \t|| Test Loss: 1.1413155206330035\n",
      "Epoch: 943 \t|| Train Loss: 0.4262206573014572 \t|| Test Loss: 1.1403371106364932\n",
      "Epoch: 944 \t|| Train Loss: 0.42566199630726914 \t|| Test Loss: 1.1393587006399832\n",
      "Epoch: 945 \t|| Train Loss: 0.42510333531308114 \t|| Test Loss: 1.1383802906434732\n",
      "Epoch: 946 \t|| Train Loss: 0.42454467431889303 \t|| Test Loss: 1.1374018806469632\n",
      "Epoch: 947 \t|| Train Loss: 0.423986013324705 \t|| Test Loss: 1.136423470650453\n",
      "Epoch: 948 \t|| Train Loss: 0.4234273523305168 \t|| Test Loss: 1.135445060653943\n",
      "Epoch: 949 \t|| Train Loss: 0.42286869133632876 \t|| Test Loss: 1.134466650657433\n",
      "Epoch: 950 \t|| Train Loss: 0.42231003034214076 \t|| Test Loss: 1.1334882406609228\n",
      "Epoch: 951 \t|| Train Loss: 0.42175136934795265 \t|| Test Loss: 1.1325098306644126\n",
      "Epoch: 952 \t|| Train Loss: 0.42119270835376454 \t|| Test Loss: 1.1315314206679026\n",
      "Epoch: 953 \t|| Train Loss: 0.42063404735957655 \t|| Test Loss: 1.1305530106713926\n",
      "Epoch: 954 \t|| Train Loss: 0.42007538636538844 \t|| Test Loss: 1.1295746006748824\n",
      "Epoch: 955 \t|| Train Loss: 0.4195167253712004 \t|| Test Loss: 1.1285961906783724\n",
      "Epoch: 956 \t|| Train Loss: 0.4189580643770123 \t|| Test Loss: 1.1276177806818624\n",
      "Epoch: 957 \t|| Train Loss: 0.41839940338282416 \t|| Test Loss: 1.1266393706853521\n",
      "Epoch: 958 \t|| Train Loss: 0.41784074238863617 \t|| Test Loss: 1.1256609606888421\n",
      "Epoch: 959 \t|| Train Loss: 0.4172820813944481 \t|| Test Loss: 1.1246825506923321\n",
      "Epoch: 960 \t|| Train Loss: 0.4167518620068301 \t|| Test Loss: 1.1237603706959152\n",
      "Epoch: 961 \t|| Train Loss: 0.41626081801255327 \t|| Test Loss: 1.1228381906994982\n",
      "Epoch: 962 \t|| Train Loss: 0.41576977401827653 \t|| Test Loss: 1.1219160107030814\n",
      "Epoch: 963 \t|| Train Loss: 0.41527873002399984 \t|| Test Loss: 1.1209938307066645\n",
      "Epoch: 964 \t|| Train Loss: 0.414787686029723 \t|| Test Loss: 1.1200716507102477\n",
      "Epoch: 965 \t|| Train Loss: 0.41429664203544625 \t|| Test Loss: 1.119149470713831\n",
      "Epoch: 966 \t|| Train Loss: 0.41380559804116956 \t|| Test Loss: 1.118227290717414\n",
      "Epoch: 967 \t|| Train Loss: 0.4133145540468927 \t|| Test Loss: 1.1173051107209973\n",
      "Epoch: 968 \t|| Train Loss: 0.41282351005261597 \t|| Test Loss: 1.1163829307245803\n",
      "Epoch: 969 \t|| Train Loss: 0.4123324660583392 \t|| Test Loss: 1.1154607507281633\n",
      "Epoch: 970 \t|| Train Loss: 0.4118414220640624 \t|| Test Loss: 1.1145385707317466\n",
      "Epoch: 971 \t|| Train Loss: 0.41135037806978564 \t|| Test Loss: 1.1136163907353298\n",
      "Epoch: 972 \t|| Train Loss: 0.4108593340755088 \t|| Test Loss: 1.1126942107389128\n",
      "Epoch: 973 \t|| Train Loss: 0.4103682900812321 \t|| Test Loss: 1.111772030742496\n",
      "Epoch: 974 \t|| Train Loss: 0.40987724608695536 \t|| Test Loss: 1.1108498507460793\n",
      "Epoch: 975 \t|| Train Loss: 0.4093862020926785 \t|| Test Loss: 1.1099276707496624\n",
      "Epoch: 976 \t|| Train Loss: 0.4088951580984018 \t|| Test Loss: 1.1090054907532454\n",
      "Epoch: 977 \t|| Train Loss: 0.40840411410412497 \t|| Test Loss: 1.1080833107568284\n",
      "Epoch: 978 \t|| Train Loss: 0.40791307010984823 \t|| Test Loss: 1.1071611307604114\n",
      "Epoch: 979 \t|| Train Loss: 0.40742202611557143 \t|| Test Loss: 1.106238950763995\n",
      "Epoch: 980 \t|| Train Loss: 0.4069309821212947 \t|| Test Loss: 1.105316770767578\n",
      "Epoch: 981 \t|| Train Loss: 0.4064399381270179 \t|| Test Loss: 1.104394590771161\n",
      "Epoch: 982 \t|| Train Loss: 0.40594889413274116 \t|| Test Loss: 1.103472410774744\n",
      "Epoch: 983 \t|| Train Loss: 0.40545785013846436 \t|| Test Loss: 1.1025502307783275\n",
      "Epoch: 984 \t|| Train Loss: 0.4049668061441876 \t|| Test Loss: 1.1016280507819105\n",
      "Epoch: 985 \t|| Train Loss: 0.4044757621499109 \t|| Test Loss: 1.1007058707854938\n",
      "Epoch: 986 \t|| Train Loss: 0.40398471815563414 \t|| Test Loss: 1.0997836907890768\n",
      "Epoch: 987 \t|| Train Loss: 0.40349367416135734 \t|| Test Loss: 1.0988615107926598\n",
      "Epoch: 988 \t|| Train Loss: 0.4030026301670805 \t|| Test Loss: 1.0979393307962428\n",
      "Epoch: 989 \t|| Train Loss: 0.4025115861728038 \t|| Test Loss: 1.097017150799826\n",
      "Epoch: 990 \t|| Train Loss: 0.40202054217852695 \t|| Test Loss: 1.0960949708034093\n",
      "Epoch: 991 \t|| Train Loss: 0.4015294981842502 \t|| Test Loss: 1.0951727908069924\n",
      "Epoch: 992 \t|| Train Loss: 0.4010384541899735 \t|| Test Loss: 1.0942506108105756\n",
      "Epoch: 993 \t|| Train Loss: 0.4005474101956967 \t|| Test Loss: 1.0933284308141586\n",
      "Epoch: 994 \t|| Train Loss: 0.40005636620141993 \t|| Test Loss: 1.092406250817742\n",
      "Epoch: 995 \t|| Train Loss: 0.3995653222071432 \t|| Test Loss: 1.0914840708213251\n",
      "Epoch: 996 \t|| Train Loss: 0.3990742782128664 \t|| Test Loss: 1.0905618908249082\n",
      "Epoch: 997 \t|| Train Loss: 0.3985832342185896 \t|| Test Loss: 1.0896397108284912\n",
      "Epoch: 998 \t|| Train Loss: 0.39809219022431286 \t|| Test Loss: 1.0887175308320742\n",
      "Epoch: 999 \t|| Train Loss: 0.39760114623003606 \t|| Test Loss: 1.0877953508356577\n",
      "Epoch: 1000 \t|| Train Loss: 0.3971101022357593 \t|| Test Loss: 1.0868731708392407\n",
      "Epoch: 1001 \t|| Train Loss: 0.3966190582414825 \t|| Test Loss: 1.085950990842824\n",
      "Epoch: 1002 \t|| Train Loss: 0.3961280142472058 \t|| Test Loss: 1.085028810846407\n",
      "Epoch: 1003 \t|| Train Loss: 0.39563697025292904 \t|| Test Loss: 1.08410663084999\n",
      "Epoch: 1004 \t|| Train Loss: 0.39514592625865225 \t|| Test Loss: 1.0831844508535735\n",
      "Epoch: 1005 \t|| Train Loss: 0.39466330894406515 \t|| Test Loss: 1.0823193908570736\n",
      "Epoch: 1006 \t|| Train Loss: 0.3942354929495353 \t|| Test Loss: 1.081454330860574\n",
      "Epoch: 1007 \t|| Train Loss: 0.39380767695500546 \t|| Test Loss: 1.0805892708640745\n",
      "Epoch: 1008 \t|| Train Loss: 0.3933798609604756 \t|| Test Loss: 1.0797242108675744\n",
      "Epoch: 1009 \t|| Train Loss: 0.3929520449659458 \t|| Test Loss: 1.078859150871075\n",
      "Epoch: 1010 \t|| Train Loss: 0.39252422897141603 \t|| Test Loss: 1.0779940908745753\n",
      "Epoch: 1011 \t|| Train Loss: 0.3920964129768862 \t|| Test Loss: 1.0771290308780757\n",
      "Epoch: 1012 \t|| Train Loss: 0.3916685969823564 \t|| Test Loss: 1.076263970881576\n",
      "Epoch: 1013 \t|| Train Loss: 0.3912407809878265 \t|| Test Loss: 1.0753989108850766\n",
      "Epoch: 1014 \t|| Train Loss: 0.39081296499329665 \t|| Test Loss: 1.0745338508885767\n",
      "Epoch: 1015 \t|| Train Loss: 0.39038514899876686 \t|| Test Loss: 1.0736687908920768\n",
      "Epoch: 1016 \t|| Train Loss: 0.38995733300423707 \t|| Test Loss: 1.0728037308955773\n",
      "Epoch: 1017 \t|| Train Loss: 0.3895295170097072 \t|| Test Loss: 1.0719386708990775\n",
      "Epoch: 1018 \t|| Train Loss: 0.38910170101517744 \t|| Test Loss: 1.071073610902578\n",
      "Epoch: 1019 \t|| Train Loss: 0.3886738850206476 \t|| Test Loss: 1.0702085509060784\n",
      "Epoch: 1020 \t|| Train Loss: 0.3882460690261177 \t|| Test Loss: 1.0693434909095787\n",
      "Epoch: 1021 \t|| Train Loss: 0.3878182530315879 \t|| Test Loss: 1.068478430913079\n",
      "Epoch: 1022 \t|| Train Loss: 0.3873904370370581 \t|| Test Loss: 1.0676133709165796\n",
      "Epoch: 1023 \t|| Train Loss: 0.38696262104252827 \t|| Test Loss: 1.06674831092008\n",
      "Epoch: 1024 \t|| Train Loss: 0.3865348050479985 \t|| Test Loss: 1.0658832509235803\n",
      "Epoch: 1025 \t|| Train Loss: 0.38610698905346863 \t|| Test Loss: 1.0650181909270804\n",
      "Epoch: 1026 \t|| Train Loss: 0.38567917305893884 \t|| Test Loss: 1.0641531309305807\n",
      "Epoch: 1027 \t|| Train Loss: 0.385251357064409 \t|| Test Loss: 1.0632880709340813\n",
      "Epoch: 1028 \t|| Train Loss: 0.3848235410698792 \t|| Test Loss: 1.0624230109375816\n",
      "Epoch: 1029 \t|| Train Loss: 0.38439572507534936 \t|| Test Loss: 1.0615579509410817\n",
      "Epoch: 1030 \t|| Train Loss: 0.3839679090808195 \t|| Test Loss: 1.0606928909445823\n",
      "Epoch: 1031 \t|| Train Loss: 0.38354009308628967 \t|| Test Loss: 1.0598278309480826\n",
      "Epoch: 1032 \t|| Train Loss: 0.3831122770917598 \t|| Test Loss: 1.0589627709515832\n",
      "Epoch: 1033 \t|| Train Loss: 0.3826844610972301 \t|| Test Loss: 1.0580977109550833\n",
      "Epoch: 1034 \t|| Train Loss: 0.3822566451027002 \t|| Test Loss: 1.0572326509585837\n",
      "Epoch: 1035 \t|| Train Loss: 0.38182882910817045 \t|| Test Loss: 1.056367590962084\n",
      "Epoch: 1036 \t|| Train Loss: 0.3814010131136406 \t|| Test Loss: 1.0555025309655846\n",
      "Epoch: 1037 \t|| Train Loss: 0.3809731971191107 \t|| Test Loss: 1.0546374709690847\n",
      "Epoch: 1038 \t|| Train Loss: 0.3805453811245809 \t|| Test Loss: 1.0537724109725852\n",
      "Epoch: 1039 \t|| Train Loss: 0.3801175651300511 \t|| Test Loss: 1.0529073509760853\n",
      "Epoch: 1040 \t|| Train Loss: 0.3796897491355212 \t|| Test Loss: 1.052042290979586\n",
      "Epoch: 1041 \t|| Train Loss: 0.3792619331409915 \t|| Test Loss: 1.051177230983086\n",
      "Epoch: 1042 \t|| Train Loss: 0.37883411714646165 \t|| Test Loss: 1.0503121709865861\n",
      "Epoch: 1043 \t|| Train Loss: 0.37840630115193175 \t|| Test Loss: 1.0494471109900867\n",
      "Epoch: 1044 \t|| Train Loss: 0.37797848515740196 \t|| Test Loss: 1.048582050993587\n",
      "Epoch: 1045 \t|| Train Loss: 0.37755066916287217 \t|| Test Loss: 1.0477169909970876\n",
      "Epoch: 1046 \t|| Train Loss: 0.3771228531683423 \t|| Test Loss: 1.0468519310005877\n",
      "Epoch: 1047 \t|| Train Loss: 0.37669503717381253 \t|| Test Loss: 1.045986871004088\n",
      "Epoch: 1048 \t|| Train Loss: 0.3762672211792827 \t|| Test Loss: 1.0451218110075886\n",
      "Epoch: 1049 \t|| Train Loss: 0.3758394051847528 \t|| Test Loss: 1.0442567510110887\n",
      "Epoch: 1050 \t|| Train Loss: 0.375411589190223 \t|| Test Loss: 1.0433916910145893\n",
      "Epoch: 1051 \t|| Train Loss: 0.37498377319569315 \t|| Test Loss: 1.0425266310180894\n",
      "Epoch: 1052 \t|| Train Loss: 0.37455595720116336 \t|| Test Loss: 1.04166157102159\n",
      "Epoch: 1053 \t|| Train Loss: 0.37412814120663357 \t|| Test Loss: 1.0407965110250905\n",
      "Epoch: 1054 \t|| Train Loss: 0.37372832095679 \t|| Test Loss: 1.0399894610283318\n",
      "Epoch: 1055 \t|| Train Loss: 0.3733592959618538 \t|| Test Loss: 1.039182411031573\n",
      "Epoch: 1056 \t|| Train Loss: 0.37299027096691767 \t|| Test Loss: 1.0383753610348148\n",
      "Epoch: 1057 \t|| Train Loss: 0.37262124597198143 \t|| Test Loss: 1.0375683110380565\n",
      "Epoch: 1058 \t|| Train Loss: 0.3722522209770453 \t|| Test Loss: 1.0367612610412977\n",
      "Epoch: 1059 \t|| Train Loss: 0.37188319598210906 \t|| Test Loss: 1.0359542110445392\n",
      "Epoch: 1060 \t|| Train Loss: 0.37151417098717293 \t|| Test Loss: 1.0351471610477805\n",
      "Epoch: 1061 \t|| Train Loss: 0.3711451459922368 \t|| Test Loss: 1.034340111051022\n",
      "Epoch: 1062 \t|| Train Loss: 0.37077612099730056 \t|| Test Loss: 1.0335330610542635\n",
      "Epoch: 1063 \t|| Train Loss: 0.37040709600236443 \t|| Test Loss: 1.032726011057505\n",
      "Epoch: 1064 \t|| Train Loss: 0.3700380710074283 \t|| Test Loss: 1.0319189610607464\n",
      "Epoch: 1065 \t|| Train Loss: 0.3696690460124921 \t|| Test Loss: 1.031111911063988\n",
      "Epoch: 1066 \t|| Train Loss: 0.36930002101755594 \t|| Test Loss: 1.0303048610672296\n",
      "Epoch: 1067 \t|| Train Loss: 0.3689309960226198 \t|| Test Loss: 1.0294978110704711\n",
      "Epoch: 1068 \t|| Train Loss: 0.3685619710276836 \t|| Test Loss: 1.0286907610737124\n",
      "Epoch: 1069 \t|| Train Loss: 0.3681929460327475 \t|| Test Loss: 1.0278837110769536\n",
      "Epoch: 1070 \t|| Train Loss: 0.36782392103781125 \t|| Test Loss: 1.0270766610801954\n",
      "Epoch: 1071 \t|| Train Loss: 0.36745489604287507 \t|| Test Loss: 1.0262696110834368\n",
      "Epoch: 1072 \t|| Train Loss: 0.3670858710479389 \t|| Test Loss: 1.025462561086678\n",
      "Epoch: 1073 \t|| Train Loss: 0.36671684605300275 \t|| Test Loss: 1.0246555110899198\n",
      "Epoch: 1074 \t|| Train Loss: 0.3663478210580666 \t|| Test Loss: 1.023848461093161\n",
      "Epoch: 1075 \t|| Train Loss: 0.3659787960631304 \t|| Test Loss: 1.0230414110964028\n",
      "Epoch: 1076 \t|| Train Loss: 0.36560977106819426 \t|| Test Loss: 1.022234361099644\n",
      "Epoch: 1077 \t|| Train Loss: 0.3652407460732581 \t|| Test Loss: 1.0214273111028855\n",
      "Epoch: 1078 \t|| Train Loss: 0.3648717210783219 \t|| Test Loss: 1.020620261106127\n",
      "Epoch: 1079 \t|| Train Loss: 0.3645026960833857 \t|| Test Loss: 1.0198132111093685\n",
      "Epoch: 1080 \t|| Train Loss: 0.3641336710884496 \t|| Test Loss: 1.01900616111261\n",
      "Epoch: 1081 \t|| Train Loss: 0.36376464609351344 \t|| Test Loss: 1.0181991111158515\n",
      "Epoch: 1082 \t|| Train Loss: 0.36339562109857726 \t|| Test Loss: 1.017392061119093\n",
      "Epoch: 1083 \t|| Train Loss: 0.3630265961036411 \t|| Test Loss: 1.0165850111223345\n",
      "Epoch: 1084 \t|| Train Loss: 0.3626575711087049 \t|| Test Loss: 1.015777961125576\n",
      "Epoch: 1085 \t|| Train Loss: 0.3622885461137687 \t|| Test Loss: 1.0149709111288174\n",
      "Epoch: 1086 \t|| Train Loss: 0.3619195211188325 \t|| Test Loss: 1.014163861132059\n",
      "Epoch: 1087 \t|| Train Loss: 0.36155049612389634 \t|| Test Loss: 1.0133568111353004\n",
      "Epoch: 1088 \t|| Train Loss: 0.36118147112896015 \t|| Test Loss: 1.012549761138542\n",
      "Epoch: 1089 \t|| Train Loss: 0.36081244613402397 \t|| Test Loss: 1.0117427111417832\n",
      "Epoch: 1090 \t|| Train Loss: 0.36044342113908784 \t|| Test Loss: 1.0109356611450246\n",
      "Epoch: 1091 \t|| Train Loss: 0.3600743961441517 \t|| Test Loss: 1.0101286111482661\n",
      "Epoch: 1092 \t|| Train Loss: 0.3597053711492155 \t|| Test Loss: 1.0093215611515078\n",
      "Epoch: 1093 \t|| Train Loss: 0.3593363461542794 \t|| Test Loss: 1.008514511154749\n",
      "Epoch: 1094 \t|| Train Loss: 0.3589673211593432 \t|| Test Loss: 1.0077074611579906\n",
      "Epoch: 1095 \t|| Train Loss: 0.3585982961644071 \t|| Test Loss: 1.006900411161232\n",
      "Epoch: 1096 \t|| Train Loss: 0.35822927116947084 \t|| Test Loss: 1.0060933611644736\n",
      "Epoch: 1097 \t|| Train Loss: 0.3578602461745347 \t|| Test Loss: 1.005286311167715\n",
      "Epoch: 1098 \t|| Train Loss: 0.35749122117959853 \t|| Test Loss: 1.0044792611709563\n",
      "Epoch: 1099 \t|| Train Loss: 0.35712219618466234 \t|| Test Loss: 1.003672211174198\n",
      "Epoch: 1100 \t|| Train Loss: 0.35675317118972616 \t|| Test Loss: 1.0028651611774395\n",
      "Epoch: 1101 \t|| Train Loss: 0.35638414619479 \t|| Test Loss: 1.002058111180681\n",
      "Epoch: 1102 \t|| Train Loss: 0.3560151211998538 \t|| Test Loss: 1.0012510611839223\n",
      "Epoch: 1103 \t|| Train Loss: 0.35564609620491766 \t|| Test Loss: 1.0004440111871638\n",
      "Epoch: 1104 \t|| Train Loss: 0.35527707120998153 \t|| Test Loss: 0.9996369611904055\n",
      "Epoch: 1105 \t|| Train Loss: 0.354920292458502 \t|| Test Loss: 0.9988888111938745\n",
      "Epoch: 1106 \t|| Train Loss: 0.35460556746351884 \t|| Test Loss: 0.9981406611973442\n",
      "Epoch: 1107 \t|| Train Loss: 0.3542908424685357 \t|| Test Loss: 0.9973925112008134\n",
      "Epoch: 1108 \t|| Train Loss: 0.3539761174735526 \t|| Test Loss: 0.9966443612042829\n",
      "Epoch: 1109 \t|| Train Loss: 0.3536613924785694 \t|| Test Loss: 0.9958962112077522\n",
      "Epoch: 1110 \t|| Train Loss: 0.35334666748358634 \t|| Test Loss: 0.9951480612112217\n",
      "Epoch: 1111 \t|| Train Loss: 0.35303194248860315 \t|| Test Loss: 0.9943999112146908\n",
      "Epoch: 1112 \t|| Train Loss: 0.35271721749362 \t|| Test Loss: 0.9936517612181603\n",
      "Epoch: 1113 \t|| Train Loss: 0.3524024924986369 \t|| Test Loss: 0.9929036112216298\n",
      "Epoch: 1114 \t|| Train Loss: 0.3520877675036538 \t|| Test Loss: 0.9921554612250993\n",
      "Epoch: 1115 \t|| Train Loss: 0.3517730425086706 \t|| Test Loss: 0.9914073112285686\n",
      "Epoch: 1116 \t|| Train Loss: 0.3514583175136875 \t|| Test Loss: 0.990659161232038\n",
      "Epoch: 1117 \t|| Train Loss: 0.3511435925187044 \t|| Test Loss: 0.9899110112355075\n",
      "Epoch: 1118 \t|| Train Loss: 0.3508288675237212 \t|| Test Loss: 0.9891628612389768\n",
      "Epoch: 1119 \t|| Train Loss: 0.35051414252873814 \t|| Test Loss: 0.9884147112424463\n",
      "Epoch: 1120 \t|| Train Loss: 0.35019941753375494 \t|| Test Loss: 0.9876665612459157\n",
      "Epoch: 1121 \t|| Train Loss: 0.34988469253877186 \t|| Test Loss: 0.9869184112493852\n",
      "Epoch: 1122 \t|| Train Loss: 0.34956996754378866 \t|| Test Loss: 0.9861702612528542\n",
      "Epoch: 1123 \t|| Train Loss: 0.3492552425488055 \t|| Test Loss: 0.9854221112563237\n",
      "Epoch: 1124 \t|| Train Loss: 0.34894051755382244 \t|| Test Loss: 0.9846739612597932\n",
      "Epoch: 1125 \t|| Train Loss: 0.3486257925588393 \t|| Test Loss: 0.9839258112632626\n",
      "Epoch: 1126 \t|| Train Loss: 0.3483110675638561 \t|| Test Loss: 0.9831776612667319\n",
      "Epoch: 1127 \t|| Train Loss: 0.3479963425688731 \t|| Test Loss: 0.9824295112702014\n",
      "Epoch: 1128 \t|| Train Loss: 0.3476816175738898 \t|| Test Loss: 0.9816813612736708\n",
      "Epoch: 1129 \t|| Train Loss: 0.34736689257890674 \t|| Test Loss: 0.9809332112771403\n",
      "Epoch: 1130 \t|| Train Loss: 0.3470521675839236 \t|| Test Loss: 0.9801850612806096\n",
      "Epoch: 1131 \t|| Train Loss: 0.3467374425889404 \t|| Test Loss: 0.9794369112840791\n",
      "Epoch: 1132 \t|| Train Loss: 0.3464227175939573 \t|| Test Loss: 0.9786887612875483\n",
      "Epoch: 1133 \t|| Train Loss: 0.3461079925989742 \t|| Test Loss: 0.9779406112910178\n",
      "Epoch: 1134 \t|| Train Loss: 0.34579326760399104 \t|| Test Loss: 0.9771924612944872\n",
      "Epoch: 1135 \t|| Train Loss: 0.3454785426090079 \t|| Test Loss: 0.9764443112979565\n",
      "Epoch: 1136 \t|| Train Loss: 0.3451638176140248 \t|| Test Loss: 0.975696161301426\n",
      "Epoch: 1137 \t|| Train Loss: 0.3448490926190416 \t|| Test Loss: 0.9749480113048954\n",
      "Epoch: 1138 \t|| Train Loss: 0.3445343676240585 \t|| Test Loss: 0.9741998613083649\n",
      "Epoch: 1139 \t|| Train Loss: 0.3442196426290754 \t|| Test Loss: 0.9734517113118342\n",
      "Epoch: 1140 \t|| Train Loss: 0.34390491763409226 \t|| Test Loss: 0.9727035613153034\n",
      "Epoch: 1141 \t|| Train Loss: 0.3435901926391091 \t|| Test Loss: 0.9719554113187729\n",
      "Epoch: 1142 \t|| Train Loss: 0.343275467644126 \t|| Test Loss: 0.9712072613222423\n",
      "Epoch: 1143 \t|| Train Loss: 0.3429607426491429 \t|| Test Loss: 0.9704591113257116\n",
      "Epoch: 1144 \t|| Train Loss: 0.3426460176541597 \t|| Test Loss: 0.9697109613291811\n",
      "Epoch: 1145 \t|| Train Loss: 0.3423312926591765 \t|| Test Loss: 0.9689628113326505\n",
      "Epoch: 1146 \t|| Train Loss: 0.34201656766419336 \t|| Test Loss: 0.9682146613361198\n",
      "Epoch: 1147 \t|| Train Loss: 0.3417018426692103 \t|| Test Loss: 0.9674665113395893\n",
      "Epoch: 1148 \t|| Train Loss: 0.34138711767422714 \t|| Test Loss: 0.9667183613430586\n",
      "Epoch: 1149 \t|| Train Loss: 0.341072392679244 \t|| Test Loss: 0.965970211346528\n",
      "Epoch: 1150 \t|| Train Loss: 0.3407576676842609 \t|| Test Loss: 0.9652220613499974\n",
      "Epoch: 1151 \t|| Train Loss: 0.34044294268927777 \t|| Test Loss: 0.9644739113534669\n",
      "Epoch: 1152 \t|| Train Loss: 0.3401282176942946 \t|| Test Loss: 0.9637257613569362\n",
      "Epoch: 1153 \t|| Train Loss: 0.33981349269931144 \t|| Test Loss: 0.9629776113604057\n",
      "Epoch: 1154 \t|| Train Loss: 0.3394987677043283 \t|| Test Loss: 0.9622294613638751\n",
      "Epoch: 1155 \t|| Train Loss: 0.3391840427093452 \t|| Test Loss: 0.9614813113673444\n",
      "Epoch: 1156 \t|| Train Loss: 0.3388693177143621 \t|| Test Loss: 0.9607331613708137\n",
      "Epoch: 1157 \t|| Train Loss: 0.3385545927193789 \t|| Test Loss: 0.9599850113742832\n",
      "Epoch: 1158 \t|| Train Loss: 0.3382398677243958 \t|| Test Loss: 0.9592368613777527\n",
      "Epoch: 1159 \t|| Train Loss: 0.33792514272941265 \t|| Test Loss: 0.958488711381222\n",
      "Epoch: 1160 \t|| Train Loss: 0.33761373503933967 \t|| Test Loss: 0.9578003513847433\n",
      "Epoch: 1161 \t|| Train Loss: 0.3373487590441532 \t|| Test Loss: 0.9571119913882645\n",
      "Epoch: 1162 \t|| Train Loss: 0.33708378304896663 \t|| Test Loss: 0.9564236313917858\n",
      "Epoch: 1163 \t|| Train Loss: 0.3368188070537802 \t|| Test Loss: 0.9557352713953069\n",
      "Epoch: 1164 \t|| Train Loss: 0.3365538310585937 \t|| Test Loss: 0.9550469113988281\n",
      "Epoch: 1165 \t|| Train Loss: 0.33628885506340717 \t|| Test Loss: 0.9543585514023494\n",
      "Epoch: 1166 \t|| Train Loss: 0.3360238790682207 \t|| Test Loss: 0.9536701914058705\n",
      "Epoch: 1167 \t|| Train Loss: 0.3357589030730341 \t|| Test Loss: 0.9529818314093917\n",
      "Epoch: 1168 \t|| Train Loss: 0.33549392707784764 \t|| Test Loss: 0.952293471412913\n",
      "Epoch: 1169 \t|| Train Loss: 0.3352289510826611 \t|| Test Loss: 0.9516051114164341\n",
      "Epoch: 1170 \t|| Train Loss: 0.3349639750874746 \t|| Test Loss: 0.9509167514199554\n",
      "Epoch: 1171 \t|| Train Loss: 0.3346989990922881 \t|| Test Loss: 0.9502283914234768\n",
      "Epoch: 1172 \t|| Train Loss: 0.3344340230971016 \t|| Test Loss: 0.9495400314269979\n",
      "Epoch: 1173 \t|| Train Loss: 0.3341690471019151 \t|| Test Loss: 0.9488516714305192\n",
      "Epoch: 1174 \t|| Train Loss: 0.3339040711067286 \t|| Test Loss: 0.9481633114340404\n",
      "Epoch: 1175 \t|| Train Loss: 0.3336390951115421 \t|| Test Loss: 0.9474749514375617\n",
      "Epoch: 1176 \t|| Train Loss: 0.3333741191163555 \t|| Test Loss: 0.9467865914410828\n",
      "Epoch: 1177 \t|| Train Loss: 0.33310914312116907 \t|| Test Loss: 0.946098231444604\n",
      "Epoch: 1178 \t|| Train Loss: 0.3328441671259826 \t|| Test Loss: 0.9454098714481253\n",
      "Epoch: 1179 \t|| Train Loss: 0.33257919113079604 \t|| Test Loss: 0.9447215114516464\n",
      "Epoch: 1180 \t|| Train Loss: 0.33231421513560955 \t|| Test Loss: 0.9440331514551676\n",
      "Epoch: 1181 \t|| Train Loss: 0.33204923914042306 \t|| Test Loss: 0.9433447914586889\n",
      "Epoch: 1182 \t|| Train Loss: 0.3317842631452365 \t|| Test Loss: 0.94265643146221\n",
      "Epoch: 1183 \t|| Train Loss: 0.33151928715005 \t|| Test Loss: 0.9419680714657312\n",
      "Epoch: 1184 \t|| Train Loss: 0.3312543111548635 \t|| Test Loss: 0.9412797114692525\n",
      "Epoch: 1185 \t|| Train Loss: 0.330989335159677 \t|| Test Loss: 0.9405913514727736\n",
      "Epoch: 1186 \t|| Train Loss: 0.33072435916449044 \t|| Test Loss: 0.9399029914762949\n",
      "Epoch: 1187 \t|| Train Loss: 0.330459383169304 \t|| Test Loss: 0.9392146314798161\n",
      "Epoch: 1188 \t|| Train Loss: 0.3301944071741175 \t|| Test Loss: 0.9385262714833373\n",
      "Epoch: 1189 \t|| Train Loss: 0.329929431178931 \t|| Test Loss: 0.9378379114868587\n",
      "Epoch: 1190 \t|| Train Loss: 0.3296644551837445 \t|| Test Loss: 0.9371495514903799\n",
      "Epoch: 1191 \t|| Train Loss: 0.329399479188558 \t|| Test Loss: 0.9364611914939012\n",
      "Epoch: 1192 \t|| Train Loss: 0.32913450319337145 \t|| Test Loss: 0.9357728314974223\n",
      "Epoch: 1193 \t|| Train Loss: 0.328869527198185 \t|| Test Loss: 0.9350844715009435\n",
      "Epoch: 1194 \t|| Train Loss: 0.3286045512029985 \t|| Test Loss: 0.9343961115044646\n",
      "Epoch: 1195 \t|| Train Loss: 0.32833957520781193 \t|| Test Loss: 0.9337077515079859\n",
      "Epoch: 1196 \t|| Train Loss: 0.3280745992126255 \t|| Test Loss: 0.9330193915115071\n",
      "Epoch: 1197 \t|| Train Loss: 0.3278096232174389 \t|| Test Loss: 0.9323310315150284\n",
      "Epoch: 1198 \t|| Train Loss: 0.32754464722225246 \t|| Test Loss: 0.9316426715185496\n",
      "Epoch: 1199 \t|| Train Loss: 0.3272796712270659 \t|| Test Loss: 0.9309543115220708\n",
      "Epoch: 1200 \t|| Train Loss: 0.3270146952318794 \t|| Test Loss: 0.930265951525592\n",
      "Epoch: 1201 \t|| Train Loss: 0.32674971923669294 \t|| Test Loss: 0.9295775915291132\n",
      "Epoch: 1202 \t|| Train Loss: 0.32648474324150645 \t|| Test Loss: 0.9288892315326344\n",
      "Epoch: 1203 \t|| Train Loss: 0.32621976724631996 \t|| Test Loss: 0.9282008715361556\n",
      "Epoch: 1204 \t|| Train Loss: 0.32595479125113336 \t|| Test Loss: 0.9275125115396768\n",
      "Epoch: 1205 \t|| Train Loss: 0.32568981525594687 \t|| Test Loss: 0.9268241515431981\n",
      "Epoch: 1206 \t|| Train Loss: 0.3254248392607604 \t|| Test Loss: 0.9261357915467192\n",
      "Epoch: 1207 \t|| Train Loss: 0.3251598632655739 \t|| Test Loss: 0.9254474315502405\n",
      "Epoch: 1208 \t|| Train Loss: 0.32489488727038734 \t|| Test Loss: 0.9247590715537617\n",
      "Epoch: 1209 \t|| Train Loss: 0.3246299112752008 \t|| Test Loss: 0.9240707115572828\n",
      "Epoch: 1210 \t|| Train Loss: 0.3243649352800143 \t|| Test Loss: 0.9233823515608041\n",
      "Epoch: 1211 \t|| Train Loss: 0.3240999592848278 \t|| Test Loss: 0.9226939915643253\n",
      "Epoch: 1212 \t|| Train Loss: 0.32383498328964133 \t|| Test Loss: 0.9220056315678467\n",
      "Epoch: 1213 \t|| Train Loss: 0.32357000729445484 \t|| Test Loss: 0.9213172715713679\n",
      "Epoch: 1214 \t|| Train Loss: 0.32330503129926835 \t|| Test Loss: 0.9206289115748891\n",
      "Epoch: 1215 \t|| Train Loss: 0.32304005530408186 \t|| Test Loss: 0.9199405515784103\n",
      "Epoch: 1216 \t|| Train Loss: 0.3227750793088953 \t|| Test Loss: 0.9192521915819313\n",
      "Epoch: 1217 \t|| Train Loss: 0.32251010331370883 \t|| Test Loss: 0.9185638315854527\n",
      "Epoch: 1218 \t|| Train Loss: 0.32224512731852234 \t|| Test Loss: 0.917875471588974\n",
      "Epoch: 1219 \t|| Train Loss: 0.32198015132333574 \t|| Test Loss: 0.9171871115924951\n",
      "Epoch: 1220 \t|| Train Loss: 0.32172109368532326 \t|| Test Loss: 0.9165594315958921\n",
      "Epoch: 1221 \t|| Train Loss: 0.32150124968979105 \t|| Test Loss: 0.9159317515992891\n",
      "Epoch: 1222 \t|| Train Loss: 0.3212814056942589 \t|| Test Loss: 0.915304071602686\n",
      "Epoch: 1223 \t|| Train Loss: 0.3210615616987268 \t|| Test Loss: 0.9146763916060827\n",
      "Epoch: 1224 \t|| Train Loss: 0.32084171770319464 \t|| Test Loss: 0.91404871160948\n",
      "Epoch: 1225 \t|| Train Loss: 0.32062187370766243 \t|| Test Loss: 0.9134210316128769\n",
      "Epoch: 1226 \t|| Train Loss: 0.3204020297121303 \t|| Test Loss: 0.9127933516162738\n",
      "Epoch: 1227 \t|| Train Loss: 0.3201821857165982 \t|| Test Loss: 0.9121656716196707\n",
      "Epoch: 1228 \t|| Train Loss: 0.31996234172106597 \t|| Test Loss: 0.9115379916230676\n",
      "Epoch: 1229 \t|| Train Loss: 0.3197424977255338 \t|| Test Loss: 0.9109103116264643\n",
      "Epoch: 1230 \t|| Train Loss: 0.31952265373000166 \t|| Test Loss: 0.9102826316298614\n",
      "Epoch: 1231 \t|| Train Loss: 0.3193028097344695 \t|| Test Loss: 0.9096549516332585\n",
      "Epoch: 1232 \t|| Train Loss: 0.3190829657389373 \t|| Test Loss: 0.9090272716366554\n",
      "Epoch: 1233 \t|| Train Loss: 0.3188631217434052 \t|| Test Loss: 0.9083995916400524\n",
      "Epoch: 1234 \t|| Train Loss: 0.31864327774787304 \t|| Test Loss: 0.9077719116434493\n",
      "Epoch: 1235 \t|| Train Loss: 0.31842343375234095 \t|| Test Loss: 0.9071442316468461\n",
      "Epoch: 1236 \t|| Train Loss: 0.3182035897568088 \t|| Test Loss: 0.9065165516502433\n",
      "Epoch: 1237 \t|| Train Loss: 0.3179837457612766 \t|| Test Loss: 0.90588887165364\n",
      "Epoch: 1238 \t|| Train Loss: 0.3177639017657444 \t|| Test Loss: 0.905261191657037\n",
      "Epoch: 1239 \t|| Train Loss: 0.3175440577702123 \t|| Test Loss: 0.9046335116604339\n",
      "Epoch: 1240 \t|| Train Loss: 0.3173242137746801 \t|| Test Loss: 0.9040058316638309\n",
      "Epoch: 1241 \t|| Train Loss: 0.3171043697791479 \t|| Test Loss: 0.9033781516672278\n",
      "Epoch: 1242 \t|| Train Loss: 0.31688452578361576 \t|| Test Loss: 0.9027504716706247\n",
      "Epoch: 1243 \t|| Train Loss: 0.3166646817880836 \t|| Test Loss: 0.9021227916740215\n",
      "Epoch: 1244 \t|| Train Loss: 0.3164448377925515 \t|| Test Loss: 0.9014951116774187\n",
      "Epoch: 1245 \t|| Train Loss: 0.31622499379701935 \t|| Test Loss: 0.9008674316808156\n",
      "Epoch: 1246 \t|| Train Loss: 0.3160051498014872 \t|| Test Loss: 0.9002397516842124\n",
      "Epoch: 1247 \t|| Train Loss: 0.315785305805955 \t|| Test Loss: 0.8996120716876096\n",
      "Epoch: 1248 \t|| Train Loss: 0.31556546181042283 \t|| Test Loss: 0.8989843916910065\n",
      "Epoch: 1249 \t|| Train Loss: 0.3153456178148907 \t|| Test Loss: 0.8983567116944033\n",
      "Epoch: 1250 \t|| Train Loss: 0.3151257738193586 \t|| Test Loss: 0.8977290316978003\n",
      "Epoch: 1251 \t|| Train Loss: 0.31490592982382637 \t|| Test Loss: 0.897101351701197\n",
      "Epoch: 1252 \t|| Train Loss: 0.31468608582829416 \t|| Test Loss: 0.8964736717045941\n",
      "Epoch: 1253 \t|| Train Loss: 0.31446624183276206 \t|| Test Loss: 0.8958459917079911\n",
      "Epoch: 1254 \t|| Train Loss: 0.31424639783722996 \t|| Test Loss: 0.8952183117113881\n",
      "Epoch: 1255 \t|| Train Loss: 0.31402655384169775 \t|| Test Loss: 0.894590631714785\n",
      "Epoch: 1256 \t|| Train Loss: 0.3138067098461656 \t|| Test Loss: 0.893962951718182\n",
      "Epoch: 1257 \t|| Train Loss: 0.31358686585063344 \t|| Test Loss: 0.8933352717215788\n",
      "Epoch: 1258 \t|| Train Loss: 0.31336702185510135 \t|| Test Loss: 0.8927075917249759\n",
      "Epoch: 1259 \t|| Train Loss: 0.31314717785956914 \t|| Test Loss: 0.8920799117283729\n",
      "Epoch: 1260 \t|| Train Loss: 0.31292733386403704 \t|| Test Loss: 0.8914522317317697\n",
      "Epoch: 1261 \t|| Train Loss: 0.3127074898685048 \t|| Test Loss: 0.8908245517351666\n",
      "Epoch: 1262 \t|| Train Loss: 0.3124876458729727 \t|| Test Loss: 0.8901968717385635\n",
      "Epoch: 1263 \t|| Train Loss: 0.3122678018774405 \t|| Test Loss: 0.8895691917419605\n",
      "Epoch: 1264 \t|| Train Loss: 0.3120479578819083 \t|| Test Loss: 0.8889415117453574\n",
      "Epoch: 1265 \t|| Train Loss: 0.3118281138863762 \t|| Test Loss: 0.8883138317487542\n",
      "Epoch: 1266 \t|| Train Loss: 0.31160826989084406 \t|| Test Loss: 0.8876861517521514\n",
      "Epoch: 1267 \t|| Train Loss: 0.31138842589531185 \t|| Test Loss: 0.8870584717555483\n",
      "Epoch: 1268 \t|| Train Loss: 0.3111685818997797 \t|| Test Loss: 0.8864307917589451\n",
      "Epoch: 1269 \t|| Train Loss: 0.3109487379042476 \t|| Test Loss: 0.8858031117623423\n",
      "Epoch: 1270 \t|| Train Loss: 0.31072889390871544 \t|| Test Loss: 0.8851754317657392\n",
      "Epoch: 1271 \t|| Train Loss: 0.31050904991318323 \t|| Test Loss: 0.8845477517691359\n",
      "Epoch: 1272 \t|| Train Loss: 0.3102892059176511 \t|| Test Loss: 0.883920071772533\n",
      "Epoch: 1273 \t|| Train Loss: 0.310069361922119 \t|| Test Loss: 0.8832923917759299\n",
      "Epoch: 1274 \t|| Train Loss: 0.3098495179265868 \t|| Test Loss: 0.8826647117793268\n",
      "Epoch: 1275 \t|| Train Loss: 0.30962967393105456 \t|| Test Loss: 0.8820370317827237\n",
      "Epoch: 1276 \t|| Train Loss: 0.30940982993552246 \t|| Test Loss: 0.8814093517861208\n",
      "Epoch: 1277 \t|| Train Loss: 0.3091899859399903 \t|| Test Loss: 0.8807816717895177\n",
      "Epoch: 1278 \t|| Train Loss: 0.3089701419444581 \t|| Test Loss: 0.8801539917929146\n",
      "Epoch: 1279 \t|| Train Loss: 0.308750297948926 \t|| Test Loss: 0.8795263117963114\n",
      "Epoch: 1280 \t|| Train Loss: 0.30853045395339385 \t|| Test Loss: 0.8788986317997086\n",
      "Epoch: 1281 \t|| Train Loss: 0.30831060995786175 \t|| Test Loss: 0.8782709518031055\n",
      "Epoch: 1282 \t|| Train Loss: 0.30809076596232954 \t|| Test Loss: 0.8776432718065024\n",
      "Epoch: 1283 \t|| Train Loss: 0.3078709219667974 \t|| Test Loss: 0.8770155918098993\n",
      "Epoch: 1284 \t|| Train Loss: 0.30765107797126523 \t|| Test Loss: 0.8763879118132962\n",
      "Epoch: 1285 \t|| Train Loss: 0.3074312339757331 \t|| Test Loss: 0.8757602318166932\n",
      "Epoch: 1286 \t|| Train Loss: 0.30722876537973764 \t|| Test Loss: 0.8751941218197897\n",
      "Epoch: 1287 \t|| Train Loss: 0.30704936438373415 \t|| Test Loss: 0.8746280118228862\n",
      "Epoch: 1288 \t|| Train Loss: 0.30686996338773076 \t|| Test Loss: 0.8740619018259828\n",
      "Epoch: 1289 \t|| Train Loss: 0.30669056239172726 \t|| Test Loss: 0.8734957918290794\n",
      "Epoch: 1290 \t|| Train Loss: 0.3065111613957238 \t|| Test Loss: 0.872929681832176\n",
      "Epoch: 1291 \t|| Train Loss: 0.3063317603997204 \t|| Test Loss: 0.8723635718352725\n",
      "Epoch: 1292 \t|| Train Loss: 0.30615235940371693 \t|| Test Loss: 0.871797461838369\n",
      "Epoch: 1293 \t|| Train Loss: 0.30597295840771344 \t|| Test Loss: 0.8712313518414657\n",
      "Epoch: 1294 \t|| Train Loss: 0.30579355741171 \t|| Test Loss: 0.8706652418445622\n",
      "Epoch: 1295 \t|| Train Loss: 0.30561415641570655 \t|| Test Loss: 0.8700991318476587\n",
      "Epoch: 1296 \t|| Train Loss: 0.3054347554197031 \t|| Test Loss: 0.8695330218507552\n",
      "Epoch: 1297 \t|| Train Loss: 0.30525535442369967 \t|| Test Loss: 0.8689669118538518\n",
      "Epoch: 1298 \t|| Train Loss: 0.3050759534276962 \t|| Test Loss: 0.8684008018569485\n",
      "Epoch: 1299 \t|| Train Loss: 0.3048965524316928 \t|| Test Loss: 0.867834691860045\n",
      "Epoch: 1300 \t|| Train Loss: 0.3047171514356893 \t|| Test Loss: 0.8672685818631415\n",
      "Epoch: 1301 \t|| Train Loss: 0.3045377504396859 \t|| Test Loss: 0.8667024718662381\n",
      "Epoch: 1302 \t|| Train Loss: 0.3043583494436824 \t|| Test Loss: 0.8661363618693347\n",
      "Epoch: 1303 \t|| Train Loss: 0.3041789484476789 \t|| Test Loss: 0.8655702518724311\n",
      "Epoch: 1304 \t|| Train Loss: 0.30399954745167546 \t|| Test Loss: 0.8650041418755279\n",
      "Epoch: 1305 \t|| Train Loss: 0.30382014645567196 \t|| Test Loss: 0.8644380318786243\n",
      "Epoch: 1306 \t|| Train Loss: 0.30364074545966857 \t|| Test Loss: 0.8638719218817208\n",
      "Epoch: 1307 \t|| Train Loss: 0.303461344463665 \t|| Test Loss: 0.8633058118848174\n",
      "Epoch: 1308 \t|| Train Loss: 0.30328194346766163 \t|| Test Loss: 0.862739701887914\n",
      "Epoch: 1309 \t|| Train Loss: 0.3031025424716582 \t|| Test Loss: 0.8621735918910105\n",
      "Epoch: 1310 \t|| Train Loss: 0.3029231414756547 \t|| Test Loss: 0.8616074818941071\n",
      "Epoch: 1311 \t|| Train Loss: 0.30274374047965125 \t|| Test Loss: 0.8610413718972036\n",
      "Epoch: 1312 \t|| Train Loss: 0.30256433948364775 \t|| Test Loss: 0.8604752619003003\n",
      "Epoch: 1313 \t|| Train Loss: 0.3023849384876443 \t|| Test Loss: 0.8599091519033968\n",
      "Epoch: 1314 \t|| Train Loss: 0.3022055374916409 \t|| Test Loss: 0.8593430419064934\n",
      "Epoch: 1315 \t|| Train Loss: 0.3020261364956374 \t|| Test Loss: 0.8587769319095899\n",
      "Epoch: 1316 \t|| Train Loss: 0.301846735499634 \t|| Test Loss: 0.8582108219126866\n",
      "Epoch: 1317 \t|| Train Loss: 0.30166733450363054 \t|| Test Loss: 0.8576447119157831\n",
      "Epoch: 1318 \t|| Train Loss: 0.30148793350762704 \t|| Test Loss: 0.8570786019188794\n",
      "Epoch: 1319 \t|| Train Loss: 0.3013085325116236 \t|| Test Loss: 0.8565124919219761\n",
      "Epoch: 1320 \t|| Train Loss: 0.30112913151562015 \t|| Test Loss: 0.8559463819250727\n",
      "Epoch: 1321 \t|| Train Loss: 0.30094973051961665 \t|| Test Loss: 0.8553802719281693\n",
      "Epoch: 1322 \t|| Train Loss: 0.30077032952361327 \t|| Test Loss: 0.8548141619312657\n",
      "Epoch: 1323 \t|| Train Loss: 0.30059092852760977 \t|| Test Loss: 0.8542480519343625\n",
      "Epoch: 1324 \t|| Train Loss: 0.30041152753160627 \t|| Test Loss: 0.8536819419374589\n",
      "Epoch: 1325 \t|| Train Loss: 0.3002321265356029 \t|| Test Loss: 0.8531158319405556\n",
      "Epoch: 1326 \t|| Train Loss: 0.3000527255395994 \t|| Test Loss: 0.852549721943652\n",
      "Epoch: 1327 \t|| Train Loss: 0.29987332454359594 \t|| Test Loss: 0.8519836119467487\n",
      "Epoch: 1328 \t|| Train Loss: 0.2996939235475925 \t|| Test Loss: 0.8514175019498452\n",
      "Epoch: 1329 \t|| Train Loss: 0.29951452255158906 \t|| Test Loss: 0.8508513919529417\n",
      "Epoch: 1330 \t|| Train Loss: 0.29933512155558556 \t|| Test Loss: 0.8502852819560383\n",
      "Epoch: 1331 \t|| Train Loss: 0.29915572055958206 \t|| Test Loss: 0.8497191719591347\n",
      "Epoch: 1332 \t|| Train Loss: 0.2989763195635787 \t|| Test Loss: 0.8491530619622315\n",
      "Epoch: 1333 \t|| Train Loss: 0.2987969185675752 \t|| Test Loss: 0.848586951965328\n",
      "Epoch: 1334 \t|| Train Loss: 0.2986175175715717 \t|| Test Loss: 0.8480208419684245\n",
      "Epoch: 1335 \t|| Train Loss: 0.2984381165755683 \t|| Test Loss: 0.847454731971521\n",
      "Epoch: 1336 \t|| Train Loss: 0.29825871557956474 \t|| Test Loss: 0.8468886219746177\n",
      "Epoch: 1337 \t|| Train Loss: 0.2980793145835613 \t|| Test Loss: 0.8463225119777142\n",
      "Epoch: 1338 \t|| Train Loss: 0.2978999135875579 \t|| Test Loss: 0.8457564019808108\n",
      "Epoch: 1339 \t|| Train Loss: 0.2977205125915544 \t|| Test Loss: 0.8451902919839073\n",
      "Epoch: 1340 \t|| Train Loss: 0.29754111159555097 \t|| Test Loss: 0.844624181987004\n",
      "Epoch: 1341 \t|| Train Loss: 0.2973617105995475 \t|| Test Loss: 0.8440580719901005\n",
      "Epoch: 1342 \t|| Train Loss: 0.2971823096035441 \t|| Test Loss: 0.843491961993197\n",
      "Epoch: 1343 \t|| Train Loss: 0.29700290860754064 \t|| Test Loss: 0.8429258519962936\n",
      "Epoch: 1344 \t|| Train Loss: 0.2968235076115371 \t|| Test Loss: 0.84235974199939\n",
      "Epoch: 1345 \t|| Train Loss: 0.2966441066155337 \t|| Test Loss: 0.8417936320024868\n",
      "Epoch: 1346 \t|| Train Loss: 0.2964647056195303 \t|| Test Loss: 0.8412275220055833\n",
      "Epoch: 1347 \t|| Train Loss: 0.2962853046235268 \t|| Test Loss: 0.8406614120086798\n",
      "Epoch: 1348 \t|| Train Loss: 0.2961059036275233 \t|| Test Loss: 0.8400953020117765\n",
      "Epoch: 1349 \t|| Train Loss: 0.29592650263151987 \t|| Test Loss: 0.8395291920148729\n",
      "Epoch: 1350 \t|| Train Loss: 0.2957471016355164 \t|| Test Loss: 0.8389630820179695\n",
      "Epoch: 1351 \t|| Train Loss: 0.29556770063951293 \t|| Test Loss: 0.8383969720210661\n",
      "Epoch: 1352 \t|| Train Loss: 0.2953882996435095 \t|| Test Loss: 0.8378308620241626\n",
      "Epoch: 1353 \t|| Train Loss: 0.29520889864750605 \t|| Test Loss: 0.8372647520272591\n",
      "Epoch: 1354 \t|| Train Loss: 0.2950294976515026 \t|| Test Loss: 0.8366986420303558\n",
      "Epoch: 1355 \t|| Train Loss: 0.2948500966554991 \t|| Test Loss: 0.8361325320334523\n",
      "Epoch: 1356 \t|| Train Loss: 0.29467069565949566 \t|| Test Loss: 0.8355664220365489\n",
      "Epoch: 1357 \t|| Train Loss: 0.2944912946634922 \t|| Test Loss: 0.8350003120396454\n",
      "Epoch: 1358 \t|| Train Loss: 0.2943200136005565 \t|| Test Loss: 0.8344966620429284\n",
      "Epoch: 1359 \t|| Train Loss: 0.2941762886043999 \t|| Test Loss: 0.8339930120462116\n",
      "Epoch: 1360 \t|| Train Loss: 0.29403256360824337 \t|| Test Loss: 0.8334893620494945\n",
      "Epoch: 1361 \t|| Train Loss: 0.2938888386120867 \t|| Test Loss: 0.8329857120527778\n",
      "Epoch: 1362 \t|| Train Loss: 0.29374511361593003 \t|| Test Loss: 0.8324820620560608\n",
      "Epoch: 1363 \t|| Train Loss: 0.29360138861977336 \t|| Test Loss: 0.831978412059344\n",
      "Epoch: 1364 \t|| Train Loss: 0.2934576636236168 \t|| Test Loss: 0.831474762062627\n",
      "Epoch: 1365 \t|| Train Loss: 0.29331393862746025 \t|| Test Loss: 0.83097111206591\n",
      "Epoch: 1366 \t|| Train Loss: 0.2931702136313036 \t|| Test Loss: 0.8304674620691932\n",
      "Epoch: 1367 \t|| Train Loss: 0.29302648863514696 \t|| Test Loss: 0.8299638120724762\n",
      "Epoch: 1368 \t|| Train Loss: 0.2928827636389903 \t|| Test Loss: 0.8294601620757593\n",
      "Epoch: 1369 \t|| Train Loss: 0.2927390386428337 \t|| Test Loss: 0.8289565120790426\n",
      "Epoch: 1370 \t|| Train Loss: 0.2925953136466771 \t|| Test Loss: 0.8284528620823256\n",
      "Epoch: 1371 \t|| Train Loss: 0.29245158865052046 \t|| Test Loss: 0.8279492120856087\n",
      "Epoch: 1372 \t|| Train Loss: 0.2923078636543638 \t|| Test Loss: 0.8274455620888919\n",
      "Epoch: 1373 \t|| Train Loss: 0.29216413865820723 \t|| Test Loss: 0.8269419120921748\n",
      "Epoch: 1374 \t|| Train Loss: 0.2920204136620506 \t|| Test Loss: 0.826438262095458\n",
      "Epoch: 1375 \t|| Train Loss: 0.291876688665894 \t|| Test Loss: 0.825934612098741\n",
      "Epoch: 1376 \t|| Train Loss: 0.29173296366973733 \t|| Test Loss: 0.825430962102024\n",
      "Epoch: 1377 \t|| Train Loss: 0.2915892386735807 \t|| Test Loss: 0.8249273121053073\n",
      "Epoch: 1378 \t|| Train Loss: 0.29144551367742405 \t|| Test Loss: 0.8244236621085903\n",
      "Epoch: 1379 \t|| Train Loss: 0.2913017886812675 \t|| Test Loss: 0.8239200121118735\n",
      "Epoch: 1380 \t|| Train Loss: 0.2911580636851109 \t|| Test Loss: 0.8234163621151565\n",
      "Epoch: 1381 \t|| Train Loss: 0.2910143386889542 \t|| Test Loss: 0.8229127121184396\n",
      "Epoch: 1382 \t|| Train Loss: 0.2908706136927976 \t|| Test Loss: 0.8224090621217227\n",
      "Epoch: 1383 \t|| Train Loss: 0.290726888696641 \t|| Test Loss: 0.8219054121250057\n",
      "Epoch: 1384 \t|| Train Loss: 0.29058316370048437 \t|| Test Loss: 0.8214017621282889\n",
      "Epoch: 1385 \t|| Train Loss: 0.29043943870432776 \t|| Test Loss: 0.8208981121315719\n",
      "Epoch: 1386 \t|| Train Loss: 0.29029571370817114 \t|| Test Loss: 0.8203944621348551\n",
      "Epoch: 1387 \t|| Train Loss: 0.2901519887120145 \t|| Test Loss: 0.8198908121381381\n",
      "Epoch: 1388 \t|| Train Loss: 0.2900082637158579 \t|| Test Loss: 0.8193871621414214\n",
      "Epoch: 1389 \t|| Train Loss: 0.2898645387197013 \t|| Test Loss: 0.8188835121447043\n",
      "Epoch: 1390 \t|| Train Loss: 0.28972081372354463 \t|| Test Loss: 0.8183798621479875\n",
      "Epoch: 1391 \t|| Train Loss: 0.289577088727388 \t|| Test Loss: 0.8178762121512706\n",
      "Epoch: 1392 \t|| Train Loss: 0.28943336373123135 \t|| Test Loss: 0.8173725621545536\n",
      "Epoch: 1393 \t|| Train Loss: 0.28928963873507485 \t|| Test Loss: 0.8168689121578367\n",
      "Epoch: 1394 \t|| Train Loss: 0.2891459137389182 \t|| Test Loss: 0.8163652621611199\n",
      "Epoch: 1395 \t|| Train Loss: 0.2890021887427615 \t|| Test Loss: 0.8158616121644029\n",
      "Epoch: 1396 \t|| Train Loss: 0.2888584637466049 \t|| Test Loss: 0.815357962167686\n",
      "Epoch: 1397 \t|| Train Loss: 0.28871473875044834 \t|| Test Loss: 0.8148543121709692\n",
      "Epoch: 1398 \t|| Train Loss: 0.28857101375429167 \t|| Test Loss: 0.8143506621742521\n",
      "Epoch: 1399 \t|| Train Loss: 0.28842728875813506 \t|| Test Loss: 0.8138470121775352\n",
      "Epoch: 1400 \t|| Train Loss: 0.2882835637619784 \t|| Test Loss: 0.8133433621808184\n",
      "Epoch: 1401 \t|| Train Loss: 0.2881398387658218 \t|| Test Loss: 0.8128397121841016\n",
      "Epoch: 1402 \t|| Train Loss: 0.2879961137696652 \t|| Test Loss: 0.8123360621873846\n",
      "Epoch: 1403 \t|| Train Loss: 0.2878523887735086 \t|| Test Loss: 0.8118324121906676\n",
      "Epoch: 1404 \t|| Train Loss: 0.28770866377735194 \t|| Test Loss: 0.8113287621939508\n",
      "Epoch: 1405 \t|| Train Loss: 0.28756493878119527 \t|| Test Loss: 0.8108251121972339\n",
      "Epoch: 1406 \t|| Train Loss: 0.2874212137850387 \t|| Test Loss: 0.8103214622005168\n",
      "Epoch: 1407 \t|| Train Loss: 0.2872774887888821 \t|| Test Loss: 0.8098178122038\n",
      "Epoch: 1408 \t|| Train Loss: 0.2871337637927254 \t|| Test Loss: 0.8093141622070832\n",
      "Epoch: 1409 \t|| Train Loss: 0.2869900387965688 \t|| Test Loss: 0.8088105122103663\n",
      "Epoch: 1410 \t|| Train Loss: 0.2868463138004122 \t|| Test Loss: 0.8083068622136494\n",
      "Epoch: 1411 \t|| Train Loss: 0.2867025888042556 \t|| Test Loss: 0.8078032122169324\n",
      "Epoch: 1412 \t|| Train Loss: 0.286558863808099 \t|| Test Loss: 0.8072995622202155\n",
      "Epoch: 1413 \t|| Train Loss: 0.28641513881194236 \t|| Test Loss: 0.8067959122234987\n",
      "Epoch: 1414 \t|| Train Loss: 0.2862714138157857 \t|| Test Loss: 0.8062922622267816\n",
      "Epoch: 1415 \t|| Train Loss: 0.2861276888196291 \t|| Test Loss: 0.8057886122300648\n",
      "Epoch: 1416 \t|| Train Loss: 0.2859839638234725 \t|| Test Loss: 0.8052849622333479\n",
      "Epoch: 1417 \t|| Train Loss: 0.28584023882731585 \t|| Test Loss: 0.8047813122366311\n",
      "Epoch: 1418 \t|| Train Loss: 0.28569651383115924 \t|| Test Loss: 0.804277662239914\n",
      "Epoch: 1419 \t|| Train Loss: 0.2855527888350026 \t|| Test Loss: 0.8037740122431971\n",
      "Epoch: 1420 \t|| Train Loss: 0.285409063838846 \t|| Test Loss: 0.8032703622464803\n",
      "Epoch: 1421 \t|| Train Loss: 0.2852653388426894 \t|| Test Loss: 0.8027667122497633\n",
      "Epoch: 1422 \t|| Train Loss: 0.28512161384653273 \t|| Test Loss: 0.8022630622530464\n",
      "Epoch: 1423 \t|| Train Loss: 0.2849778888503761 \t|| Test Loss: 0.8017594122563294\n",
      "Epoch: 1424 \t|| Train Loss: 0.2848341638542195 \t|| Test Loss: 0.8012557622596127\n",
      "Epoch: 1425 \t|| Train Loss: 0.2846904388580629 \t|| Test Loss: 0.8007521122628958\n",
      "Epoch: 1426 \t|| Train Loss: 0.2845467138619062 \t|| Test Loss: 0.8002484622661787\n",
      "Epoch: 1427 \t|| Train Loss: 0.28440298886574966 \t|| Test Loss: 0.7997448122694619\n",
      "Epoch: 1428 \t|| Train Loss: 0.284259263869593 \t|| Test Loss: 0.799241162272745\n",
      "Epoch: 1429 \t|| Train Loss: 0.28411553887343644 \t|| Test Loss: 0.7987375122760281\n",
      "Epoch: 1430 \t|| Train Loss: 0.2839718138772798 \t|| Test Loss: 0.7982338622793111\n",
      "Epoch: 1431 \t|| Train Loss: 0.2838280888811232 \t|| Test Loss: 0.7977302122825943\n",
      "Epoch: 1432 \t|| Train Loss: 0.2836843638849665 \t|| Test Loss: 0.7972265622858774\n",
      "Epoch: 1433 \t|| Train Loss: 0.28354063888880987 \t|| Test Loss: 0.7967229122891605\n",
      "Epoch: 1434 \t|| Train Loss: 0.2833969138926533 \t|| Test Loss: 0.7962192622924434\n",
      "Epoch: 1435 \t|| Train Loss: 0.28325318889649664 \t|| Test Loss: 0.7957156122957267\n",
      "Epoch: 1436 \t|| Train Loss: 0.28310946390034003 \t|| Test Loss: 0.7952119622990097\n",
      "Epoch: 1437 \t|| Train Loss: 0.2829657389041834 \t|| Test Loss: 0.7947083123022928\n",
      "Epoch: 1438 \t|| Train Loss: 0.28282201390802675 \t|| Test Loss: 0.794204662305576\n",
      "Epoch: 1439 \t|| Train Loss: 0.28267828891187013 \t|| Test Loss: 0.7937010123088589\n",
      "Epoch: 1440 \t|| Train Loss: 0.28254911536671046 \t|| Test Loss: 0.7932607123114894\n",
      "Epoch: 1441 \t|| Train Loss: 0.28243621536986874 \t|| Test Loss: 0.7928204123141198\n",
      "Epoch: 1442 \t|| Train Loss: 0.282323315373027 \t|| Test Loss: 0.7923801123167502\n",
      "Epoch: 1443 \t|| Train Loss: 0.28221041537618535 \t|| Test Loss: 0.7919398123193806\n",
      "Epoch: 1444 \t|| Train Loss: 0.28209751537934363 \t|| Test Loss: 0.7914995123220111\n",
      "Epoch: 1445 \t|| Train Loss: 0.2819846153825019 \t|| Test Loss: 0.7910592123246415\n",
      "Epoch: 1446 \t|| Train Loss: 0.2818717153856602 \t|| Test Loss: 0.7906189123272719\n",
      "Epoch: 1447 \t|| Train Loss: 0.28175881538881853 \t|| Test Loss: 0.7901786123299022\n",
      "Epoch: 1448 \t|| Train Loss: 0.28164591539197675 \t|| Test Loss: 0.7897383123325327\n",
      "Epoch: 1449 \t|| Train Loss: 0.28153301539513514 \t|| Test Loss: 0.7892980123351632\n",
      "Epoch: 1450 \t|| Train Loss: 0.28142011539829337 \t|| Test Loss: 0.7888577123377936\n",
      "Epoch: 1451 \t|| Train Loss: 0.28130721540145165 \t|| Test Loss: 0.788417412340424\n",
      "Epoch: 1452 \t|| Train Loss: 0.28119431540461 \t|| Test Loss: 0.7879771123430542\n",
      "Epoch: 1453 \t|| Train Loss: 0.28108141540776826 \t|| Test Loss: 0.7875368123456847\n",
      "Epoch: 1454 \t|| Train Loss: 0.2809685154109266 \t|| Test Loss: 0.7870965123483152\n",
      "Epoch: 1455 \t|| Train Loss: 0.2808556154140849 \t|| Test Loss: 0.7866562123509457\n",
      "Epoch: 1456 \t|| Train Loss: 0.28074271541724316 \t|| Test Loss: 0.786215912353576\n",
      "Epoch: 1457 \t|| Train Loss: 0.28062981542040144 \t|| Test Loss: 0.7857756123562064\n",
      "Epoch: 1458 \t|| Train Loss: 0.2805169154235597 \t|| Test Loss: 0.7853353123588368\n",
      "Epoch: 1459 \t|| Train Loss: 0.280404015426718 \t|| Test Loss: 0.7848950123614672\n",
      "Epoch: 1460 \t|| Train Loss: 0.2802911154298763 \t|| Test Loss: 0.7844547123640977\n",
      "Epoch: 1461 \t|| Train Loss: 0.2801782154330346 \t|| Test Loss: 0.7840144123667281\n",
      "Epoch: 1462 \t|| Train Loss: 0.2800653154361929 \t|| Test Loss: 0.7835741123693585\n",
      "Epoch: 1463 \t|| Train Loss: 0.27995241543935123 \t|| Test Loss: 0.7831338123719889\n",
      "Epoch: 1464 \t|| Train Loss: 0.2798395154425095 \t|| Test Loss: 0.7826935123746193\n",
      "Epoch: 1465 \t|| Train Loss: 0.2797266154456678 \t|| Test Loss: 0.7822532123772498\n",
      "Epoch: 1466 \t|| Train Loss: 0.2796137154488261 \t|| Test Loss: 0.7818129123798802\n",
      "Epoch: 1467 \t|| Train Loss: 0.27950081545198435 \t|| Test Loss: 0.7813726123825104\n",
      "Epoch: 1468 \t|| Train Loss: 0.2793879154551427 \t|| Test Loss: 0.7809323123851408\n",
      "Epoch: 1469 \t|| Train Loss: 0.2792750154583009 \t|| Test Loss: 0.7804920123877713\n",
      "Epoch: 1470 \t|| Train Loss: 0.27916211546145925 \t|| Test Loss: 0.7800517123904018\n",
      "Epoch: 1471 \t|| Train Loss: 0.27904921546461753 \t|| Test Loss: 0.7796114123930321\n",
      "Epoch: 1472 \t|| Train Loss: 0.27893631546777586 \t|| Test Loss: 0.7791711123956626\n",
      "Epoch: 1473 \t|| Train Loss: 0.27882341547093414 \t|| Test Loss: 0.778730812398293\n",
      "Epoch: 1474 \t|| Train Loss: 0.2787105154740924 \t|| Test Loss: 0.7782905124009233\n",
      "Epoch: 1475 \t|| Train Loss: 0.27859761547725076 \t|| Test Loss: 0.7778502124035539\n",
      "Epoch: 1476 \t|| Train Loss: 0.27848471548040904 \t|| Test Loss: 0.7774099124061843\n",
      "Epoch: 1477 \t|| Train Loss: 0.2783718154835674 \t|| Test Loss: 0.7769696124088146\n",
      "Epoch: 1478 \t|| Train Loss: 0.2782589154867256 \t|| Test Loss: 0.776529312411445\n",
      "Epoch: 1479 \t|| Train Loss: 0.2781460154898839 \t|| Test Loss: 0.7760890124140755\n",
      "Epoch: 1480 \t|| Train Loss: 0.27803311549304216 \t|| Test Loss: 0.7756487124167059\n",
      "Epoch: 1481 \t|| Train Loss: 0.2779202154962005 \t|| Test Loss: 0.7752084124193364\n",
      "Epoch: 1482 \t|| Train Loss: 0.2778073154993588 \t|| Test Loss: 0.7747681124219666\n",
      "Epoch: 1483 \t|| Train Loss: 0.27769441550251706 \t|| Test Loss: 0.774327812424597\n",
      "Epoch: 1484 \t|| Train Loss: 0.27758151550567534 \t|| Test Loss: 0.7738875124272274\n",
      "Epoch: 1485 \t|| Train Loss: 0.2774686155088336 \t|| Test Loss: 0.7734472124298579\n",
      "Epoch: 1486 \t|| Train Loss: 0.2773557155119919 \t|| Test Loss: 0.7730069124324884\n",
      "Epoch: 1487 \t|| Train Loss: 0.27724281551515023 \t|| Test Loss: 0.7725666124351187\n",
      "Epoch: 1488 \t|| Train Loss: 0.27712991551830857 \t|| Test Loss: 0.7721263124377491\n",
      "Epoch: 1489 \t|| Train Loss: 0.2770170155214668 \t|| Test Loss: 0.7716860124403795\n",
      "Epoch: 1490 \t|| Train Loss: 0.2769041155246251 \t|| Test Loss: 0.7712457124430099\n",
      "Epoch: 1491 \t|| Train Loss: 0.27679121552778346 \t|| Test Loss: 0.7708054124456404\n",
      "Epoch: 1492 \t|| Train Loss: 0.2766783155309417 \t|| Test Loss: 0.7703651124482708\n",
      "Epoch: 1493 \t|| Train Loss: 0.2765654155341 \t|| Test Loss: 0.7699248124509012\n",
      "Epoch: 1494 \t|| Train Loss: 0.2764525155372583 \t|| Test Loss: 0.7694845124535316\n",
      "Epoch: 1495 \t|| Train Loss: 0.27633961554041664 \t|| Test Loss: 0.7690442124561621\n",
      "Epoch: 1496 \t|| Train Loss: 0.2762267155435749 \t|| Test Loss: 0.7686039124587924\n",
      "Epoch: 1497 \t|| Train Loss: 0.2761138155467332 \t|| Test Loss: 0.7681636124614227\n",
      "Epoch: 1498 \t|| Train Loss: 0.2760009155498915 \t|| Test Loss: 0.7677233124640532\n",
      "Epoch: 1499 \t|| Train Loss: 0.27588801555304976 \t|| Test Loss: 0.7672830124666836\n",
      "Epoch: 1500 \t|| Train Loss: 0.27577511555620804 \t|| Test Loss: 0.766842712469314\n",
      "Epoch: 1501 \t|| Train Loss: 0.2756622155593663 \t|| Test Loss: 0.7664024124719446\n",
      "Epoch: 1502 \t|| Train Loss: 0.2755493155625247 \t|| Test Loss: 0.7659621124745749\n",
      "Epoch: 1503 \t|| Train Loss: 0.27543641556568293 \t|| Test Loss: 0.7655218124772053\n",
      "Epoch: 1504 \t|| Train Loss: 0.27532351556884127 \t|| Test Loss: 0.7650815124798357\n",
      "Epoch: 1505 \t|| Train Loss: 0.2752106155719995 \t|| Test Loss: 0.7646412124824661\n",
      "Epoch: 1506 \t|| Train Loss: 0.27509771557515783 \t|| Test Loss: 0.7642009124850965\n",
      "Epoch: 1507 \t|| Train Loss: 0.27498481557831617 \t|| Test Loss: 0.763760612487727\n",
      "Epoch: 1508 \t|| Train Loss: 0.2748719155814744 \t|| Test Loss: 0.7633203124903574\n",
      "Epoch: 1509 \t|| Train Loss: 0.2747590155846327 \t|| Test Loss: 0.7628800124929878\n",
      "Epoch: 1510 \t|| Train Loss: 0.274646115587791 \t|| Test Loss: 0.7624397124956183\n",
      "Epoch: 1511 \t|| Train Loss: 0.2745332155909493 \t|| Test Loss: 0.7619994124982485\n",
      "Epoch: 1512 \t|| Train Loss: 0.27442031559410757 \t|| Test Loss: 0.761559112500879\n",
      "Epoch: 1513 \t|| Train Loss: 0.2743074155972659 \t|| Test Loss: 0.7611188125035093\n",
      "Epoch: 1514 \t|| Train Loss: 0.2741945156004242 \t|| Test Loss: 0.7606785125061397\n",
      "Epoch: 1515 \t|| Train Loss: 0.27408161560358246 \t|| Test Loss: 0.7602382125087702\n",
      "Epoch: 1516 \t|| Train Loss: 0.27396871560674074 \t|| Test Loss: 0.7597979125114007\n",
      "Epoch: 1517 \t|| Train Loss: 0.2738558156098991 \t|| Test Loss: 0.7593576125140311\n",
      "Epoch: 1518 \t|| Train Loss: 0.2737429156130574 \t|| Test Loss: 0.7589173125166615\n",
      "Epoch: 1519 \t|| Train Loss: 0.27363001561621564 \t|| Test Loss: 0.7584770125192918\n",
      "Epoch: 1520 \t|| Train Loss: 0.273517115619374 \t|| Test Loss: 0.7580367125219223\n",
      "Epoch: 1521 \t|| Train Loss: 0.27340421562253225 \t|| Test Loss: 0.7575964125245527\n",
      "Epoch: 1522 \t|| Train Loss: 0.27329131562569053 \t|| Test Loss: 0.7571561125271831\n",
      "Epoch: 1523 \t|| Train Loss: 0.2731784156288488 \t|| Test Loss: 0.7567158125298136\n",
      "Epoch: 1524 \t|| Train Loss: 0.27306551563200715 \t|| Test Loss: 0.756275512532444\n",
      "Epoch: 1525 \t|| Train Loss: 0.27295261563516543 \t|| Test Loss: 0.7558352125350745\n",
      "Epoch: 1526 \t|| Train Loss: 0.2728397156383237 \t|| Test Loss: 0.7553949125377046\n",
      "Epoch: 1527 \t|| Train Loss: 0.272726815641482 \t|| Test Loss: 0.7549546125403352\n",
      "Epoch: 1528 \t|| Train Loss: 0.2726139156446403 \t|| Test Loss: 0.7545143125429655\n",
      "Epoch: 1529 \t|| Train Loss: 0.2725010156477986 \t|| Test Loss: 0.754074012545596\n",
      "Epoch: 1530 \t|| Train Loss: 0.2723881156509569 \t|| Test Loss: 0.7536337125482264\n",
      "Epoch: 1531 \t|| Train Loss: 0.27227521565411517 \t|| Test Loss: 0.7531934125508568\n",
      "Epoch: 1532 \t|| Train Loss: 0.27216231565727345 \t|| Test Loss: 0.7527531125534871\n",
      "Epoch: 1533 \t|| Train Loss: 0.27204997765578137 \t|| Test Loss: 0.7523770525559519\n",
      "Epoch: 1534 \t|| Train Loss: 0.27196296165857087 \t|| Test Loss: 0.7520009925584168\n",
      "Epoch: 1535 \t|| Train Loss: 0.2718759456613603 \t|| Test Loss: 0.7516249325608815\n",
      "Epoch: 1536 \t|| Train Loss: 0.2717889296641498 \t|| Test Loss: 0.7512488725633462\n",
      "Epoch: 1537 \t|| Train Loss: 0.27170191366693924 \t|| Test Loss: 0.750872812565811\n",
      "Epoch: 1538 \t|| Train Loss: 0.2716148976697287 \t|| Test Loss: 0.7504967525682756\n",
      "Epoch: 1539 \t|| Train Loss: 0.27152788167251823 \t|| Test Loss: 0.7501206925707404\n",
      "Epoch: 1540 \t|| Train Loss: 0.2714408656753077 \t|| Test Loss: 0.7497446325732051\n",
      "Epoch: 1541 \t|| Train Loss: 0.27135384967809706 \t|| Test Loss: 0.7493685725756698\n",
      "Epoch: 1542 \t|| Train Loss: 0.2712668336808866 \t|| Test Loss: 0.7489925125781346\n",
      "Epoch: 1543 \t|| Train Loss: 0.27117981768367605 \t|| Test Loss: 0.7486164525805993\n",
      "Epoch: 1544 \t|| Train Loss: 0.27109280168646555 \t|| Test Loss: 0.7482403925830641\n",
      "Epoch: 1545 \t|| Train Loss: 0.27100578568925504 \t|| Test Loss: 0.7478643325855288\n",
      "Epoch: 1546 \t|| Train Loss: 0.2709187696920445 \t|| Test Loss: 0.7474882725879937\n",
      "Epoch: 1547 \t|| Train Loss: 0.270831753694834 \t|| Test Loss: 0.7471122125904583\n",
      "Epoch: 1548 \t|| Train Loss: 0.2707447376976234 \t|| Test Loss: 0.7467361525929231\n",
      "Epoch: 1549 \t|| Train Loss: 0.27065772170041286 \t|| Test Loss: 0.7463600925953877\n",
      "Epoch: 1550 \t|| Train Loss: 0.27057070570320235 \t|| Test Loss: 0.7459840325978526\n",
      "Epoch: 1551 \t|| Train Loss: 0.2704836897059918 \t|| Test Loss: 0.7456079726003173\n",
      "Epoch: 1552 \t|| Train Loss: 0.27039667370878134 \t|| Test Loss: 0.745231912602782\n",
      "Epoch: 1553 \t|| Train Loss: 0.2703096577115708 \t|| Test Loss: 0.7448558526052468\n",
      "Epoch: 1554 \t|| Train Loss: 0.27022264171436017 \t|| Test Loss: 0.7444797926077115\n",
      "Epoch: 1555 \t|| Train Loss: 0.2701356257171497 \t|| Test Loss: 0.7441037326101763\n",
      "Epoch: 1556 \t|| Train Loss: 0.27004860971993916 \t|| Test Loss: 0.7437276726126409\n",
      "Epoch: 1557 \t|| Train Loss: 0.26996159372272865 \t|| Test Loss: 0.7433516126151056\n",
      "Epoch: 1558 \t|| Train Loss: 0.26987457772551815 \t|| Test Loss: 0.7429755526175704\n",
      "Epoch: 1559 \t|| Train Loss: 0.26978756172830753 \t|| Test Loss: 0.7425994926200351\n",
      "Epoch: 1560 \t|| Train Loss: 0.2697005457310971 \t|| Test Loss: 0.7422234326225\n",
      "Epoch: 1561 \t|| Train Loss: 0.2696135297338865 \t|| Test Loss: 0.7418473726249646\n",
      "Epoch: 1562 \t|| Train Loss: 0.26952651373667597 \t|| Test Loss: 0.7414713126274295\n",
      "Epoch: 1563 \t|| Train Loss: 0.26943949773946546 \t|| Test Loss: 0.7410952526298942\n",
      "Epoch: 1564 \t|| Train Loss: 0.2693524817422549 \t|| Test Loss: 0.740719192632359\n",
      "Epoch: 1565 \t|| Train Loss: 0.26926546574504445 \t|| Test Loss: 0.7403431326348237\n",
      "Epoch: 1566 \t|| Train Loss: 0.2691784497478339 \t|| Test Loss: 0.7399670726372884\n",
      "Epoch: 1567 \t|| Train Loss: 0.26909143375062333 \t|| Test Loss: 0.7395910126397531\n",
      "Epoch: 1568 \t|| Train Loss: 0.26900441775341283 \t|| Test Loss: 0.7392149526422178\n",
      "Epoch: 1569 \t|| Train Loss: 0.26891740175620227 \t|| Test Loss: 0.7388388926446826\n",
      "Epoch: 1570 \t|| Train Loss: 0.26883038575899176 \t|| Test Loss: 0.7384628326471473\n",
      "Epoch: 1571 \t|| Train Loss: 0.26874336976178126 \t|| Test Loss: 0.7380867726496121\n",
      "Epoch: 1572 \t|| Train Loss: 0.2686563537645707 \t|| Test Loss: 0.7377107126520768\n",
      "Epoch: 1573 \t|| Train Loss: 0.26856933776736014 \t|| Test Loss: 0.7373346526545415\n",
      "Epoch: 1574 \t|| Train Loss: 0.26848232177014963 \t|| Test Loss: 0.7369585926570064\n",
      "Epoch: 1575 \t|| Train Loss: 0.26839530577293913 \t|| Test Loss: 0.7365825326594712\n",
      "Epoch: 1576 \t|| Train Loss: 0.26830828977572857 \t|| Test Loss: 0.7362064726619358\n",
      "Epoch: 1577 \t|| Train Loss: 0.26822127377851807 \t|| Test Loss: 0.7358304126644006\n",
      "Epoch: 1578 \t|| Train Loss: 0.2681342577813075 \t|| Test Loss: 0.7354543526668653\n",
      "Epoch: 1579 \t|| Train Loss: 0.268047241784097 \t|| Test Loss: 0.73507829266933\n",
      "Epoch: 1580 \t|| Train Loss: 0.2679602257868864 \t|| Test Loss: 0.7347022326717948\n",
      "Epoch: 1581 \t|| Train Loss: 0.26787320978967594 \t|| Test Loss: 0.7343261726742595\n",
      "Epoch: 1582 \t|| Train Loss: 0.2677861937924654 \t|| Test Loss: 0.7339501126767243\n",
      "Epoch: 1583 \t|| Train Loss: 0.2676991777952549 \t|| Test Loss: 0.733574052679189\n",
      "Epoch: 1584 \t|| Train Loss: 0.2676121617980443 \t|| Test Loss: 0.7331979926816536\n",
      "Epoch: 1585 \t|| Train Loss: 0.26752514580083375 \t|| Test Loss: 0.7328219326841184\n",
      "Epoch: 1586 \t|| Train Loss: 0.26743812980362325 \t|| Test Loss: 0.7324458726865831\n",
      "Epoch: 1587 \t|| Train Loss: 0.26735111380641274 \t|| Test Loss: 0.732069812689048\n",
      "Epoch: 1588 \t|| Train Loss: 0.2672640978092022 \t|| Test Loss: 0.7316937526915128\n",
      "Epoch: 1589 \t|| Train Loss: 0.2671770818119917 \t|| Test Loss: 0.7313176926939775\n",
      "Epoch: 1590 \t|| Train Loss: 0.2670900658147811 \t|| Test Loss: 0.7309416326964422\n",
      "Epoch: 1591 \t|| Train Loss: 0.2670030498175706 \t|| Test Loss: 0.730565572698907\n",
      "Epoch: 1592 \t|| Train Loss: 0.2669160338203601 \t|| Test Loss: 0.7301895127013717\n",
      "Epoch: 1593 \t|| Train Loss: 0.26682901782314955 \t|| Test Loss: 0.7298134527038365\n",
      "Epoch: 1594 \t|| Train Loss: 0.26674200182593905 \t|| Test Loss: 0.7294373927063011\n",
      "Epoch: 1595 \t|| Train Loss: 0.2666549858287285 \t|| Test Loss: 0.7290613327087658\n",
      "Epoch: 1596 \t|| Train Loss: 0.266567969831518 \t|| Test Loss: 0.7286852727112306\n",
      "Epoch: 1597 \t|| Train Loss: 0.2664809538343074 \t|| Test Loss: 0.7283092127136953\n",
      "Epoch: 1598 \t|| Train Loss: 0.2663939378370969 \t|| Test Loss: 0.7279331527161601\n",
      "Epoch: 1599 \t|| Train Loss: 0.26630692183988636 \t|| Test Loss: 0.7275570927186248\n",
      "Epoch: 1600 \t|| Train Loss: 0.26621990584267585 \t|| Test Loss: 0.7271810327210897\n",
      "Epoch: 1601 \t|| Train Loss: 0.2661328898454653 \t|| Test Loss: 0.7268049727235544\n",
      "Epoch: 1602 \t|| Train Loss: 0.2660458738482548 \t|| Test Loss: 0.7264289127260192\n",
      "Epoch: 1603 \t|| Train Loss: 0.26595885785104423 \t|| Test Loss: 0.7260528527284839\n",
      "Epoch: 1604 \t|| Train Loss: 0.26587184185383367 \t|| Test Loss: 0.7256767927309486\n",
      "Epoch: 1605 \t|| Train Loss: 0.2657848258566232 \t|| Test Loss: 0.7253007327334133\n",
      "Epoch: 1606 \t|| Train Loss: 0.2656978098594126 \t|| Test Loss: 0.724924672735878\n",
      "Epoch: 1607 \t|| Train Loss: 0.26561079386220215 \t|| Test Loss: 0.7245486127383428\n",
      "Epoch: 1608 \t|| Train Loss: 0.2655237778649916 \t|| Test Loss: 0.7241725527408075\n",
      "Epoch: 1609 \t|| Train Loss: 0.2654367618677811 \t|| Test Loss: 0.7237964927432723\n",
      "Epoch: 1610 \t|| Train Loss: 0.26534974587057053 \t|| Test Loss: 0.723420432745737\n",
      "Epoch: 1611 \t|| Train Loss: 0.26526272987335997 \t|| Test Loss: 0.7230443727482018\n",
      "Epoch: 1612 \t|| Train Loss: 0.26517571387614947 \t|| Test Loss: 0.7226683127506665\n",
      "Epoch: 1613 \t|| Train Loss: 0.26508869787893896 \t|| Test Loss: 0.7222922527531311\n",
      "Epoch: 1614 \t|| Train Loss: 0.2650016818817284 \t|| Test Loss: 0.721916192755596\n",
      "Epoch: 1615 \t|| Train Loss: 0.2649146658845179 \t|| Test Loss: 0.7215401327580608\n",
      "Epoch: 1616 \t|| Train Loss: 0.26482764988730734 \t|| Test Loss: 0.7211640727605255\n",
      "Epoch: 1617 \t|| Train Loss: 0.26474063389009683 \t|| Test Loss: 0.7207880127629902\n",
      "Epoch: 1618 \t|| Train Loss: 0.2646536178928863 \t|| Test Loss: 0.720411952765455\n",
      "Epoch: 1619 \t|| Train Loss: 0.2645666018956757 \t|| Test Loss: 0.7200358927679197\n",
      "Epoch: 1620 \t|| Train Loss: 0.26447958589846526 \t|| Test Loss: 0.7196598327703845\n",
      "Epoch: 1621 \t|| Train Loss: 0.2643925699012547 \t|| Test Loss: 0.7192837727728492\n",
      "Epoch: 1622 \t|| Train Loss: 0.2643055539040442 \t|| Test Loss: 0.7189077127753138\n",
      "Epoch: 1623 \t|| Train Loss: 0.26421853790683364 \t|| Test Loss: 0.7185316527777788\n",
      "Epoch: 1624 \t|| Train Loss: 0.2641315219096231 \t|| Test Loss: 0.7181555927802435\n",
      "Epoch: 1625 \t|| Train Loss: 0.2640445059124126 \t|| Test Loss: 0.7177795327827081\n",
      "Epoch: 1626 \t|| Train Loss: 0.26395748991520207 \t|| Test Loss: 0.717403472785173\n",
      "Epoch: 1627 \t|| Train Loss: 0.26387047391799145 \t|| Test Loss: 0.7170274127876375\n",
      "Epoch: 1628 \t|| Train Loss: 0.26378345792078095 \t|| Test Loss: 0.7166513527901024\n",
      "Epoch: 1629 \t|| Train Loss: 0.26369644192357045 \t|| Test Loss: 0.7162752927925672\n",
      "Epoch: 1630 \t|| Train Loss: 0.2636094259263599 \t|| Test Loss: 0.7158992327950319\n",
      "Epoch: 1631 \t|| Train Loss: 0.2635224099291494 \t|| Test Loss: 0.7155231727974967\n",
      "Epoch: 1632 \t|| Train Loss: 0.2634353939319389 \t|| Test Loss: 0.7151471127999613\n",
      "Epoch: 1633 \t|| Train Loss: 0.2633483779347283 \t|| Test Loss: 0.714771052802426\n",
      "Epoch: 1634 \t|| Train Loss: 0.2632613619375178 \t|| Test Loss: 0.7143949928048908\n",
      "Epoch: 1635 \t|| Train Loss: 0.26317434594030725 \t|| Test Loss: 0.7140189328073556\n",
      "Epoch: 1636 \t|| Train Loss: 0.26308732994309675 \t|| Test Loss: 0.7136428728098203\n",
      "Epoch: 1637 \t|| Train Loss: 0.2630003139458862 \t|| Test Loss: 0.7132668128122851\n",
      "Epoch: 1638 \t|| Train Loss: 0.2629132979486757 \t|| Test Loss: 0.7128907528147498\n",
      "Epoch: 1639 \t|| Train Loss: 0.2628262819514652 \t|| Test Loss: 0.7125146928172145\n",
      "Epoch: 1640 \t|| Train Loss: 0.26273926595425456 \t|| Test Loss: 0.7121386328196794\n",
      "Epoch: 1641 \t|| Train Loss: 0.26265224995704406 \t|| Test Loss: 0.711762572822144\n",
      "Epoch: 1642 \t|| Train Loss: 0.26256523395983355 \t|| Test Loss: 0.7113865128246089\n",
      "Epoch: 1643 \t|| Train Loss: 0.262478217962623 \t|| Test Loss: 0.7110104528270735\n",
      "Epoch: 1644 \t|| Train Loss: 0.26239120196541244 \t|| Test Loss: 0.7106343928295382\n",
      "Epoch: 1645 \t|| Train Loss: 0.2623077245331452 \t|| Test Loss: 0.7103234628323243\n",
      "Epoch: 1646 \t|| Train Loss: 0.2622415555358327 \t|| Test Loss: 0.7100125328351102\n",
      "Epoch: 1647 \t|| Train Loss: 0.26217538653852024 \t|| Test Loss: 0.7097016028378962\n",
      "Epoch: 1648 \t|| Train Loss: 0.26210921754120775 \t|| Test Loss: 0.7093906728406821\n",
      "Epoch: 1649 \t|| Train Loss: 0.26204304854389526 \t|| Test Loss: 0.7090797428434682\n",
      "Epoch: 1650 \t|| Train Loss: 0.2619768795465828 \t|| Test Loss: 0.708768812846254\n",
      "Epoch: 1651 \t|| Train Loss: 0.2619107105492703 \t|| Test Loss: 0.70845788284904\n",
      "Epoch: 1652 \t|| Train Loss: 0.26184454155195785 \t|| Test Loss: 0.7081469528518262\n",
      "Epoch: 1653 \t|| Train Loss: 0.2617783725546453 \t|| Test Loss: 0.7078360228546121\n",
      "Epoch: 1654 \t|| Train Loss: 0.26171220355733293 \t|| Test Loss: 0.7075250928573981\n",
      "Epoch: 1655 \t|| Train Loss: 0.2616460345600204 \t|| Test Loss: 0.707214162860184\n",
      "Epoch: 1656 \t|| Train Loss: 0.26157986556270796 \t|| Test Loss: 0.70690323286297\n",
      "Epoch: 1657 \t|| Train Loss: 0.26151369656539547 \t|| Test Loss: 0.7065923028657561\n",
      "Epoch: 1658 \t|| Train Loss: 0.2614475275680829 \t|| Test Loss: 0.706281372868542\n",
      "Epoch: 1659 \t|| Train Loss: 0.2613813585707705 \t|| Test Loss: 0.705970442871328\n",
      "Epoch: 1660 \t|| Train Loss: 0.261315189573458 \t|| Test Loss: 0.7056595128741139\n",
      "Epoch: 1661 \t|| Train Loss: 0.2612490205761456 \t|| Test Loss: 0.7053485828768999\n",
      "Epoch: 1662 \t|| Train Loss: 0.2611828515788331 \t|| Test Loss: 0.7050376528796859\n",
      "Epoch: 1663 \t|| Train Loss: 0.2611166825815206 \t|| Test Loss: 0.7047267228824718\n",
      "Epoch: 1664 \t|| Train Loss: 0.26105051358420805 \t|| Test Loss: 0.7044157928852579\n",
      "Epoch: 1665 \t|| Train Loss: 0.2609843445868957 \t|| Test Loss: 0.7041048628880437\n",
      "Epoch: 1666 \t|| Train Loss: 0.2609181755895832 \t|| Test Loss: 0.7037939328908299\n",
      "Epoch: 1667 \t|| Train Loss: 0.2608520065922707 \t|| Test Loss: 0.7034830028936158\n",
      "Epoch: 1668 \t|| Train Loss: 0.2607858375949582 \t|| Test Loss: 0.7031720728964019\n",
      "Epoch: 1669 \t|| Train Loss: 0.2607196685976457 \t|| Test Loss: 0.7028611428991878\n",
      "Epoch: 1670 \t|| Train Loss: 0.26065349960033324 \t|| Test Loss: 0.7025502129019738\n",
      "Epoch: 1671 \t|| Train Loss: 0.2605873306030208 \t|| Test Loss: 0.7022392829047599\n",
      "Epoch: 1672 \t|| Train Loss: 0.2605211616057083 \t|| Test Loss: 0.7019283529075457\n",
      "Epoch: 1673 \t|| Train Loss: 0.26045499260839583 \t|| Test Loss: 0.7016174229103318\n",
      "Epoch: 1674 \t|| Train Loss: 0.26038882361108334 \t|| Test Loss: 0.7013064929131178\n",
      "Epoch: 1675 \t|| Train Loss: 0.2603226546137709 \t|| Test Loss: 0.7009955629159037\n",
      "Epoch: 1676 \t|| Train Loss: 0.26025648561645837 \t|| Test Loss: 0.7006846329186895\n",
      "Epoch: 1677 \t|| Train Loss: 0.260190316619146 \t|| Test Loss: 0.7003737029214756\n",
      "Epoch: 1678 \t|| Train Loss: 0.26012414762183345 \t|| Test Loss: 0.7000627729242617\n",
      "Epoch: 1679 \t|| Train Loss: 0.26005797862452096 \t|| Test Loss: 0.6997518429270476\n",
      "Epoch: 1680 \t|| Train Loss: 0.2599918096272085 \t|| Test Loss: 0.6994409129298337\n",
      "Epoch: 1681 \t|| Train Loss: 0.25992564062989604 \t|| Test Loss: 0.6991299829326196\n",
      "Epoch: 1682 \t|| Train Loss: 0.25985947163258355 \t|| Test Loss: 0.6988190529354056\n",
      "Epoch: 1683 \t|| Train Loss: 0.25979330263527106 \t|| Test Loss: 0.6985081229381914\n",
      "Epoch: 1684 \t|| Train Loss: 0.2597271336379586 \t|| Test Loss: 0.6981971929409777\n",
      "Epoch: 1685 \t|| Train Loss: 0.25966096464064614 \t|| Test Loss: 0.6978862629437635\n",
      "Epoch: 1686 \t|| Train Loss: 0.25959479564333365 \t|| Test Loss: 0.6975753329465496\n",
      "Epoch: 1687 \t|| Train Loss: 0.25952862664602117 \t|| Test Loss: 0.6972644029493356\n",
      "Epoch: 1688 \t|| Train Loss: 0.2594624576487087 \t|| Test Loss: 0.6969534729521214\n",
      "Epoch: 1689 \t|| Train Loss: 0.2593962886513962 \t|| Test Loss: 0.6966425429549075\n",
      "Epoch: 1690 \t|| Train Loss: 0.2593301196540837 \t|| Test Loss: 0.6963316129576935\n",
      "Epoch: 1691 \t|| Train Loss: 0.2592639506567712 \t|| Test Loss: 0.6960206829604795\n",
      "Epoch: 1692 \t|| Train Loss: 0.2591977816594588 \t|| Test Loss: 0.6957097529632653\n",
      "Epoch: 1693 \t|| Train Loss: 0.25913161266214624 \t|| Test Loss: 0.6953988229660514\n",
      "Epoch: 1694 \t|| Train Loss: 0.2590654436648338 \t|| Test Loss: 0.6950878929688374\n",
      "Epoch: 1695 \t|| Train Loss: 0.2589992746675214 \t|| Test Loss: 0.6947769629716234\n",
      "Epoch: 1696 \t|| Train Loss: 0.2589331056702089 \t|| Test Loss: 0.6944660329744093\n",
      "Epoch: 1697 \t|| Train Loss: 0.2588669366728964 \t|| Test Loss: 0.6941551029771953\n",
      "Epoch: 1698 \t|| Train Loss: 0.25880076767558396 \t|| Test Loss: 0.6938441729799814\n",
      "Epoch: 1699 \t|| Train Loss: 0.2587345986782714 \t|| Test Loss: 0.6935332429827674\n",
      "Epoch: 1700 \t|| Train Loss: 0.25866842968095893 \t|| Test Loss: 0.6932223129855533\n",
      "Epoch: 1701 \t|| Train Loss: 0.25860226068364656 \t|| Test Loss: 0.6929113829883393\n",
      "Epoch: 1702 \t|| Train Loss: 0.258536091686334 \t|| Test Loss: 0.6926004529911254\n",
      "Epoch: 1703 \t|| Train Loss: 0.2584699226890216 \t|| Test Loss: 0.6922895229939112\n",
      "Epoch: 1704 \t|| Train Loss: 0.2584037536917091 \t|| Test Loss: 0.6919785929966973\n",
      "Epoch: 1705 \t|| Train Loss: 0.2583375846943966 \t|| Test Loss: 0.6916676629994832\n",
      "Epoch: 1706 \t|| Train Loss: 0.2582714156970841 \t|| Test Loss: 0.6913567330022692\n",
      "Epoch: 1707 \t|| Train Loss: 0.25820524669977163 \t|| Test Loss: 0.6910458030050552\n",
      "Epoch: 1708 \t|| Train Loss: 0.25813907770245914 \t|| Test Loss: 0.6907348730078412\n",
      "Epoch: 1709 \t|| Train Loss: 0.2580729087051467 \t|| Test Loss: 0.6904239430106272\n",
      "Epoch: 1710 \t|| Train Loss: 0.2580067397078342 \t|| Test Loss: 0.6901130130134132\n",
      "Epoch: 1711 \t|| Train Loss: 0.25794057071052173 \t|| Test Loss: 0.6898020830161992\n",
      "Epoch: 1712 \t|| Train Loss: 0.25787440171320924 \t|| Test Loss: 0.6894911530189851\n",
      "Epoch: 1713 \t|| Train Loss: 0.25780823271589676 \t|| Test Loss: 0.6891802230217711\n",
      "Epoch: 1714 \t|| Train Loss: 0.2577420637185843 \t|| Test Loss: 0.688869293024557\n",
      "Epoch: 1715 \t|| Train Loss: 0.25767589472127184 \t|| Test Loss: 0.6885583630273432\n",
      "Epoch: 1716 \t|| Train Loss: 0.25760972572395935 \t|| Test Loss: 0.688247433030129\n",
      "Epoch: 1717 \t|| Train Loss: 0.2575435567266469 \t|| Test Loss: 0.6879365030329151\n",
      "Epoch: 1718 \t|| Train Loss: 0.2574773877293344 \t|| Test Loss: 0.687625573035701\n",
      "Epoch: 1719 \t|| Train Loss: 0.25741121873202194 \t|| Test Loss: 0.687314643038487\n",
      "Epoch: 1720 \t|| Train Loss: 0.25734504973470945 \t|| Test Loss: 0.687003713041273\n",
      "Epoch: 1721 \t|| Train Loss: 0.257278880737397 \t|| Test Loss: 0.686692783044059\n",
      "Epoch: 1722 \t|| Train Loss: 0.2572127117400845 \t|| Test Loss: 0.686381853046845\n",
      "Epoch: 1723 \t|| Train Loss: 0.25714654274277204 \t|| Test Loss: 0.6860709230496309\n",
      "Epoch: 1724 \t|| Train Loss: 0.25708037374545956 \t|| Test Loss: 0.685759993052417\n",
      "Epoch: 1725 \t|| Train Loss: 0.25701420474814707 \t|| Test Loss: 0.6854490630552029\n",
      "Epoch: 1726 \t|| Train Loss: 0.2569480357508346 \t|| Test Loss: 0.6851381330579889\n",
      "Epoch: 1727 \t|| Train Loss: 0.2568818667535221 \t|| Test Loss: 0.6848272030607749\n",
      "Epoch: 1728 \t|| Train Loss: 0.2568156977562096 \t|| Test Loss: 0.6845162730635609\n",
      "Epoch: 1729 \t|| Train Loss: 0.2567495287588971 \t|| Test Loss: 0.684205343066347\n",
      "Epoch: 1730 \t|| Train Loss: 0.25668335976158463 \t|| Test Loss: 0.6838944130691329\n",
      "Epoch: 1731 \t|| Train Loss: 0.25661719076427214 \t|| Test Loss: 0.6835834830719189\n",
      "Epoch: 1732 \t|| Train Loss: 0.2565510217669597 \t|| Test Loss: 0.6832725530747048\n",
      "Epoch: 1733 \t|| Train Loss: 0.2564848527696473 \t|| Test Loss: 0.6829616230774909\n",
      "Epoch: 1734 \t|| Train Loss: 0.2564186837723348 \t|| Test Loss: 0.6826506930802767\n",
      "Epoch: 1735 \t|| Train Loss: 0.2563525147750223 \t|| Test Loss: 0.6823397630830627\n",
      "Epoch: 1736 \t|| Train Loss: 0.2562863457777098 \t|| Test Loss: 0.6820288330858487\n",
      "Epoch: 1737 \t|| Train Loss: 0.2562201767803973 \t|| Test Loss: 0.6817179030886348\n",
      "Epoch: 1738 \t|| Train Loss: 0.2561540077830849 \t|| Test Loss: 0.6814069730914207\n",
      "Epoch: 1739 \t|| Train Loss: 0.2560878387857724 \t|| Test Loss: 0.6810960430942067\n",
      "Epoch: 1740 \t|| Train Loss: 0.25602166978845997 \t|| Test Loss: 0.6807851130969926\n",
      "Epoch: 1741 \t|| Train Loss: 0.2559555007911475 \t|| Test Loss: 0.6804741830997786\n",
      "Epoch: 1742 \t|| Train Loss: 0.255889331793835 \t|| Test Loss: 0.6801632531025648\n",
      "Epoch: 1743 \t|| Train Loss: 0.2558231627965225 \t|| Test Loss: 0.6798523231053506\n",
      "Epoch: 1744 \t|| Train Loss: 0.25575699379921 \t|| Test Loss: 0.6795413931081367\n",
      "Epoch: 1745 \t|| Train Loss: 0.25569082480189753 \t|| Test Loss: 0.6792304631109226\n",
      "Epoch: 1746 \t|| Train Loss: 0.25562465580458504 \t|| Test Loss: 0.6789195331137086\n",
      "Epoch: 1747 \t|| Train Loss: 0.25555848680727256 \t|| Test Loss: 0.6786086031164945\n",
      "Epoch: 1748 \t|| Train Loss: 0.25549231780996007 \t|| Test Loss: 0.6782976731192806\n",
      "Epoch: 1749 \t|| Train Loss: 0.25542614881264764 \t|| Test Loss: 0.6779867431220664\n",
      "Epoch: 1750 \t|| Train Loss: 0.25535997981533515 \t|| Test Loss: 0.6776758131248524\n",
      "Epoch: 1751 \t|| Train Loss: 0.25529381081802266 \t|| Test Loss: 0.6773648831276387\n",
      "Epoch: 1752 \t|| Train Loss: 0.2552276418207102 \t|| Test Loss: 0.6770539531304245\n",
      "Epoch: 1753 \t|| Train Loss: 0.2551614728233977 \t|| Test Loss: 0.6767430231332104\n",
      "Epoch: 1754 \t|| Train Loss: 0.2550953038260852 \t|| Test Loss: 0.6764320931359964\n",
      "Epoch: 1755 \t|| Train Loss: 0.25502913482877276 \t|| Test Loss: 0.6761211631387825\n",
      "Epoch: 1756 \t|| Train Loss: 0.2549629658314603 \t|| Test Loss: 0.6758102331415685\n",
      "Epoch: 1757 \t|| Train Loss: 0.25489679683414784 \t|| Test Loss: 0.6754993031443544\n",
      "Epoch: 1758 \t|| Train Loss: 0.25483062783683536 \t|| Test Loss: 0.6751883731471404\n",
      "Epoch: 1759 \t|| Train Loss: 0.2547644588395229 \t|| Test Loss: 0.6748774431499263\n",
      "Epoch: 1760 \t|| Train Loss: 0.2546982898422104 \t|| Test Loss: 0.6745665131527124\n",
      "Epoch: 1761 \t|| Train Loss: 0.2546321208448979 \t|| Test Loss: 0.6742555831554983\n",
      "Epoch: 1762 \t|| Train Loss: 0.25456595184758546 \t|| Test Loss: 0.6739446531582842\n",
      "Epoch: 1763 \t|| Train Loss: 0.25449978285027297 \t|| Test Loss: 0.6736337231610703\n",
      "Epoch: 1764 \t|| Train Loss: 0.2544336138529605 \t|| Test Loss: 0.6733227931638563\n",
      "Epoch: 1765 \t|| Train Loss: 0.254367444855648 \t|| Test Loss: 0.6730118631666423\n",
      "Epoch: 1766 \t|| Train Loss: 0.2543012758583355 \t|| Test Loss: 0.6727009331694283\n",
      "Epoch: 1767 \t|| Train Loss: 0.2542351068610231 \t|| Test Loss: 0.6723900031722142\n",
      "Epoch: 1768 \t|| Train Loss: 0.2541689378637106 \t|| Test Loss: 0.6720790731750002\n",
      "Epoch: 1769 \t|| Train Loss: 0.2541027688663981 \t|| Test Loss: 0.6717681431777862\n",
      "Epoch: 1770 \t|| Train Loss: 0.2540365998690856 \t|| Test Loss: 0.6714572131805723\n",
      "Epoch: 1771 \t|| Train Loss: 0.2539704308717731 \t|| Test Loss: 0.6711462831833582\n",
      "Epoch: 1772 \t|| Train Loss: 0.2539042618744607 \t|| Test Loss: 0.6708353531861441\n",
      "Epoch: 1773 \t|| Train Loss: 0.25383809287714826 \t|| Test Loss: 0.6705244231889301\n",
      "Epoch: 1774 \t|| Train Loss: 0.2537719238798357 \t|| Test Loss: 0.6702134931917161\n",
      "Epoch: 1775 \t|| Train Loss: 0.2537057548825232 \t|| Test Loss: 0.6699025631945019\n",
      "Epoch: 1776 \t|| Train Loss: 0.2536395858852108 \t|| Test Loss: 0.6695916331972882\n",
      "Epoch: 1777 \t|| Train Loss: 0.2535734168878983 \t|| Test Loss: 0.669280703200074\n",
      "Epoch: 1778 \t|| Train Loss: 0.2535072478905858 \t|| Test Loss: 0.66896977320286\n",
      "Epoch: 1779 \t|| Train Loss: 0.2534410788932734 \t|| Test Loss: 0.6686588432056461\n",
      "Epoch: 1780 \t|| Train Loss: 0.2533749098959609 \t|| Test Loss: 0.6683479132084319\n",
      "Epoch: 1781 \t|| Train Loss: 0.2533087408986484 \t|| Test Loss: 0.668036983211218\n",
      "Epoch: 1782 \t|| Train Loss: 0.2532425719013359 \t|| Test Loss: 0.667726053214004\n",
      "Epoch: 1783 \t|| Train Loss: 0.25317640290402343 \t|| Test Loss: 0.66741512321679\n",
      "Epoch: 1784 \t|| Train Loss: 0.25311023390671095 \t|| Test Loss: 0.6671041932195759\n",
      "Epoch: 1785 \t|| Train Loss: 0.2530526529063848 \t|| Test Loss: 0.6668592832218441\n",
      "Epoch: 1786 \t|| Train Loss: 0.2530021919085288 \t|| Test Loss: 0.6666143732241119\n",
      "Epoch: 1787 \t|| Train Loss: 0.2529517309106728 \t|| Test Loss: 0.6663694632263799\n",
      "Epoch: 1788 \t|| Train Loss: 0.25290126991281686 \t|| Test Loss: 0.6661245532286479\n",
      "Epoch: 1789 \t|| Train Loss: 0.2528508089149609 \t|| Test Loss: 0.6658796432309161\n",
      "Epoch: 1790 \t|| Train Loss: 0.25280034791710493 \t|| Test Loss: 0.665634733233184\n",
      "Epoch: 1791 \t|| Train Loss: 0.25274988691924893 \t|| Test Loss: 0.665389823235452\n",
      "Epoch: 1792 \t|| Train Loss: 0.25269942592139294 \t|| Test Loss: 0.6651449132377201\n",
      "Epoch: 1793 \t|| Train Loss: 0.25264896492353695 \t|| Test Loss: 0.664900003239988\n",
      "Epoch: 1794 \t|| Train Loss: 0.25259850392568095 \t|| Test Loss: 0.664655093242256\n",
      "Epoch: 1795 \t|| Train Loss: 0.25254804292782496 \t|| Test Loss: 0.664410183244524\n",
      "Epoch: 1796 \t|| Train Loss: 0.25249758192996896 \t|| Test Loss: 0.6641652732467922\n",
      "Epoch: 1797 \t|| Train Loss: 0.252447120932113 \t|| Test Loss: 0.66392036324906\n",
      "Epoch: 1798 \t|| Train Loss: 0.25239665993425703 \t|| Test Loss: 0.6636754532513282\n",
      "Epoch: 1799 \t|| Train Loss: 0.25234619893640103 \t|| Test Loss: 0.6634305432535962\n",
      "Epoch: 1800 \t|| Train Loss: 0.25229573793854504 \t|| Test Loss: 0.6631856332558641\n",
      "Epoch: 1801 \t|| Train Loss: 0.25224527694068904 \t|| Test Loss: 0.6629407232581321\n",
      "Epoch: 1802 \t|| Train Loss: 0.25219481594283316 \t|| Test Loss: 0.6626958132604002\n",
      "Epoch: 1803 \t|| Train Loss: 0.2521443549449771 \t|| Test Loss: 0.6624509032626682\n",
      "Epoch: 1804 \t|| Train Loss: 0.2520938939471211 \t|| Test Loss: 0.6622059932649362\n",
      "Epoch: 1805 \t|| Train Loss: 0.2520434329492652 \t|| Test Loss: 0.6619610832672043\n",
      "Epoch: 1806 \t|| Train Loss: 0.2519929719514092 \t|| Test Loss: 0.6617161732694723\n",
      "Epoch: 1807 \t|| Train Loss: 0.2519425109535532 \t|| Test Loss: 0.6614712632717403\n",
      "Epoch: 1808 \t|| Train Loss: 0.2518920499556972 \t|| Test Loss: 0.6612263532740081\n",
      "Epoch: 1809 \t|| Train Loss: 0.25184158895784126 \t|| Test Loss: 0.6609814432762763\n",
      "Epoch: 1810 \t|| Train Loss: 0.25179112795998526 \t|| Test Loss: 0.6607365332785443\n",
      "Epoch: 1811 \t|| Train Loss: 0.25174066696212927 \t|| Test Loss: 0.6604916232808123\n",
      "Epoch: 1812 \t|| Train Loss: 0.2516902059642733 \t|| Test Loss: 0.6602467132830803\n",
      "Epoch: 1813 \t|| Train Loss: 0.2516397449664173 \t|| Test Loss: 0.6600018032853483\n",
      "Epoch: 1814 \t|| Train Loss: 0.25158928396856134 \t|| Test Loss: 0.6597568932876164\n",
      "Epoch: 1815 \t|| Train Loss: 0.2515388229707053 \t|| Test Loss: 0.6595119832898844\n",
      "Epoch: 1816 \t|| Train Loss: 0.25148836197284935 \t|| Test Loss: 0.6592670732921524\n",
      "Epoch: 1817 \t|| Train Loss: 0.2514379009749934 \t|| Test Loss: 0.6590221632944203\n",
      "Epoch: 1818 \t|| Train Loss: 0.2513874399771374 \t|| Test Loss: 0.6587772532966885\n",
      "Epoch: 1819 \t|| Train Loss: 0.2513369789792814 \t|| Test Loss: 0.6585323432989564\n",
      "Epoch: 1820 \t|| Train Loss: 0.25128651798142543 \t|| Test Loss: 0.6582874333012244\n",
      "Epoch: 1821 \t|| Train Loss: 0.25123605698356943 \t|| Test Loss: 0.6580425233034924\n",
      "Epoch: 1822 \t|| Train Loss: 0.2511855959857135 \t|| Test Loss: 0.6577976133057605\n",
      "Epoch: 1823 \t|| Train Loss: 0.25113513498785744 \t|| Test Loss: 0.6575527033080284\n",
      "Epoch: 1824 \t|| Train Loss: 0.2510846739900015 \t|| Test Loss: 0.6573077933102965\n",
      "Epoch: 1825 \t|| Train Loss: 0.25103421299214557 \t|| Test Loss: 0.6570628833125645\n",
      "Epoch: 1826 \t|| Train Loss: 0.2509837519942896 \t|| Test Loss: 0.6568179733148324\n",
      "Epoch: 1827 \t|| Train Loss: 0.2509332909964335 \t|| Test Loss: 0.6565730633171004\n",
      "Epoch: 1828 \t|| Train Loss: 0.25088282999857753 \t|| Test Loss: 0.6563281533193684\n",
      "Epoch: 1829 \t|| Train Loss: 0.2508323690007216 \t|| Test Loss: 0.6560832433216366\n",
      "Epoch: 1830 \t|| Train Loss: 0.2507819080028656 \t|| Test Loss: 0.6558383333239044\n",
      "Epoch: 1831 \t|| Train Loss: 0.2507314470050096 \t|| Test Loss: 0.6555934233261727\n",
      "Epoch: 1832 \t|| Train Loss: 0.25068098600715366 \t|| Test Loss: 0.6553485133284405\n",
      "Epoch: 1833 \t|| Train Loss: 0.25063052500929767 \t|| Test Loss: 0.6551036033307085\n",
      "Epoch: 1834 \t|| Train Loss: 0.2505800640114417 \t|| Test Loss: 0.6548586933329765\n",
      "Epoch: 1835 \t|| Train Loss: 0.2505296030135857 \t|| Test Loss: 0.6546137833352446\n",
      "Epoch: 1836 \t|| Train Loss: 0.25047914201572974 \t|| Test Loss: 0.6543688733375126\n",
      "Epoch: 1837 \t|| Train Loss: 0.25042868101787374 \t|| Test Loss: 0.6541239633397806\n",
      "Epoch: 1838 \t|| Train Loss: 0.25037822002001775 \t|| Test Loss: 0.6538790533420487\n",
      "Epoch: 1839 \t|| Train Loss: 0.25032775902216176 \t|| Test Loss: 0.6536341433443166\n",
      "Epoch: 1840 \t|| Train Loss: 0.25027729802430576 \t|| Test Loss: 0.6533892333465847\n",
      "Epoch: 1841 \t|| Train Loss: 0.2502268370264498 \t|| Test Loss: 0.6531443233488526\n",
      "Epoch: 1842 \t|| Train Loss: 0.2501763760285938 \t|| Test Loss: 0.6528994133511207\n",
      "Epoch: 1843 \t|| Train Loss: 0.2501259150307379 \t|| Test Loss: 0.6526545033533887\n",
      "Epoch: 1844 \t|| Train Loss: 0.25007545403288184 \t|| Test Loss: 0.6524095933556567\n",
      "Epoch: 1845 \t|| Train Loss: 0.25002499303502584 \t|| Test Loss: 0.6521646833579247\n",
      "Epoch: 1846 \t|| Train Loss: 0.24997453203716996 \t|| Test Loss: 0.6519197733601927\n",
      "Epoch: 1847 \t|| Train Loss: 0.24992407103931388 \t|| Test Loss: 0.6516748633624607\n",
      "Epoch: 1848 \t|| Train Loss: 0.24987361004145794 \t|| Test Loss: 0.6514299533647288\n",
      "Epoch: 1849 \t|| Train Loss: 0.24982314904360195 \t|| Test Loss: 0.6511850433669968\n",
      "Epoch: 1850 \t|| Train Loss: 0.24977268804574598 \t|| Test Loss: 0.6509401333692647\n",
      "Epoch: 1851 \t|| Train Loss: 0.24972222704789 \t|| Test Loss: 0.6506952233715328\n",
      "Epoch: 1852 \t|| Train Loss: 0.249671766050034 \t|| Test Loss: 0.6504503133738009\n",
      "Epoch: 1853 \t|| Train Loss: 0.24962130505217806 \t|| Test Loss: 0.6502054033760688\n",
      "Epoch: 1854 \t|| Train Loss: 0.24957084405432206 \t|| Test Loss: 0.6499604933783368\n",
      "Epoch: 1855 \t|| Train Loss: 0.24952038305646607 \t|| Test Loss: 0.649715583380605\n",
      "Epoch: 1856 \t|| Train Loss: 0.24946992205861007 \t|| Test Loss: 0.6494706733828728\n",
      "Epoch: 1857 \t|| Train Loss: 0.24941946106075408 \t|| Test Loss: 0.6492257633851409\n",
      "Epoch: 1858 \t|| Train Loss: 0.24936900006289814 \t|| Test Loss: 0.6489808533874089\n",
      "Epoch: 1859 \t|| Train Loss: 0.24931853906504214 \t|| Test Loss: 0.6487359433896768\n",
      "Epoch: 1860 \t|| Train Loss: 0.24926807806718615 \t|| Test Loss: 0.6484910333919448\n",
      "Epoch: 1861 \t|| Train Loss: 0.24921761706933016 \t|| Test Loss: 0.648246123394213\n",
      "Epoch: 1862 \t|| Train Loss: 0.2491671560714742 \t|| Test Loss: 0.648001213396481\n",
      "Epoch: 1863 \t|| Train Loss: 0.2491166950736182 \t|| Test Loss: 0.6477563033987488\n",
      "Epoch: 1864 \t|| Train Loss: 0.24906623407576226 \t|| Test Loss: 0.6475113934010169\n",
      "Epoch: 1865 \t|| Train Loss: 0.24901577307790626 \t|| Test Loss: 0.647266483403285\n",
      "Epoch: 1866 \t|| Train Loss: 0.24896531208005027 \t|| Test Loss: 0.6470215734055531\n",
      "Epoch: 1867 \t|| Train Loss: 0.2489148510821943 \t|| Test Loss: 0.6467766634078209\n",
      "Epoch: 1868 \t|| Train Loss: 0.24886439008433828 \t|| Test Loss: 0.646531753410089\n",
      "Epoch: 1869 \t|| Train Loss: 0.2488139290864823 \t|| Test Loss: 0.646286843412357\n",
      "Epoch: 1870 \t|| Train Loss: 0.24876346808862632 \t|| Test Loss: 0.646041933414625\n",
      "Epoch: 1871 \t|| Train Loss: 0.24871300709077032 \t|| Test Loss: 0.645797023416893\n",
      "Epoch: 1872 \t|| Train Loss: 0.24866254609291433 \t|| Test Loss: 0.645552113419161\n",
      "Epoch: 1873 \t|| Train Loss: 0.2486120850950584 \t|| Test Loss: 0.6453072034214291\n",
      "Epoch: 1874 \t|| Train Loss: 0.2485616240972024 \t|| Test Loss: 0.645062293423697\n",
      "Epoch: 1875 \t|| Train Loss: 0.2485111630993464 \t|| Test Loss: 0.6448173834259651\n",
      "Epoch: 1876 \t|| Train Loss: 0.2484607021014904 \t|| Test Loss: 0.6445724734282331\n",
      "Epoch: 1877 \t|| Train Loss: 0.2484102411036345 \t|| Test Loss: 0.6443275634305011\n",
      "Epoch: 1878 \t|| Train Loss: 0.24835978010577847 \t|| Test Loss: 0.644082653432769\n",
      "Epoch: 1879 \t|| Train Loss: 0.24830931910792248 \t|| Test Loss: 0.6438377434350372\n",
      "Epoch: 1880 \t|| Train Loss: 0.2482588581100665 \t|| Test Loss: 0.6435928334373051\n",
      "Epoch: 1881 \t|| Train Loss: 0.24820839711221057 \t|| Test Loss: 0.6433479234395731\n",
      "Epoch: 1882 \t|| Train Loss: 0.24815793611435452 \t|| Test Loss: 0.6431030134418412\n",
      "Epoch: 1883 \t|| Train Loss: 0.24810747511649858 \t|| Test Loss: 0.6428581034441092\n",
      "Epoch: 1884 \t|| Train Loss: 0.2480570141186426 \t|| Test Loss: 0.6426131934463772\n",
      "Epoch: 1885 \t|| Train Loss: 0.2480065531207866 \t|| Test Loss: 0.6423682834486452\n",
      "Epoch: 1886 \t|| Train Loss: 0.24795609212293063 \t|| Test Loss: 0.6421233734509133\n",
      "Epoch: 1887 \t|| Train Loss: 0.2479056311250746 \t|| Test Loss: 0.6418784634531812\n",
      "Epoch: 1888 \t|| Train Loss: 0.24785517012721864 \t|| Test Loss: 0.6416335534554493\n",
      "Epoch: 1889 \t|| Train Loss: 0.2478047091293627 \t|| Test Loss: 0.6413886434577172\n",
      "Epoch: 1890 \t|| Train Loss: 0.2477542481315067 \t|| Test Loss: 0.6411437334599853\n",
      "Epoch: 1891 \t|| Train Loss: 0.2477037871336507 \t|| Test Loss: 0.6408988234622532\n",
      "Epoch: 1892 \t|| Train Loss: 0.24765332613579472 \t|| Test Loss: 0.6406539134645213\n",
      "Epoch: 1893 \t|| Train Loss: 0.24760286513793878 \t|| Test Loss: 0.6404090034667893\n",
      "Epoch: 1894 \t|| Train Loss: 0.24755240414008278 \t|| Test Loss: 0.6401640934690573\n",
      "Epoch: 1895 \t|| Train Loss: 0.24750194314222673 \t|| Test Loss: 0.6399191834713254\n",
      "Epoch: 1896 \t|| Train Loss: 0.2474514821443708 \t|| Test Loss: 0.6396742734735934\n",
      "Epoch: 1897 \t|| Train Loss: 0.24740102114651483 \t|| Test Loss: 0.6394293634758614\n",
      "Epoch: 1898 \t|| Train Loss: 0.24735056014865883 \t|| Test Loss: 0.6391844534781294\n",
      "Epoch: 1899 \t|| Train Loss: 0.24730009915080284 \t|| Test Loss: 0.6389395434803973\n",
      "Epoch: 1900 \t|| Train Loss: 0.24724963815294684 \t|| Test Loss: 0.6386946334826654\n",
      "Epoch: 1901 \t|| Train Loss: 0.2471991771550909 \t|| Test Loss: 0.6384497234849335\n",
      "Epoch: 1902 \t|| Train Loss: 0.2471487161572349 \t|| Test Loss: 0.6382048134872014\n",
      "Epoch: 1903 \t|| Train Loss: 0.24709825515937886 \t|| Test Loss: 0.6379599034894695\n",
      "Epoch: 1904 \t|| Train Loss: 0.24704779416152295 \t|| Test Loss: 0.6377149934917374\n",
      "Epoch: 1905 \t|| Train Loss: 0.24699733316366695 \t|| Test Loss: 0.6374700834940054\n",
      "Epoch: 1906 \t|| Train Loss: 0.24694687216581102 \t|| Test Loss: 0.6372251734962735\n",
      "Epoch: 1907 \t|| Train Loss: 0.24689641116795502 \t|| Test Loss: 0.6369802634985415\n",
      "Epoch: 1908 \t|| Train Loss: 0.24684595017009903 \t|| Test Loss: 0.6367353535008095\n",
      "Epoch: 1909 \t|| Train Loss: 0.2467954891722431 \t|| Test Loss: 0.6364904435030775\n",
      "Epoch: 1910 \t|| Train Loss: 0.2467450281743871 \t|| Test Loss: 0.6362455335053455\n",
      "Epoch: 1911 \t|| Train Loss: 0.2466945671765311 \t|| Test Loss: 0.6360006235076134\n",
      "Epoch: 1912 \t|| Train Loss: 0.2466441061786751 \t|| Test Loss: 0.6357557135098816\n",
      "Epoch: 1913 \t|| Train Loss: 0.2465936451808191 \t|| Test Loss: 0.6355108035121495\n",
      "Epoch: 1914 \t|| Train Loss: 0.24654318418296314 \t|| Test Loss: 0.6352658935144175\n",
      "Epoch: 1915 \t|| Train Loss: 0.24649272318510715 \t|| Test Loss: 0.6350209835166856\n",
      "Epoch: 1916 \t|| Train Loss: 0.24644226218725115 \t|| Test Loss: 0.6347760735189536\n",
      "Epoch: 1917 \t|| Train Loss: 0.24639180118939516 \t|| Test Loss: 0.6345311635212216\n",
      "Epoch: 1918 \t|| Train Loss: 0.24634134019153922 \t|| Test Loss: 0.6342862535234897\n",
      "Epoch: 1919 \t|| Train Loss: 0.24629087919368323 \t|| Test Loss: 0.6340413435257577\n",
      "Epoch: 1920 \t|| Train Loss: 0.24624041819582723 \t|| Test Loss: 0.6337964335280256\n",
      "Epoch: 1921 \t|| Train Loss: 0.24618995719797124 \t|| Test Loss: 0.6335515235302938\n",
      "Epoch: 1922 \t|| Train Loss: 0.24613949620011527 \t|| Test Loss: 0.6333066135325617\n",
      "Epoch: 1923 \t|| Train Loss: 0.24608903520225928 \t|| Test Loss: 0.6330617035348297\n",
      "Epoch: 1924 \t|| Train Loss: 0.24603857420440328 \t|| Test Loss: 0.6328167935370976\n",
      "Epoch: 1925 \t|| Train Loss: 0.24598811320654734 \t|| Test Loss: 0.6325718835393658\n",
      "Epoch: 1926 \t|| Train Loss: 0.24593765220869135 \t|| Test Loss: 0.6323269735416337\n",
      "Epoch: 1927 \t|| Train Loss: 0.24588719121083535 \t|| Test Loss: 0.6320820635439017\n",
      "Epoch: 1928 \t|| Train Loss: 0.24583673021297942 \t|| Test Loss: 0.6318371535461698\n",
      "Epoch: 1929 \t|| Train Loss: 0.24578626921512337 \t|| Test Loss: 0.6315922435484378\n",
      "Epoch: 1930 \t|| Train Loss: 0.24573580821726743 \t|| Test Loss: 0.6313473335507058\n",
      "Epoch: 1931 \t|| Train Loss: 0.24568534721941146 \t|| Test Loss: 0.6311024235529739\n",
      "Epoch: 1932 \t|| Train Loss: 0.24563488622155544 \t|| Test Loss: 0.6308575135552419\n",
      "Epoch: 1933 \t|| Train Loss: 0.24558442522369947 \t|| Test Loss: 0.6306126035575098\n",
      "Epoch: 1934 \t|| Train Loss: 0.24553396422584348 \t|| Test Loss: 0.6303676935597778\n",
      "Epoch: 1935 \t|| Train Loss: 0.24548350322798748 \t|| Test Loss: 0.6301227835620458\n",
      "Epoch: 1936 \t|| Train Loss: 0.2454330422301315 \t|| Test Loss: 0.6298778735643139\n",
      "Epoch: 1937 \t|| Train Loss: 0.2453825812322755 \t|| Test Loss: 0.6296329635665818\n",
      "Epoch: 1938 \t|| Train Loss: 0.24533212023441955 \t|| Test Loss: 0.62938805356885\n",
      "Epoch: 1939 \t|| Train Loss: 0.2452816592365636 \t|| Test Loss: 0.6291431435711179\n",
      "Epoch: 1940 \t|| Train Loss: 0.24523119823870757 \t|| Test Loss: 0.6288982335733858\n",
      "Epoch: 1941 \t|| Train Loss: 0.24518073724085165 \t|| Test Loss: 0.6286533235756538\n",
      "Epoch: 1942 \t|| Train Loss: 0.2451302762429956 \t|| Test Loss: 0.628408413577922\n",
      "Epoch: 1943 \t|| Train Loss: 0.2450798152451396 \t|| Test Loss: 0.6281635035801899\n",
      "Epoch: 1944 \t|| Train Loss: 0.24502935424728367 \t|| Test Loss: 0.6279185935824578\n",
      "Epoch: 1945 \t|| Train Loss: 0.24497889324942768 \t|| Test Loss: 0.627673683584726\n",
      "Epoch: 1946 \t|| Train Loss: 0.24492843225157174 \t|| Test Loss: 0.6274287735869939\n",
      "Epoch: 1947 \t|| Train Loss: 0.2448779712537157 \t|| Test Loss: 0.6271838635892619\n",
      "Epoch: 1948 \t|| Train Loss: 0.24482751025585975 \t|| Test Loss: 0.62693895359153\n",
      "Epoch: 1949 \t|| Train Loss: 0.24477704925800375 \t|| Test Loss: 0.626694043593798\n",
      "Epoch: 1950 \t|| Train Loss: 0.24472658826014776 \t|| Test Loss: 0.626449133596066\n",
      "Epoch: 1951 \t|| Train Loss: 0.2446761272622918 \t|| Test Loss: 0.626204223598334\n",
      "Epoch: 1952 \t|| Train Loss: 0.2446256662644358 \t|| Test Loss: 0.6259593136006021\n",
      "Epoch: 1953 \t|| Train Loss: 0.24457520526657986 \t|| Test Loss: 0.6257144036028702\n",
      "Epoch: 1954 \t|| Train Loss: 0.2445247442687238 \t|| Test Loss: 0.6254694936051381\n",
      "Epoch: 1955 \t|| Train Loss: 0.24447428327086787 \t|| Test Loss: 0.6252245836074061\n",
      "Epoch: 1956 \t|| Train Loss: 0.2444238222730119 \t|| Test Loss: 0.6249796736096741\n",
      "Epoch: 1957 \t|| Train Loss: 0.2443733612751559 \t|| Test Loss: 0.6247347636119421\n",
      "Epoch: 1958 \t|| Train Loss: 0.24432290027729991 \t|| Test Loss: 0.6244898536142102\n",
      "Epoch: 1959 \t|| Train Loss: 0.24427243927944392 \t|| Test Loss: 0.6242449436164781\n",
      "Epoch: 1960 \t|| Train Loss: 0.24422197828158793 \t|| Test Loss: 0.6240000336187462\n",
      "Epoch: 1961 \t|| Train Loss: 0.244171517283732 \t|| Test Loss: 0.6237551236210142\n",
      "Epoch: 1962 \t|| Train Loss: 0.244121056285876 \t|| Test Loss: 0.6235102136232822\n",
      "Epoch: 1963 \t|| Train Loss: 0.24407059528802 \t|| Test Loss: 0.6232653036255502\n",
      "Epoch: 1964 \t|| Train Loss: 0.244020134290164 \t|| Test Loss: 0.6230203936278182\n",
      "Epoch: 1965 \t|| Train Loss: 0.243969673292308 \t|| Test Loss: 0.6227754836300863\n",
      "Epoch: 1966 \t|| Train Loss: 0.2439192122944521 \t|| Test Loss: 0.6225305736323543\n",
      "Epoch: 1967 \t|| Train Loss: 0.24386875129659602 \t|| Test Loss: 0.6222856636346223\n",
      "Epoch: 1968 \t|| Train Loss: 0.2438182902987401 \t|| Test Loss: 0.6220407536368902\n",
      "Epoch: 1969 \t|| Train Loss: 0.24376782930088411 \t|| Test Loss: 0.6217958436391583\n",
      "Epoch: 1970 \t|| Train Loss: 0.24371736830302812 \t|| Test Loss: 0.6215509336414262\n",
      "Epoch: 1971 \t|| Train Loss: 0.24366690730517213 \t|| Test Loss: 0.6213060236436944\n",
      "Epoch: 1972 \t|| Train Loss: 0.2436164463073162 \t|| Test Loss: 0.6210611136459623\n",
      "Epoch: 1973 \t|| Train Loss: 0.2435678298857582 \t|| Test Loss: 0.6208831136481993\n",
      "Epoch: 1974 \t|| Train Loss: 0.2435278298876208 \t|| Test Loss: 0.6207051136504365\n",
      "Epoch: 1975 \t|| Train Loss: 0.24348782988948345 \t|| Test Loss: 0.6205271136526734\n",
      "Epoch: 1976 \t|| Train Loss: 0.24344782989134614 \t|| Test Loss: 0.6203491136549106\n",
      "Epoch: 1977 \t|| Train Loss: 0.2434078298932088 \t|| Test Loss: 0.6201711136571475\n",
      "Epoch: 1978 \t|| Train Loss: 0.24336782989507139 \t|| Test Loss: 0.6199931136593846\n",
      "Epoch: 1979 \t|| Train Loss: 0.24332782989693405 \t|| Test Loss: 0.6198151136616217\n",
      "Epoch: 1980 \t|| Train Loss: 0.2432878298987967 \t|| Test Loss: 0.6196371136638588\n",
      "Epoch: 1981 \t|| Train Loss: 0.24324782990065935 \t|| Test Loss: 0.6194591136660957\n",
      "Epoch: 1982 \t|| Train Loss: 0.24320782990252204 \t|| Test Loss: 0.6192811136683328\n",
      "Epoch: 1983 \t|| Train Loss: 0.24316782990438462 \t|| Test Loss: 0.6191031136705698\n",
      "Epoch: 1984 \t|| Train Loss: 0.24312782990624732 \t|| Test Loss: 0.6189251136728069\n",
      "Epoch: 1985 \t|| Train Loss: 0.24308782990810998 \t|| Test Loss: 0.6187471136750439\n",
      "Epoch: 1986 \t|| Train Loss: 0.24304782990997262 \t|| Test Loss: 0.6185691136772811\n",
      "Epoch: 1987 \t|| Train Loss: 0.24300782991183528 \t|| Test Loss: 0.6183911136795179\n",
      "Epoch: 1988 \t|| Train Loss: 0.24296782991369792 \t|| Test Loss: 0.6182131136817551\n",
      "Epoch: 1989 \t|| Train Loss: 0.24292782991556056 \t|| Test Loss: 0.6180351136839922\n",
      "Epoch: 1990 \t|| Train Loss: 0.24288782991742322 \t|| Test Loss: 0.6178571136862293\n",
      "Epoch: 1991 \t|| Train Loss: 0.24284782991928586 \t|| Test Loss: 0.6176791136884663\n",
      "Epoch: 1992 \t|| Train Loss: 0.24280782992114847 \t|| Test Loss: 0.6175011136907034\n",
      "Epoch: 1993 \t|| Train Loss: 0.24276782992301116 \t|| Test Loss: 0.6173231136929405\n",
      "Epoch: 1994 \t|| Train Loss: 0.2427278299248738 \t|| Test Loss: 0.6171451136951774\n",
      "Epoch: 1995 \t|| Train Loss: 0.2426878299267364 \t|| Test Loss: 0.6169671136974146\n",
      "Epoch: 1996 \t|| Train Loss: 0.2426478299285991 \t|| Test Loss: 0.6167891136996516\n",
      "Epoch: 1997 \t|| Train Loss: 0.24260782993046176 \t|| Test Loss: 0.6166111137018886\n",
      "Epoch: 1998 \t|| Train Loss: 0.2425678299323244 \t|| Test Loss: 0.6164331137041257\n",
      "Epoch: 1999 \t|| Train Loss: 0.24252782993418703 \t|| Test Loss: 0.6162551137063628\n",
      "Epoch: 2000 \t|| Train Loss: 0.2424878299360497 \t|| Test Loss: 0.6160771137085999\n",
      "Epoch: 2001 \t|| Train Loss: 0.24244782993791233 \t|| Test Loss: 0.6158991137108368\n",
      "Epoch: 2002 \t|| Train Loss: 0.242407829939775 \t|| Test Loss: 0.6157211137130738\n",
      "Epoch: 2003 \t|| Train Loss: 0.24236782994163764 \t|| Test Loss: 0.6155431137153109\n",
      "Epoch: 2004 \t|| Train Loss: 0.24232782994350027 \t|| Test Loss: 0.615365113717548\n",
      "Epoch: 2005 \t|| Train Loss: 0.24228782994536294 \t|| Test Loss: 0.6151871137197851\n",
      "Epoch: 2006 \t|| Train Loss: 0.24224782994722557 \t|| Test Loss: 0.615009113722022\n",
      "Epoch: 2007 \t|| Train Loss: 0.24220782994908827 \t|| Test Loss: 0.6148311137242591\n",
      "Epoch: 2008 \t|| Train Loss: 0.24216782995095087 \t|| Test Loss: 0.6146531137264961\n",
      "Epoch: 2009 \t|| Train Loss: 0.2421278299528135 \t|| Test Loss: 0.6144751137287332\n",
      "Epoch: 2010 \t|| Train Loss: 0.24208782995467618 \t|| Test Loss: 0.6142971137309704\n",
      "Epoch: 2011 \t|| Train Loss: 0.2420478299565388 \t|| Test Loss: 0.6141191137332074\n",
      "Epoch: 2012 \t|| Train Loss: 0.24200782995840148 \t|| Test Loss: 0.6139411137354444\n",
      "Epoch: 2013 \t|| Train Loss: 0.2419678299602641 \t|| Test Loss: 0.6137631137376814\n",
      "Epoch: 2014 \t|| Train Loss: 0.24192782996212675 \t|| Test Loss: 0.6135851137399186\n",
      "Epoch: 2015 \t|| Train Loss: 0.24188782996398941 \t|| Test Loss: 0.6134071137421555\n",
      "Epoch: 2016 \t|| Train Loss: 0.24184782996585205 \t|| Test Loss: 0.6132291137443925\n",
      "Epoch: 2017 \t|| Train Loss: 0.24180782996771472 \t|| Test Loss: 0.6130511137466297\n",
      "Epoch: 2018 \t|| Train Loss: 0.24176782996957735 \t|| Test Loss: 0.6128731137488668\n",
      "Epoch: 2019 \t|| Train Loss: 0.24172782997144004 \t|| Test Loss: 0.6126951137511039\n",
      "Epoch: 2020 \t|| Train Loss: 0.24168782997330265 \t|| Test Loss: 0.6125171137533408\n",
      "Epoch: 2021 \t|| Train Loss: 0.2416478299751653 \t|| Test Loss: 0.6123391137555778\n",
      "Epoch: 2022 \t|| Train Loss: 0.24160782997702798 \t|| Test Loss: 0.6121611137578149\n",
      "Epoch: 2023 \t|| Train Loss: 0.24156782997889065 \t|| Test Loss: 0.611983113760052\n",
      "Epoch: 2024 \t|| Train Loss: 0.24152782998075323 \t|| Test Loss: 0.6118051137622891\n",
      "Epoch: 2025 \t|| Train Loss: 0.24148782998261592 \t|| Test Loss: 0.6116271137645259\n",
      "Epoch: 2026 \t|| Train Loss: 0.24144782998447853 \t|| Test Loss: 0.6114491137667633\n",
      "Epoch: 2027 \t|| Train Loss: 0.24140782998634122 \t|| Test Loss: 0.6112711137690001\n",
      "Epoch: 2028 \t|| Train Loss: 0.24136782998820389 \t|| Test Loss: 0.6110931137712373\n",
      "Epoch: 2029 \t|| Train Loss: 0.24132782999006652 \t|| Test Loss: 0.6109151137734743\n",
      "Epoch: 2030 \t|| Train Loss: 0.24128782999192916 \t|| Test Loss: 0.6107371137757114\n",
      "Epoch: 2031 \t|| Train Loss: 0.24124782999379182 \t|| Test Loss: 0.6105591137779485\n",
      "Epoch: 2032 \t|| Train Loss: 0.24120782999565443 \t|| Test Loss: 0.6103811137801854\n",
      "Epoch: 2033 \t|| Train Loss: 0.24116782999751707 \t|| Test Loss: 0.6102031137824225\n",
      "Epoch: 2034 \t|| Train Loss: 0.2411278299993797 \t|| Test Loss: 0.6100251137846595\n",
      "Epoch: 2035 \t|| Train Loss: 0.2410878300012424 \t|| Test Loss: 0.6098471137868966\n",
      "Epoch: 2036 \t|| Train Loss: 0.241047830003105 \t|| Test Loss: 0.6096691137891337\n",
      "Epoch: 2037 \t|| Train Loss: 0.24100783000496767 \t|| Test Loss: 0.6094911137913707\n",
      "Epoch: 2038 \t|| Train Loss: 0.2409678300068303 \t|| Test Loss: 0.6093131137936079\n",
      "Epoch: 2039 \t|| Train Loss: 0.240927830008693 \t|| Test Loss: 0.6091351137958447\n",
      "Epoch: 2040 \t|| Train Loss: 0.24088783001055564 \t|| Test Loss: 0.6089571137980818\n",
      "Epoch: 2041 \t|| Train Loss: 0.2408478300124183 \t|| Test Loss: 0.6087791138003189\n",
      "Epoch: 2042 \t|| Train Loss: 0.24080783001428094 \t|| Test Loss: 0.608601113802556\n",
      "Epoch: 2043 \t|| Train Loss: 0.2407678300161436 \t|| Test Loss: 0.608423113804793\n",
      "Epoch: 2044 \t|| Train Loss: 0.24072783001800618 \t|| Test Loss: 0.6082451138070301\n",
      "Epoch: 2045 \t|| Train Loss: 0.24068783001986888 \t|| Test Loss: 0.6080671138092671\n",
      "Epoch: 2046 \t|| Train Loss: 0.24064783002173148 \t|| Test Loss: 0.6078891138115041\n",
      "Epoch: 2047 \t|| Train Loss: 0.24060783002359423 \t|| Test Loss: 0.6077111138137413\n",
      "Epoch: 2048 \t|| Train Loss: 0.24056783002545679 \t|| Test Loss: 0.6075331138159783\n",
      "Epoch: 2049 \t|| Train Loss: 0.24052783002731942 \t|| Test Loss: 0.6073551138182154\n",
      "Epoch: 2050 \t|| Train Loss: 0.24048783002918211 \t|| Test Loss: 0.6071771138204524\n",
      "Epoch: 2051 \t|| Train Loss: 0.24044783003104478 \t|| Test Loss: 0.6069991138226895\n",
      "Epoch: 2052 \t|| Train Loss: 0.24040783003290742 \t|| Test Loss: 0.6068211138249264\n",
      "Epoch: 2053 \t|| Train Loss: 0.24036783003477008 \t|| Test Loss: 0.6066431138271635\n",
      "Epoch: 2054 \t|| Train Loss: 0.24032783003663272 \t|| Test Loss: 0.6064651138294007\n",
      "Epoch: 2055 \t|| Train Loss: 0.24028783003849535 \t|| Test Loss: 0.6062871138316377\n",
      "Epoch: 2056 \t|| Train Loss: 0.24024783004035802 \t|| Test Loss: 0.6061091138338748\n",
      "Epoch: 2057 \t|| Train Loss: 0.24020783004222065 \t|| Test Loss: 0.6059311138361119\n",
      "Epoch: 2058 \t|| Train Loss: 0.24016783004408335 \t|| Test Loss: 0.6057531138383488\n",
      "Epoch: 2059 \t|| Train Loss: 0.24012783004594596 \t|| Test Loss: 0.6055751138405859\n",
      "Epoch: 2060 \t|| Train Loss: 0.2400878300478086 \t|| Test Loss: 0.6053971138428229\n",
      "Epoch: 2061 \t|| Train Loss: 0.2400478300496712 \t|| Test Loss: 0.60521911384506\n",
      "Epoch: 2062 \t|| Train Loss: 0.2400078300515339 \t|| Test Loss: 0.6050411138472971\n",
      "Epoch: 2063 \t|| Train Loss: 0.23996783005339656 \t|| Test Loss: 0.6048631138495341\n",
      "Epoch: 2064 \t|| Train Loss: 0.2399278300552592 \t|| Test Loss: 0.6046851138517711\n",
      "Epoch: 2065 \t|| Train Loss: 0.23988783005712183 \t|| Test Loss: 0.6045071138540081\n",
      "Epoch: 2066 \t|| Train Loss: 0.2398478300589845 \t|| Test Loss: 0.6043291138562452\n",
      "Epoch: 2067 \t|| Train Loss: 0.23980783006084713 \t|| Test Loss: 0.6041511138584823\n",
      "Epoch: 2068 \t|| Train Loss: 0.2397678300627098 \t|| Test Loss: 0.6039731138607194\n",
      "Epoch: 2069 \t|| Train Loss: 0.2397278300645725 \t|| Test Loss: 0.6037951138629564\n",
      "Epoch: 2070 \t|| Train Loss: 0.23968783006643507 \t|| Test Loss: 0.6036171138651935\n",
      "Epoch: 2071 \t|| Train Loss: 0.23964783006829776 \t|| Test Loss: 0.6034391138674305\n",
      "Epoch: 2072 \t|| Train Loss: 0.23960783007016043 \t|| Test Loss: 0.6032611138696675\n",
      "Epoch: 2073 \t|| Train Loss: 0.23956783007202306 \t|| Test Loss: 0.6030831138719046\n",
      "Epoch: 2074 \t|| Train Loss: 0.23952783007388573 \t|| Test Loss: 0.6029051138741417\n",
      "Epoch: 2075 \t|| Train Loss: 0.23948783007574836 \t|| Test Loss: 0.6027271138763787\n",
      "Epoch: 2076 \t|| Train Loss: 0.23944783007761097 \t|| Test Loss: 0.6025491138786158\n",
      "Epoch: 2077 \t|| Train Loss: 0.23940783007947367 \t|| Test Loss: 0.6023711138808527\n",
      "Epoch: 2078 \t|| Train Loss: 0.23936783008133622 \t|| Test Loss: 0.6021931138830898\n",
      "Epoch: 2079 \t|| Train Loss: 0.2393278300831989 \t|| Test Loss: 0.6020151138853269\n",
      "Epoch: 2080 \t|| Train Loss: 0.2392878300850616 \t|| Test Loss: 0.601837113887564\n",
      "Epoch: 2081 \t|| Train Loss: 0.2392478300869242 \t|| Test Loss: 0.601659113889801\n",
      "Epoch: 2082 \t|| Train Loss: 0.2392078300887869 \t|| Test Loss: 0.6014811138920381\n",
      "Epoch: 2083 \t|| Train Loss: 0.2391678300906495 \t|| Test Loss: 0.601303113894275\n",
      "Epoch: 2084 \t|| Train Loss: 0.23912783009251215 \t|| Test Loss: 0.6011251138965121\n",
      "Epoch: 2085 \t|| Train Loss: 0.23908783009437484 \t|| Test Loss: 0.6009471138987493\n",
      "Epoch: 2086 \t|| Train Loss: 0.23904783009623748 \t|| Test Loss: 0.6007691139009863\n",
      "Epoch: 2087 \t|| Train Loss: 0.23900783009810014 \t|| Test Loss: 0.6005911139032234\n",
      "Epoch: 2088 \t|| Train Loss: 0.23896783009996278 \t|| Test Loss: 0.6004131139054604\n",
      "Epoch: 2089 \t|| Train Loss: 0.23892783010182544 \t|| Test Loss: 0.6002351139076975\n",
      "Epoch: 2090 \t|| Train Loss: 0.23888783010368808 \t|| Test Loss: 0.6000571139099345\n",
      "Epoch: 2091 \t|| Train Loss: 0.2388478301055507 \t|| Test Loss: 0.5998791139121715\n",
      "Epoch: 2092 \t|| Train Loss: 0.23880783010741338 \t|| Test Loss: 0.5997011139144086\n",
      "Epoch: 2093 \t|| Train Loss: 0.238767830109276 \t|| Test Loss: 0.5995231139166457\n",
      "Epoch: 2094 \t|| Train Loss: 0.23872783011113868 \t|| Test Loss: 0.5993451139188826\n",
      "Epoch: 2095 \t|| Train Loss: 0.23868783011300126 \t|| Test Loss: 0.5991671139211199\n",
      "Epoch: 2096 \t|| Train Loss: 0.23864783011486396 \t|| Test Loss: 0.5989891139233567\n",
      "Epoch: 2097 \t|| Train Loss: 0.23860783011672662 \t|| Test Loss: 0.5988111139255939\n",
      "Epoch: 2098 \t|| Train Loss: 0.23856783011858926 \t|| Test Loss: 0.5986331139278309\n",
      "Epoch: 2099 \t|| Train Loss: 0.23852783012045187 \t|| Test Loss: 0.598455113930068\n",
      "Epoch: 2100 \t|| Train Loss: 0.23848783012231456 \t|| Test Loss: 0.598277113932305\n",
      "Epoch: 2101 \t|| Train Loss: 0.2384478301241772 \t|| Test Loss: 0.5980991139345422\n",
      "Epoch: 2102 \t|| Train Loss: 0.23840783012603986 \t|| Test Loss: 0.597921113936779\n",
      "Epoch: 2103 \t|| Train Loss: 0.2383678301279025 \t|| Test Loss: 0.5977431139390161\n",
      "Epoch: 2104 \t|| Train Loss: 0.23832783012976516 \t|| Test Loss: 0.5975651139412532\n",
      "Epoch: 2105 \t|| Train Loss: 0.2382878301316278 \t|| Test Loss: 0.5973871139434903\n",
      "Epoch: 2106 \t|| Train Loss: 0.23824783013349043 \t|| Test Loss: 0.5972091139457273\n",
      "Epoch: 2107 \t|| Train Loss: 0.2382078301353531 \t|| Test Loss: 0.5970311139479644\n",
      "Epoch: 2108 \t|| Train Loss: 0.23816783013721574 \t|| Test Loss: 0.5968531139502014\n",
      "Epoch: 2109 \t|| Train Loss: 0.2381278301390784 \t|| Test Loss: 0.5966751139524386\n",
      "Epoch: 2110 \t|| Train Loss: 0.23808783014094104 \t|| Test Loss: 0.5964971139546755\n",
      "Epoch: 2111 \t|| Train Loss: 0.23804783014280365 \t|| Test Loss: 0.5963191139569126\n",
      "Epoch: 2112 \t|| Train Loss: 0.23800783014466634 \t|| Test Loss: 0.5961411139591496\n",
      "Epoch: 2113 \t|| Train Loss: 0.23796783014652897 \t|| Test Loss: 0.5959631139613867\n",
      "Epoch: 2114 \t|| Train Loss: 0.23792783014839164 \t|| Test Loss: 0.5957851139636239\n",
      "Epoch: 2115 \t|| Train Loss: 0.23788783015025428 \t|| Test Loss: 0.5956071139658607\n",
      "Epoch: 2116 \t|| Train Loss: 0.2378478301521169 \t|| Test Loss: 0.5954291139680977\n",
      "Epoch: 2117 \t|| Train Loss: 0.23780783015397958 \t|| Test Loss: 0.5952511139703349\n",
      "Epoch: 2118 \t|| Train Loss: 0.2377678301558422 \t|| Test Loss: 0.595073113972572\n",
      "Epoch: 2119 \t|| Train Loss: 0.23772783015770488 \t|| Test Loss: 0.5948951139748091\n",
      "Epoch: 2120 \t|| Train Loss: 0.23768783015956751 \t|| Test Loss: 0.594717113977046\n",
      "Epoch: 2121 \t|| Train Loss: 0.2376478301614302 \t|| Test Loss: 0.5945391139792832\n",
      "Epoch: 2122 \t|| Train Loss: 0.23760783016329282 \t|| Test Loss: 0.5943611139815201\n",
      "Epoch: 2123 \t|| Train Loss: 0.23756783016515545 \t|| Test Loss: 0.5941831139837572\n",
      "Epoch: 2124 \t|| Train Loss: 0.23752783016701812 \t|| Test Loss: 0.5940051139859943\n",
      "Epoch: 2125 \t|| Train Loss: 0.2374878301688808 \t|| Test Loss: 0.5938271139882314\n",
      "Epoch: 2126 \t|| Train Loss: 0.2374478301707434 \t|| Test Loss: 0.5936491139904684\n",
      "Epoch: 2127 \t|| Train Loss: 0.23740783017260608 \t|| Test Loss: 0.5934711139927055\n",
      "Epoch: 2128 \t|| Train Loss: 0.2373678301744687 \t|| Test Loss: 0.5932931139949426\n",
      "Epoch: 2129 \t|| Train Loss: 0.23732783017633136 \t|| Test Loss: 0.5931151139971795\n",
      "Epoch: 2130 \t|| Train Loss: 0.237287830178194 \t|| Test Loss: 0.5929371139994165\n",
      "Epoch: 2131 \t|| Train Loss: 0.23724783018005668 \t|| Test Loss: 0.5927591140016537\n",
      "Epoch: 2132 \t|| Train Loss: 0.23720783018191932 \t|| Test Loss: 0.5925811140038907\n",
      "Epoch: 2133 \t|| Train Loss: 0.23716783018378193 \t|| Test Loss: 0.5924031140061278\n",
      "Epoch: 2134 \t|| Train Loss: 0.23712783018564462 \t|| Test Loss: 0.5922251140083648\n",
      "Epoch: 2135 \t|| Train Loss: 0.23708783018750723 \t|| Test Loss: 0.5920471140106018\n",
      "Epoch: 2136 \t|| Train Loss: 0.23704783018936987 \t|| Test Loss: 0.5918691140128389\n",
      "Epoch: 2137 \t|| Train Loss: 0.23700783019123256 \t|| Test Loss: 0.5916911140150759\n",
      "Epoch: 2138 \t|| Train Loss: 0.23696783019309522 \t|| Test Loss: 0.591513114017313\n",
      "Epoch: 2139 \t|| Train Loss: 0.23692783019495786 \t|| Test Loss: 0.59133511401955\n",
      "Epoch: 2140 \t|| Train Loss: 0.23688783019682053 \t|| Test Loss: 0.5911571140217871\n",
      "Epoch: 2141 \t|| Train Loss: 0.23684783019868316 \t|| Test Loss: 0.5909791140240241\n",
      "Epoch: 2142 \t|| Train Loss: 0.23680783020054577 \t|| Test Loss: 0.5908011140262612\n",
      "Epoch: 2143 \t|| Train Loss: 0.23676783020240846 \t|| Test Loss: 0.5906231140284983\n",
      "Epoch: 2144 \t|| Train Loss: 0.2367278302042711 \t|| Test Loss: 0.5904451140307353\n",
      "Epoch: 2145 \t|| Train Loss: 0.23668783020613376 \t|| Test Loss: 0.5902671140329725\n",
      "Epoch: 2146 \t|| Train Loss: 0.23664783020799635 \t|| Test Loss: 0.5900891140352094\n",
      "Epoch: 2147 \t|| Train Loss: 0.23660783020985904 \t|| Test Loss: 0.5899111140374466\n",
      "Epoch: 2148 \t|| Train Loss: 0.23656783021172165 \t|| Test Loss: 0.5897331140396835\n",
      "Epoch: 2149 \t|| Train Loss: 0.23652783021358434 \t|| Test Loss: 0.5895551140419207\n",
      "Epoch: 2150 \t|| Train Loss: 0.236487830215447 \t|| Test Loss: 0.5893771140441576\n",
      "Epoch: 2151 \t|| Train Loss: 0.23644783021730964 \t|| Test Loss: 0.5891991140463947\n",
      "Epoch: 2152 \t|| Train Loss: 0.23640783021917228 \t|| Test Loss: 0.5890211140486319\n",
      "Epoch: 2153 \t|| Train Loss: 0.23636783022103494 \t|| Test Loss: 0.5888431140508688\n",
      "Epoch: 2154 \t|| Train Loss: 0.23632783022289758 \t|| Test Loss: 0.588665114053106\n",
      "Epoch: 2155 \t|| Train Loss: 0.23628783022476024 \t|| Test Loss: 0.5884871140553429\n",
      "Epoch: 2156 \t|| Train Loss: 0.23624783022662282 \t|| Test Loss: 0.58830911405758\n",
      "Epoch: 2157 \t|| Train Loss: 0.23620783022848552 \t|| Test Loss: 0.588131114059817\n",
      "Epoch: 2158 \t|| Train Loss: 0.23616783023034812 \t|| Test Loss: 0.5879531140620541\n",
      "Epoch: 2159 \t|| Train Loss: 0.23612783023221082 \t|| Test Loss: 0.5877751140642911\n",
      "Epoch: 2160 \t|| Train Loss: 0.23608783023407348 \t|| Test Loss: 0.5875971140665281\n",
      "Epoch: 2161 \t|| Train Loss: 0.23604783023593612 \t|| Test Loss: 0.5874191140687652\n",
      "Epoch: 2162 \t|| Train Loss: 0.23600783023779873 \t|| Test Loss: 0.5872411140710023\n",
      "Epoch: 2163 \t|| Train Loss: 0.23596783023966142 \t|| Test Loss: 0.5870631140732393\n",
      "Epoch: 2164 \t|| Train Loss: 0.23592783024152406 \t|| Test Loss: 0.5868851140754764\n",
      "Epoch: 2165 \t|| Train Loss: 0.23588783024338672 \t|| Test Loss: 0.5867071140777134\n",
      "Epoch: 2166 \t|| Train Loss: 0.2358478302452494 \t|| Test Loss: 0.5865291140799506\n",
      "Epoch: 2167 \t|| Train Loss: 0.235807830247112 \t|| Test Loss: 0.5863511140821875\n",
      "Epoch: 2168 \t|| Train Loss: 0.23576783024897466 \t|| Test Loss: 0.5861731140844246\n",
      "Epoch: 2169 \t|| Train Loss: 0.23572783025083735 \t|| Test Loss: 0.5859951140866616\n",
      "Epoch: 2170 \t|| Train Loss: 0.23568783025269996 \t|| Test Loss: 0.5858171140888987\n",
      "Epoch: 2171 \t|| Train Loss: 0.23564783025456265 \t|| Test Loss: 0.5856391140911358\n",
      "Epoch: 2172 \t|| Train Loss: 0.2356078302564253 \t|| Test Loss: 0.5854611140933728\n",
      "Epoch: 2173 \t|| Train Loss: 0.2355678302582879 \t|| Test Loss: 0.5852831140956098\n",
      "Epoch: 2174 \t|| Train Loss: 0.23552783026015053 \t|| Test Loss: 0.5851051140978469\n",
      "Epoch: 2175 \t|| Train Loss: 0.2354878302620132 \t|| Test Loss: 0.5849271141000839\n",
      "Epoch: 2176 \t|| Train Loss: 0.23544783026387583 \t|| Test Loss: 0.584749114102321\n",
      "Epoch: 2177 \t|| Train Loss: 0.23540783026573847 \t|| Test Loss: 0.584571114104558\n",
      "Epoch: 2178 \t|| Train Loss: 0.23536783026760116 \t|| Test Loss: 0.5843931141067951\n",
      "Epoch: 2179 \t|| Train Loss: 0.23532783026946383 \t|| Test Loss: 0.5842151141090322\n",
      "Epoch: 2180 \t|| Train Loss: 0.23528783027132646 \t|| Test Loss: 0.5840371141112692\n",
      "Epoch: 2181 \t|| Train Loss: 0.23524783027318907 \t|| Test Loss: 0.5838591141135062\n",
      "Epoch: 2182 \t|| Train Loss: 0.2352078302750517 \t|| Test Loss: 0.5836811141157433\n",
      "Epoch: 2183 \t|| Train Loss: 0.23516783027691437 \t|| Test Loss: 0.5835031141179803\n",
      "Epoch: 2184 \t|| Train Loss: 0.23512783027877707 \t|| Test Loss: 0.5833251141202174\n",
      "Epoch: 2185 \t|| Train Loss: 0.2350878302806397 \t|| Test Loss: 0.5831471141224546\n",
      "Epoch: 2186 \t|| Train Loss: 0.2350478302825023 \t|| Test Loss: 0.5829691141246915\n",
      "Epoch: 2187 \t|| Train Loss: 0.23500783028436495 \t|| Test Loss: 0.5827911141269286\n",
      "Epoch: 2188 \t|| Train Loss: 0.2349678302862276 \t|| Test Loss: 0.5826131141291657\n",
      "Epoch: 2189 \t|| Train Loss: 0.2349278302880903 \t|| Test Loss: 0.5824351141314028\n",
      "Epoch: 2190 \t|| Train Loss: 0.23488783028995294 \t|| Test Loss: 0.5822571141336398\n",
      "Epoch: 2191 \t|| Train Loss: 0.2348478302918156 \t|| Test Loss: 0.5820791141358768\n",
      "Epoch: 2192 \t|| Train Loss: 0.23480783029367824 \t|| Test Loss: 0.5819011141381139\n",
      "Epoch: 2193 \t|| Train Loss: 0.23476783029554088 \t|| Test Loss: 0.5817231141403509\n",
      "Epoch: 2194 \t|| Train Loss: 0.23472783029740354 \t|| Test Loss: 0.581545114142588\n",
      "Epoch: 2195 \t|| Train Loss: 0.23468783029926624 \t|| Test Loss: 0.5813671141448251\n",
      "Epoch: 2196 \t|| Train Loss: 0.23464783030112885 \t|| Test Loss: 0.5811891141470621\n",
      "Epoch: 2197 \t|| Train Loss: 0.23460783030299143 \t|| Test Loss: 0.5810111141492992\n",
      "Epoch: 2198 \t|| Train Loss: 0.2345678303048541 \t|| Test Loss: 0.5808331141515362\n",
      "Epoch: 2199 \t|| Train Loss: 0.23452783030671678 \t|| Test Loss: 0.5806551141537731\n",
      "Epoch: 2200 \t|| Train Loss: 0.23448783030857942 \t|| Test Loss: 0.5804771141560103\n",
      "Epoch: 2201 \t|| Train Loss: 0.23444783031044208 \t|| Test Loss: 0.5802991141582473\n",
      "Epoch: 2202 \t|| Train Loss: 0.23440783031230472 \t|| Test Loss: 0.5801211141604844\n",
      "Epoch: 2203 \t|| Train Loss: 0.23436783031416736 \t|| Test Loss: 0.5799431141627214\n",
      "Epoch: 2204 \t|| Train Loss: 0.23432783031603002 \t|| Test Loss: 0.5797651141649585\n",
      "Epoch: 2205 \t|| Train Loss: 0.23428783031789266 \t|| Test Loss: 0.5795871141671955\n",
      "Epoch: 2206 \t|| Train Loss: 0.23424783031975532 \t|| Test Loss: 0.5794091141694325\n",
      "Epoch: 2207 \t|| Train Loss: 0.23420783032161796 \t|| Test Loss: 0.5792311141716696\n",
      "Epoch: 2208 \t|| Train Loss: 0.2341678303234806 \t|| Test Loss: 0.5790531141739066\n",
      "Epoch: 2209 \t|| Train Loss: 0.23412783032534326 \t|| Test Loss: 0.5788751141761438\n",
      "Epoch: 2210 \t|| Train Loss: 0.2340878303272059 \t|| Test Loss: 0.5786971141783808\n",
      "Epoch: 2211 \t|| Train Loss: 0.23404783032906856 \t|| Test Loss: 0.5785191141806179\n",
      "Epoch: 2212 \t|| Train Loss: 0.2340078303309312 \t|| Test Loss: 0.5783411141828548\n",
      "Epoch: 2213 \t|| Train Loss: 0.2339678303327939 \t|| Test Loss: 0.5781631141850919\n",
      "Epoch: 2214 \t|| Train Loss: 0.2339278303346565 \t|| Test Loss: 0.5779851141873291\n",
      "Epoch: 2215 \t|| Train Loss: 0.23388783033651914 \t|| Test Loss: 0.577807114189566\n",
      "Epoch: 2216 \t|| Train Loss: 0.2338478303383818 \t|| Test Loss: 0.5776291141918032\n",
      "Epoch: 2217 \t|| Train Loss: 0.23380783034024444 \t|| Test Loss: 0.5774511141940402\n",
      "Epoch: 2218 \t|| Train Loss: 0.23376783034210707 \t|| Test Loss: 0.5772731141962772\n",
      "Epoch: 2219 \t|| Train Loss: 0.23372783034396974 \t|| Test Loss: 0.5770951141985142\n",
      "Epoch: 2220 \t|| Train Loss: 0.23368783034583238 \t|| Test Loss: 0.5769171142007513\n",
      "Epoch: 2221 \t|| Train Loss: 0.23364783034769507 \t|| Test Loss: 0.5767391142029883\n",
      "Epoch: 2222 \t|| Train Loss: 0.23360783034955768 \t|| Test Loss: 0.5765611142052254\n",
      "Epoch: 2223 \t|| Train Loss: 0.23356783035142037 \t|| Test Loss: 0.5763831142074626\n",
      "Epoch: 2224 \t|| Train Loss: 0.23352783035328298 \t|| Test Loss: 0.5762051142096996\n",
      "Epoch: 2225 \t|| Train Loss: 0.23348783035514561 \t|| Test Loss: 0.5760271142119364\n",
      "Epoch: 2226 \t|| Train Loss: 0.23344783035700828 \t|| Test Loss: 0.5758491142141737\n",
      "Epoch: 2227 \t|| Train Loss: 0.23340783035887092 \t|| Test Loss: 0.5756711142164107\n",
      "Epoch: 2228 \t|| Train Loss: 0.23336783036073355 \t|| Test Loss: 0.5754931142186478\n",
      "Epoch: 2229 \t|| Train Loss: 0.23332783036259622 \t|| Test Loss: 0.5753151142208848\n",
      "Epoch: 2230 \t|| Train Loss: 0.23328783036445885 \t|| Test Loss: 0.5751371142231219\n",
      "Epoch: 2231 \t|| Train Loss: 0.23324783036632152 \t|| Test Loss: 0.574959114225359\n",
      "Epoch: 2232 \t|| Train Loss: 0.23320783036818415 \t|| Test Loss: 0.5747811142275959\n",
      "Epoch: 2233 \t|| Train Loss: 0.23316783037004685 \t|| Test Loss: 0.574603114229833\n",
      "Epoch: 2234 \t|| Train Loss: 0.23312783037190946 \t|| Test Loss: 0.5744251142320701\n",
      "Epoch: 2235 \t|| Train Loss: 0.23308783037377215 \t|| Test Loss: 0.5742471142343071\n",
      "Epoch: 2236 \t|| Train Loss: 0.23304783037563478 \t|| Test Loss: 0.5740691142365442\n",
      "Epoch: 2237 \t|| Train Loss: 0.23300783037749745 \t|| Test Loss: 0.5738911142387811\n",
      "Epoch: 2238 \t|| Train Loss: 0.23296783037936009 \t|| Test Loss: 0.5737131142410183\n",
      "Epoch: 2239 \t|| Train Loss: 0.23292783038122272 \t|| Test Loss: 0.5735351142432553\n",
      "Epoch: 2240 \t|| Train Loss: 0.2328878303830854 \t|| Test Loss: 0.5733571142454923\n",
      "Epoch: 2241 \t|| Train Loss: 0.23284783038494802 \t|| Test Loss: 0.5731791142477294\n",
      "Epoch: 2242 \t|| Train Loss: 0.23280783038681063 \t|| Test Loss: 0.5730011142499665\n",
      "Epoch: 2243 \t|| Train Loss: 0.23276783038867332 \t|| Test Loss: 0.5728231142522036\n",
      "Epoch: 2244 \t|| Train Loss: 0.23272783039053593 \t|| Test Loss: 0.5726451142544406\n",
      "Epoch: 2245 \t|| Train Loss: 0.23268783039239863 \t|| Test Loss: 0.5724671142566776\n",
      "Epoch: 2246 \t|| Train Loss: 0.23264783039426123 \t|| Test Loss: 0.5722891142589146\n",
      "Epoch: 2247 \t|| Train Loss: 0.23260783039612393 \t|| Test Loss: 0.5721111142611518\n",
      "Epoch: 2248 \t|| Train Loss: 0.2325678303979865 \t|| Test Loss: 0.5719331142633889\n",
      "Epoch: 2249 \t|| Train Loss: 0.2325278303998492 \t|| Test Loss: 0.5717551142656259\n",
      "Epoch: 2250 \t|| Train Loss: 0.23248783040171186 \t|| Test Loss: 0.5715771142678628\n",
      "Epoch: 2251 \t|| Train Loss: 0.2324478304035745 \t|| Test Loss: 0.5713991142700999\n",
      "Epoch: 2252 \t|| Train Loss: 0.2324078304054372 \t|| Test Loss: 0.5712211142723371\n",
      "Epoch: 2253 \t|| Train Loss: 0.2323678304072998 \t|| Test Loss: 0.571043114274574\n",
      "Epoch: 2254 \t|| Train Loss: 0.23232783040916244 \t|| Test Loss: 0.5708651142768112\n",
      "Epoch: 2255 \t|| Train Loss: 0.2322878304110251 \t|| Test Loss: 0.5706871142790482\n",
      "Epoch: 2256 \t|| Train Loss: 0.23224783041288774 \t|| Test Loss: 0.5705091142812853\n",
      "Epoch: 2257 \t|| Train Loss: 0.2322078304147504 \t|| Test Loss: 0.5703311142835223\n",
      "Epoch: 2258 \t|| Train Loss: 0.23216783041661304 \t|| Test Loss: 0.5701531142857593\n",
      "Epoch: 2259 \t|| Train Loss: 0.23212783041847568 \t|| Test Loss: 0.5699751142879963\n",
      "Epoch: 2260 \t|| Train Loss: 0.23208783042033834 \t|| Test Loss: 0.5697971142902334\n",
      "Epoch: 2261 \t|| Train Loss: 0.23204783042220098 \t|| Test Loss: 0.5696191142924705\n",
      "Epoch: 2262 \t|| Train Loss: 0.23200783042406364 \t|| Test Loss: 0.5694411142947076\n",
      "Epoch: 2263 \t|| Train Loss: 0.23196783042592628 \t|| Test Loss: 0.5692631142969445\n",
      "Epoch: 2264 \t|| Train Loss: 0.23192783042778897 \t|| Test Loss: 0.5690851142991817\n",
      "Epoch: 2265 \t|| Train Loss: 0.23188783042965158 \t|| Test Loss: 0.5689071143014186\n",
      "Epoch: 2266 \t|| Train Loss: 0.23184783043151427 \t|| Test Loss: 0.5687291143036558\n",
      "Epoch: 2267 \t|| Train Loss: 0.23180783043337683 \t|| Test Loss: 0.5685511143058928\n",
      "Epoch: 2268 \t|| Train Loss: 0.23176783043523952 \t|| Test Loss: 0.5683731143081298\n",
      "Epoch: 2269 \t|| Train Loss: 0.23172783043710216 \t|| Test Loss: 0.568195114310367\n",
      "Epoch: 2270 \t|| Train Loss: 0.23168783043896482 \t|| Test Loss: 0.568017114312604\n",
      "Epoch: 2271 \t|| Train Loss: 0.23164783044082746 \t|| Test Loss: 0.567839114314841\n",
      "Epoch: 2272 \t|| Train Loss: 0.23160783044269012 \t|| Test Loss: 0.5676611143170781\n",
      "Epoch: 2273 \t|| Train Loss: 0.23156783044455276 \t|| Test Loss: 0.567483114319315\n",
      "Epoch: 2274 \t|| Train Loss: 0.2315278304464154 \t|| Test Loss: 0.5673051143215522\n",
      "Epoch: 2275 \t|| Train Loss: 0.23148783044827806 \t|| Test Loss: 0.5671271143237891\n",
      "Epoch: 2276 \t|| Train Loss: 0.2314478304501407 \t|| Test Loss: 0.5669491143260263\n",
      "Epoch: 2277 \t|| Train Loss: 0.2314078326656556 \t|| Test Loss: 0.5668389143287194\n",
      "Epoch: 2278 \t|| Train Loss: 0.23137293266743925 \t|| Test Loss: 0.5667287143314123\n",
      "Epoch: 2279 \t|| Train Loss: 0.23133803266922293 \t|| Test Loss: 0.5666185143341054\n",
      "Epoch: 2280 \t|| Train Loss: 0.23130313267100658 \t|| Test Loss: 0.5665083143367984\n",
      "Epoch: 2281 \t|| Train Loss: 0.23126823267279026 \t|| Test Loss: 0.5663981143394914\n",
      "Epoch: 2282 \t|| Train Loss: 0.23123333267457397 \t|| Test Loss: 0.5662879143421843\n",
      "Epoch: 2283 \t|| Train Loss: 0.2311984326763576 \t|| Test Loss: 0.5661777143448774\n",
      "Epoch: 2284 \t|| Train Loss: 0.2311635326781413 \t|| Test Loss: 0.5660675143475704\n",
      "Epoch: 2285 \t|| Train Loss: 0.23112863267992495 \t|| Test Loss: 0.5659573143502634\n",
      "Epoch: 2286 \t|| Train Loss: 0.23109373268170863 \t|| Test Loss: 0.5658471143529563\n",
      "Epoch: 2287 \t|| Train Loss: 0.2310588326834923 \t|| Test Loss: 0.5657369143556493\n",
      "Epoch: 2288 \t|| Train Loss: 0.23102393268527593 \t|| Test Loss: 0.5656267143583424\n",
      "Epoch: 2289 \t|| Train Loss: 0.23098903268705967 \t|| Test Loss: 0.5655165143610353\n",
      "Epoch: 2290 \t|| Train Loss: 0.2309541326888433 \t|| Test Loss: 0.5654063143637283\n",
      "Epoch: 2291 \t|| Train Loss: 0.23091923269062695 \t|| Test Loss: 0.5652961143664214\n",
      "Epoch: 2292 \t|| Train Loss: 0.23088433269241065 \t|| Test Loss: 0.5651859143691145\n",
      "Epoch: 2293 \t|| Train Loss: 0.23084943269419428 \t|| Test Loss: 0.5650757143718074\n",
      "Epoch: 2294 \t|| Train Loss: 0.230814532695978 \t|| Test Loss: 0.5649655143745003\n",
      "Epoch: 2295 \t|| Train Loss: 0.23077963269776164 \t|| Test Loss: 0.5648553143771934\n",
      "Epoch: 2296 \t|| Train Loss: 0.23074473269954526 \t|| Test Loss: 0.5647451143798865\n",
      "Epoch: 2297 \t|| Train Loss: 0.230709832701329 \t|| Test Loss: 0.5646349143825793\n",
      "Epoch: 2298 \t|| Train Loss: 0.23067493270311265 \t|| Test Loss: 0.5645247143852723\n",
      "Epoch: 2299 \t|| Train Loss: 0.2306400327048963 \t|| Test Loss: 0.5644145143879654\n",
      "Epoch: 2300 \t|| Train Loss: 0.23060513270667996 \t|| Test Loss: 0.5643043143906585\n",
      "Epoch: 2301 \t|| Train Loss: 0.23057023270846363 \t|| Test Loss: 0.5641941143933514\n",
      "Epoch: 2302 \t|| Train Loss: 0.23053533271024734 \t|| Test Loss: 0.5640839143960443\n",
      "Epoch: 2303 \t|| Train Loss: 0.23050043271203097 \t|| Test Loss: 0.5639737143987374\n",
      "Epoch: 2304 \t|| Train Loss: 0.23046553271381462 \t|| Test Loss: 0.5638635144014303\n",
      "Epoch: 2305 \t|| Train Loss: 0.23043063271559833 \t|| Test Loss: 0.5637533144041235\n",
      "Epoch: 2306 \t|| Train Loss: 0.23039573271738195 \t|| Test Loss: 0.5636431144068164\n",
      "Epoch: 2307 \t|| Train Loss: 0.23036083271916566 \t|| Test Loss: 0.5635329144095095\n",
      "Epoch: 2308 \t|| Train Loss: 0.2303259327209493 \t|| Test Loss: 0.5634227144122024\n",
      "Epoch: 2309 \t|| Train Loss: 0.230291032722733 \t|| Test Loss: 0.5633125144148955\n",
      "Epoch: 2310 \t|| Train Loss: 0.23025613272451667 \t|| Test Loss: 0.5632023144175884\n",
      "Epoch: 2311 \t|| Train Loss: 0.23022123272630032 \t|| Test Loss: 0.5630921144202815\n",
      "Epoch: 2312 \t|| Train Loss: 0.23018633272808403 \t|| Test Loss: 0.5629819144229745\n",
      "Epoch: 2313 \t|| Train Loss: 0.23015143272986766 \t|| Test Loss: 0.5628717144256675\n",
      "Epoch: 2314 \t|| Train Loss: 0.2301165327316513 \t|| Test Loss: 0.5627615144283604\n",
      "Epoch: 2315 \t|| Train Loss: 0.23008163273343502 \t|| Test Loss: 0.5626513144310534\n",
      "Epoch: 2316 \t|| Train Loss: 0.23004673273521864 \t|| Test Loss: 0.5625411144337464\n",
      "Epoch: 2317 \t|| Train Loss: 0.23001183273700235 \t|| Test Loss: 0.5624309144364394\n",
      "Epoch: 2318 \t|| Train Loss: 0.22997693273878603 \t|| Test Loss: 0.5623207144391325\n",
      "Epoch: 2319 \t|| Train Loss: 0.22994203274056968 \t|| Test Loss: 0.5622105144418255\n",
      "Epoch: 2320 \t|| Train Loss: 0.22990713274235336 \t|| Test Loss: 0.5621003144445186\n",
      "Epoch: 2321 \t|| Train Loss: 0.229872232744137 \t|| Test Loss: 0.5619901144472115\n",
      "Epoch: 2322 \t|| Train Loss: 0.22983733274592072 \t|| Test Loss: 0.5618799144499045\n",
      "Epoch: 2323 \t|| Train Loss: 0.22980243274770434 \t|| Test Loss: 0.5617697144525975\n",
      "Epoch: 2324 \t|| Train Loss: 0.22976753274948805 \t|| Test Loss: 0.5616595144552905\n",
      "Epoch: 2325 \t|| Train Loss: 0.2297326327512717 \t|| Test Loss: 0.5615493144579835\n",
      "Epoch: 2326 \t|| Train Loss: 0.22969773275305533 \t|| Test Loss: 0.5614391144606765\n",
      "Epoch: 2327 \t|| Train Loss: 0.22966283275483904 \t|| Test Loss: 0.5613289144633695\n",
      "Epoch: 2328 \t|| Train Loss: 0.2296279327566227 \t|| Test Loss: 0.5612187144660625\n",
      "Epoch: 2329 \t|| Train Loss: 0.22959303275840637 \t|| Test Loss: 0.5611085144687555\n",
      "Epoch: 2330 \t|| Train Loss: 0.22955813276019005 \t|| Test Loss: 0.5609983144714485\n",
      "Epoch: 2331 \t|| Train Loss: 0.2295232327619737 \t|| Test Loss: 0.5608881144741416\n",
      "Epoch: 2332 \t|| Train Loss: 0.2294883327637574 \t|| Test Loss: 0.5607779144768346\n",
      "Epoch: 2333 \t|| Train Loss: 0.22945343276554103 \t|| Test Loss: 0.5606677144795276\n",
      "Epoch: 2334 \t|| Train Loss: 0.22941853276732468 \t|| Test Loss: 0.5605575144822206\n",
      "Epoch: 2335 \t|| Train Loss: 0.2293836327691084 \t|| Test Loss: 0.5604473144849136\n",
      "Epoch: 2336 \t|| Train Loss: 0.22934873277089202 \t|| Test Loss: 0.5603371144876067\n",
      "Epoch: 2337 \t|| Train Loss: 0.22931383277267572 \t|| Test Loss: 0.5602269144902995\n",
      "Epoch: 2338 \t|| Train Loss: 0.2292789327744594 \t|| Test Loss: 0.5601167144929926\n",
      "Epoch: 2339 \t|| Train Loss: 0.22924403277624306 \t|| Test Loss: 0.5600065144956856\n",
      "Epoch: 2340 \t|| Train Loss: 0.22920913277802674 \t|| Test Loss: 0.5598963144983786\n",
      "Epoch: 2341 \t|| Train Loss: 0.2291742327798104 \t|| Test Loss: 0.5597861145010716\n",
      "Epoch: 2342 \t|| Train Loss: 0.22913933278159404 \t|| Test Loss: 0.5596759145037646\n",
      "Epoch: 2343 \t|| Train Loss: 0.2291044327833777 \t|| Test Loss: 0.5595657145064575\n",
      "Epoch: 2344 \t|| Train Loss: 0.22906953278516137 \t|| Test Loss: 0.5594555145091508\n",
      "Epoch: 2345 \t|| Train Loss: 0.22903463278694508 \t|| Test Loss: 0.5593453145118437\n",
      "Epoch: 2346 \t|| Train Loss: 0.2289997327887287 \t|| Test Loss: 0.5592351145145367\n",
      "Epoch: 2347 \t|| Train Loss: 0.2289648327905124 \t|| Test Loss: 0.5591249145172297\n",
      "Epoch: 2348 \t|| Train Loss: 0.22892993279229606 \t|| Test Loss: 0.5590147145199227\n",
      "Epoch: 2349 \t|| Train Loss: 0.2288950327940798 \t|| Test Loss: 0.5589045145226156\n",
      "Epoch: 2350 \t|| Train Loss: 0.2288601327958634 \t|| Test Loss: 0.5587943145253087\n",
      "Epoch: 2351 \t|| Train Loss: 0.22882523279764708 \t|| Test Loss: 0.5586841145280017\n",
      "Epoch: 2352 \t|| Train Loss: 0.22879033279943073 \t|| Test Loss: 0.5585739145306947\n",
      "Epoch: 2353 \t|| Train Loss: 0.22875543280121438 \t|| Test Loss: 0.5584637145333876\n",
      "Epoch: 2354 \t|| Train Loss: 0.22872053280299812 \t|| Test Loss: 0.5583535145360807\n",
      "Epoch: 2355 \t|| Train Loss: 0.22868563280478177 \t|| Test Loss: 0.5582433145387737\n",
      "Epoch: 2356 \t|| Train Loss: 0.2286507328065654 \t|| Test Loss: 0.5581331145414667\n",
      "Epoch: 2357 \t|| Train Loss: 0.2286158328083491 \t|| Test Loss: 0.5580229145441596\n",
      "Epoch: 2358 \t|| Train Loss: 0.22858093281013278 \t|| Test Loss: 0.5579127145468528\n",
      "Epoch: 2359 \t|| Train Loss: 0.22854603281191643 \t|| Test Loss: 0.5578025145495457\n",
      "Epoch: 2360 \t|| Train Loss: 0.22851113281370009 \t|| Test Loss: 0.5576923145522386\n",
      "Epoch: 2361 \t|| Train Loss: 0.22847623281548382 \t|| Test Loss: 0.5575821145549317\n",
      "Epoch: 2362 \t|| Train Loss: 0.22844133281726747 \t|| Test Loss: 0.5574719145576247\n",
      "Epoch: 2363 \t|| Train Loss: 0.2284064328190511 \t|| Test Loss: 0.5573617145603178\n",
      "Epoch: 2364 \t|| Train Loss: 0.22837153282083475 \t|| Test Loss: 0.5572515145630107\n",
      "Epoch: 2365 \t|| Train Loss: 0.22833663282261848 \t|| Test Loss: 0.5571413145657037\n",
      "Epoch: 2366 \t|| Train Loss: 0.22830173282440214 \t|| Test Loss: 0.5570311145683967\n",
      "Epoch: 2367 \t|| Train Loss: 0.2282668328261858 \t|| Test Loss: 0.5569209145710897\n",
      "Epoch: 2368 \t|| Train Loss: 0.22823193282796947 \t|| Test Loss: 0.5568107145737827\n",
      "Epoch: 2369 \t|| Train Loss: 0.22819703282975318 \t|| Test Loss: 0.5567005145764758\n",
      "Epoch: 2370 \t|| Train Loss: 0.2281621328315368 \t|| Test Loss: 0.5565903145791687\n",
      "Epoch: 2371 \t|| Train Loss: 0.22812723283332045 \t|| Test Loss: 0.5564801145818616\n",
      "Epoch: 2372 \t|| Train Loss: 0.2280923328351041 \t|| Test Loss: 0.5563699145845548\n",
      "Epoch: 2373 \t|| Train Loss: 0.22805743283688784 \t|| Test Loss: 0.5562597145872477\n",
      "Epoch: 2374 \t|| Train Loss: 0.2280225328386715 \t|| Test Loss: 0.5561495145899408\n",
      "Epoch: 2375 \t|| Train Loss: 0.22798763284045515 \t|| Test Loss: 0.5560393145926337\n",
      "Epoch: 2376 \t|| Train Loss: 0.22795273284223883 \t|| Test Loss: 0.5559291145953267\n",
      "Epoch: 2377 \t|| Train Loss: 0.2279178328440225 \t|| Test Loss: 0.5558189145980197\n",
      "Epoch: 2378 \t|| Train Loss: 0.22788293284580616 \t|| Test Loss: 0.5557087146007128\n",
      "Epoch: 2379 \t|| Train Loss: 0.2278480328475898 \t|| Test Loss: 0.5555985146034059\n",
      "Epoch: 2380 \t|| Train Loss: 0.22781313284937346 \t|| Test Loss: 0.5554883146060988\n",
      "Epoch: 2381 \t|| Train Loss: 0.22777823285115714 \t|| Test Loss: 0.5553781146087917\n",
      "Epoch: 2382 \t|| Train Loss: 0.22774333285294085 \t|| Test Loss: 0.5552679146114848\n",
      "Epoch: 2383 \t|| Train Loss: 0.22770843285472447 \t|| Test Loss: 0.5551577146141777\n",
      "Epoch: 2384 \t|| Train Loss: 0.22767353285650818 \t|| Test Loss: 0.5550475146168709\n",
      "Epoch: 2385 \t|| Train Loss: 0.22763863285829183 \t|| Test Loss: 0.5549373146195637\n",
      "Epoch: 2386 \t|| Train Loss: 0.2276037328600755 \t|| Test Loss: 0.5548271146222569\n",
      "Epoch: 2387 \t|| Train Loss: 0.22756883286185917 \t|| Test Loss: 0.5547169146249498\n",
      "Epoch: 2388 \t|| Train Loss: 0.22753393286364285 \t|| Test Loss: 0.5546067146276429\n",
      "Epoch: 2389 \t|| Train Loss: 0.2274990328654265 \t|| Test Loss: 0.5544965146303358\n",
      "Epoch: 2390 \t|| Train Loss: 0.22746413286721018 \t|| Test Loss: 0.5543863146330289\n",
      "Epoch: 2391 \t|| Train Loss: 0.22742923286899383 \t|| Test Loss: 0.5542761146357219\n",
      "Epoch: 2392 \t|| Train Loss: 0.22739433287077748 \t|| Test Loss: 0.5541659146384148\n",
      "Epoch: 2393 \t|| Train Loss: 0.22735943287256116 \t|| Test Loss: 0.5540557146411078\n",
      "Epoch: 2394 \t|| Train Loss: 0.22732453287434487 \t|| Test Loss: 0.5539455146438009\n",
      "Epoch: 2395 \t|| Train Loss: 0.22728963287612852 \t|| Test Loss: 0.5538353146464938\n",
      "Epoch: 2396 \t|| Train Loss: 0.2272547328779122 \t|| Test Loss: 0.5537251146491868\n",
      "Epoch: 2397 \t|| Train Loss: 0.22721983287969585 \t|| Test Loss: 0.5536149146518798\n",
      "Epoch: 2398 \t|| Train Loss: 0.22718493288147953 \t|| Test Loss: 0.5535047146545728\n",
      "Epoch: 2399 \t|| Train Loss: 0.2271500328832632 \t|| Test Loss: 0.553394514657266\n",
      "Epoch: 2400 \t|| Train Loss: 0.22711513288504684 \t|| Test Loss: 0.5532843146599589\n",
      "Epoch: 2401 \t|| Train Loss: 0.22708023288683057 \t|| Test Loss: 0.5531741146626519\n",
      "Epoch: 2402 \t|| Train Loss: 0.22704533288861423 \t|| Test Loss: 0.5530639146653449\n",
      "Epoch: 2403 \t|| Train Loss: 0.22701043289039785 \t|| Test Loss: 0.5529537146680379\n",
      "Epoch: 2404 \t|| Train Loss: 0.22697553289218156 \t|| Test Loss: 0.5528435146707309\n",
      "Epoch: 2405 \t|| Train Loss: 0.22694063289396524 \t|| Test Loss: 0.5527333146734239\n",
      "Epoch: 2406 \t|| Train Loss: 0.2269057328957489 \t|| Test Loss: 0.5526231146761169\n",
      "Epoch: 2407 \t|| Train Loss: 0.22687083289753254 \t|| Test Loss: 0.5525129146788099\n",
      "Epoch: 2408 \t|| Train Loss: 0.2268359328993162 \t|| Test Loss: 0.5524027146815029\n",
      "Epoch: 2409 \t|| Train Loss: 0.22680103290109987 \t|| Test Loss: 0.5522925146841959\n",
      "Epoch: 2410 \t|| Train Loss: 0.22676613290288355 \t|| Test Loss: 0.5521823146868889\n",
      "Epoch: 2411 \t|| Train Loss: 0.2267312329046672 \t|| Test Loss: 0.5520721146895821\n",
      "Epoch: 2412 \t|| Train Loss: 0.22669633290645091 \t|| Test Loss: 0.551961914692275\n",
      "Epoch: 2413 \t|| Train Loss: 0.22666143290823454 \t|| Test Loss: 0.551851714694968\n",
      "Epoch: 2414 \t|| Train Loss: 0.22662653291001825 \t|| Test Loss: 0.551741514697661\n",
      "Epoch: 2415 \t|| Train Loss: 0.2265916329118019 \t|| Test Loss: 0.5516313147003541\n",
      "Epoch: 2416 \t|| Train Loss: 0.22655673291358552 \t|| Test Loss: 0.551521114703047\n",
      "Epoch: 2417 \t|| Train Loss: 0.22652183291536926 \t|| Test Loss: 0.55141091470574\n",
      "Epoch: 2418 \t|| Train Loss: 0.22648693291715288 \t|| Test Loss: 0.551300714708433\n",
      "Epoch: 2419 \t|| Train Loss: 0.22645203291893656 \t|| Test Loss: 0.551190514711126\n",
      "Epoch: 2420 \t|| Train Loss: 0.22641713292072024 \t|| Test Loss: 0.551080314713819\n",
      "Epoch: 2421 \t|| Train Loss: 0.2263822329225039 \t|| Test Loss: 0.550970114716512\n",
      "Epoch: 2422 \t|| Train Loss: 0.2263473329242876 \t|| Test Loss: 0.550859914719205\n",
      "Epoch: 2423 \t|| Train Loss: 0.22631243292607123 \t|| Test Loss: 0.550749714721898\n",
      "Epoch: 2424 \t|| Train Loss: 0.22627753292785494 \t|| Test Loss: 0.550639514724591\n",
      "Epoch: 2425 \t|| Train Loss: 0.22624263292963862 \t|| Test Loss: 0.550529314727284\n",
      "Epoch: 2426 \t|| Train Loss: 0.22620773293142227 \t|| Test Loss: 0.5504191147299771\n",
      "Epoch: 2427 \t|| Train Loss: 0.22617283293320592 \t|| Test Loss: 0.55030891473267\n",
      "Epoch: 2428 \t|| Train Loss: 0.22613793293498957 \t|| Test Loss: 0.5501987147353631\n",
      "Epoch: 2429 \t|| Train Loss: 0.2261030329367733 \t|| Test Loss: 0.5500885147380561\n",
      "Epoch: 2430 \t|| Train Loss: 0.2260681329385569 \t|| Test Loss: 0.5499783147407491\n",
      "Epoch: 2431 \t|| Train Loss: 0.22603323294034058 \t|| Test Loss: 0.5498681147434421\n",
      "Epoch: 2432 \t|| Train Loss: 0.2259983329421243 \t|| Test Loss: 0.5497579147461351\n",
      "Epoch: 2433 \t|| Train Loss: 0.22596343294390792 \t|| Test Loss: 0.5496477147488281\n",
      "Epoch: 2434 \t|| Train Loss: 0.22592853294569162 \t|| Test Loss: 0.5495375147515211\n",
      "Epoch: 2435 \t|| Train Loss: 0.22589363294747528 \t|| Test Loss: 0.5494273147542141\n",
      "Epoch: 2436 \t|| Train Loss: 0.2258587329492589 \t|| Test Loss: 0.5493171147569071\n",
      "Epoch: 2437 \t|| Train Loss: 0.2258238329510426 \t|| Test Loss: 0.5492069147596\n",
      "Epoch: 2438 \t|| Train Loss: 0.22578893295282626 \t|| Test Loss: 0.5490967147622932\n",
      "Epoch: 2439 \t|| Train Loss: 0.22575403295460994 \t|| Test Loss: 0.5489865147649862\n",
      "Epoch: 2440 \t|| Train Loss: 0.22571913295639362 \t|| Test Loss: 0.5488763147676792\n",
      "Epoch: 2441 \t|| Train Loss: 0.22568423295817727 \t|| Test Loss: 0.5487661147703722\n",
      "Epoch: 2442 \t|| Train Loss: 0.22564933295996098 \t|| Test Loss: 0.5486559147730652\n",
      "Epoch: 2443 \t|| Train Loss: 0.2256144329617446 \t|| Test Loss: 0.5485457147757582\n",
      "Epoch: 2444 \t|| Train Loss: 0.2255795329635283 \t|| Test Loss: 0.5484355147784512\n",
      "Epoch: 2445 \t|| Train Loss: 0.22554463296531196 \t|| Test Loss: 0.5483253147811442\n",
      "Epoch: 2446 \t|| Train Loss: 0.22550973296709564 \t|| Test Loss: 0.5482151147838372\n",
      "Epoch: 2447 \t|| Train Loss: 0.2254748329688793 \t|| Test Loss: 0.5481049147865302\n",
      "Epoch: 2448 \t|| Train Loss: 0.22543993297066295 \t|| Test Loss: 0.5479947147892231\n",
      "Epoch: 2449 \t|| Train Loss: 0.22540503297244663 \t|| Test Loss: 0.5478845147919161\n",
      "Epoch: 2450 \t|| Train Loss: 0.2253701329742303 \t|| Test Loss: 0.5477743147946093\n",
      "Epoch: 2451 \t|| Train Loss: 0.22533523297601396 \t|| Test Loss: 0.5476641147973023\n",
      "Epoch: 2452 \t|| Train Loss: 0.22530033297779767 \t|| Test Loss: 0.5475539147999952\n",
      "Epoch: 2453 \t|| Train Loss: 0.2252654329795813 \t|| Test Loss: 0.5474437148026883\n",
      "Epoch: 2454 \t|| Train Loss: 0.225230532981365 \t|| Test Loss: 0.5473335148053812\n",
      "Epoch: 2455 \t|| Train Loss: 0.22519563298314865 \t|| Test Loss: 0.5472233148080743\n",
      "Epoch: 2456 \t|| Train Loss: 0.22516073298493233 \t|| Test Loss: 0.5471131148107672\n",
      "Epoch: 2457 \t|| Train Loss: 0.225125832986716 \t|| Test Loss: 0.5470029148134603\n",
      "Epoch: 2458 \t|| Train Loss: 0.22509093298849964 \t|| Test Loss: 0.5468927148161532\n",
      "Epoch: 2459 \t|| Train Loss: 0.22505603299028332 \t|| Test Loss: 0.5467825148188463\n",
      "Epoch: 2460 \t|| Train Loss: 0.22502113299206697 \t|| Test Loss: 0.5466723148215392\n",
      "Epoch: 2461 \t|| Train Loss: 0.2249862329938507 \t|| Test Loss: 0.5465621148242322\n",
      "Epoch: 2462 \t|| Train Loss: 0.22495133299563436 \t|| Test Loss: 0.5464519148269252\n",
      "Epoch: 2463 \t|| Train Loss: 0.22491643299741798 \t|| Test Loss: 0.5463417148296184\n",
      "Epoch: 2464 \t|| Train Loss: 0.22488153299920172 \t|| Test Loss: 0.5462315148323112\n",
      "Epoch: 2465 \t|| Train Loss: 0.22484663300098537 \t|| Test Loss: 0.5461213148350044\n",
      "Epoch: 2466 \t|| Train Loss: 0.22481173300276902 \t|| Test Loss: 0.5460111148376973\n",
      "Epoch: 2467 \t|| Train Loss: 0.22477683300455267 \t|| Test Loss: 0.5459009148403904\n",
      "Epoch: 2468 \t|| Train Loss: 0.22474193300633633 \t|| Test Loss: 0.5457907148430833\n",
      "Epoch: 2469 \t|| Train Loss: 0.22470703300812006 \t|| Test Loss: 0.5456805148457764\n",
      "Epoch: 2470 \t|| Train Loss: 0.22467213300990369 \t|| Test Loss: 0.5455703148484694\n",
      "Epoch: 2471 \t|| Train Loss: 0.22463723301168734 \t|| Test Loss: 0.5454601148511623\n",
      "Epoch: 2472 \t|| Train Loss: 0.22460233301347107 \t|| Test Loss: 0.5453499148538553\n",
      "Epoch: 2473 \t|| Train Loss: 0.22456743301525464 \t|| Test Loss: 0.5452397148565483\n",
      "Epoch: 2474 \t|| Train Loss: 0.22453253301703838 \t|| Test Loss: 0.5451295148592413\n",
      "Epoch: 2475 \t|| Train Loss: 0.22449763301882203 \t|| Test Loss: 0.5450193148619344\n",
      "Epoch: 2476 \t|| Train Loss: 0.22446273302060565 \t|| Test Loss: 0.5449091148646273\n",
      "Epoch: 2477 \t|| Train Loss: 0.22442783302238936 \t|| Test Loss: 0.5447989148673205\n",
      "Epoch: 2478 \t|| Train Loss: 0.22439293302417304 \t|| Test Loss: 0.5446887148700134\n",
      "Epoch: 2479 \t|| Train Loss: 0.2243580330259567 \t|| Test Loss: 0.5445785148727064\n",
      "Epoch: 2480 \t|| Train Loss: 0.22432313302774035 \t|| Test Loss: 0.5444683148753994\n",
      "Epoch: 2481 \t|| Train Loss: 0.22428823302952403 \t|| Test Loss: 0.5443581148780924\n",
      "Epoch: 2482 \t|| Train Loss: 0.22425333303130773 \t|| Test Loss: 0.5442479148807855\n",
      "Epoch: 2483 \t|| Train Loss: 0.22421843303309136 \t|| Test Loss: 0.5441377148834784\n",
      "Epoch: 2484 \t|| Train Loss: 0.224183533034875 \t|| Test Loss: 0.5440275148861714\n",
      "Epoch: 2485 \t|| Train Loss: 0.22414863303665872 \t|| Test Loss: 0.5439173148888644\n",
      "Epoch: 2486 \t|| Train Loss: 0.2241137330384424 \t|| Test Loss: 0.5438071148915574\n",
      "Epoch: 2487 \t|| Train Loss: 0.22407883304022605 \t|| Test Loss: 0.5436969148942504\n",
      "Epoch: 2488 \t|| Train Loss: 0.22404393304200973 \t|| Test Loss: 0.5435867148969434\n",
      "Epoch: 2489 \t|| Train Loss: 0.22400903304379338 \t|| Test Loss: 0.5434765148996364\n",
      "Epoch: 2490 \t|| Train Loss: 0.22397413304557706 \t|| Test Loss: 0.5433663149023296\n",
      "Epoch: 2491 \t|| Train Loss: 0.22393923304736077 \t|| Test Loss: 0.5432561149050225\n",
      "Epoch: 2492 \t|| Train Loss: 0.22390433304914442 \t|| Test Loss: 0.5431459149077155\n",
      "Epoch: 2493 \t|| Train Loss: 0.22386943305092805 \t|| Test Loss: 0.5430357149104085\n",
      "Epoch: 2494 \t|| Train Loss: 0.22383453305271175 \t|| Test Loss: 0.5429255149131016\n",
      "Epoch: 2495 \t|| Train Loss: 0.22379963305449543 \t|| Test Loss: 0.5428153149157944\n",
      "Epoch: 2496 \t|| Train Loss: 0.22376473305627909 \t|| Test Loss: 0.5427051149184875\n",
      "Epoch: 2497 \t|| Train Loss: 0.22372983305806274 \t|| Test Loss: 0.5425949149211805\n",
      "Epoch: 2498 \t|| Train Loss: 0.22369493305984642 \t|| Test Loss: 0.5424847149238735\n",
      "Epoch: 2499 \t|| Train Loss: 0.22366003306163013 \t|| Test Loss: 0.5423745149265665\n",
      "Epoch: 2500 \t|| Train Loss: 0.22362513306341375 \t|| Test Loss: 0.5422643149292595\n",
      "Epoch: 2501 \t|| Train Loss: 0.22359023306519746 \t|| Test Loss: 0.5421541149319524\n",
      "Epoch: 2502 \t|| Train Loss: 0.2235553330669811 \t|| Test Loss: 0.5420439149346455\n",
      "Epoch: 2503 \t|| Train Loss: 0.2235204330687648 \t|| Test Loss: 0.5419337149373384\n",
      "Epoch: 2504 \t|| Train Loss: 0.22348553307054844 \t|| Test Loss: 0.5418235149400316\n",
      "Epoch: 2505 \t|| Train Loss: 0.22345063307233212 \t|| Test Loss: 0.5417133149427246\n",
      "Epoch: 2506 \t|| Train Loss: 0.22341573307411577 \t|| Test Loss: 0.5416031149454174\n",
      "Epoch: 2507 \t|| Train Loss: 0.22338083307589943 \t|| Test Loss: 0.5414929149481106\n",
      "Epoch: 2508 \t|| Train Loss: 0.2233459330776831 \t|| Test Loss: 0.5413827149508036\n",
      "Epoch: 2509 \t|| Train Loss: 0.22331103307946681 \t|| Test Loss: 0.5412725149534966\n",
      "Epoch: 2510 \t|| Train Loss: 0.22327613308125044 \t|| Test Loss: 0.5411623149561896\n",
      "Epoch: 2511 \t|| Train Loss: 0.22324123308303415 \t|| Test Loss: 0.5410521149588825\n",
      "Epoch: 2512 \t|| Train Loss: 0.2232063330848178 \t|| Test Loss: 0.5409419149615756\n",
      "Epoch: 2513 \t|| Train Loss: 0.22317143308660142 \t|| Test Loss: 0.5408317149642685\n",
      "Epoch: 2514 \t|| Train Loss: 0.22313653308838513 \t|| Test Loss: 0.5407215149669616\n",
      "Epoch: 2515 \t|| Train Loss: 0.22310163309016878 \t|| Test Loss: 0.5406113149696545\n",
      "Epoch: 2516 \t|| Train Loss: 0.22306673309195246 \t|| Test Loss: 0.5405011149723475\n",
      "Epoch: 2517 \t|| Train Loss: 0.22303183309373614 \t|| Test Loss: 0.5403909149750405\n",
      "Epoch: 2518 \t|| Train Loss: 0.2229969330955198 \t|| Test Loss: 0.5402807149777334\n",
      "Epoch: 2519 \t|| Train Loss: 0.22296203309730345 \t|| Test Loss: 0.5401705149804266\n",
      "Epoch: 2520 \t|| Train Loss: 0.22292713309908713 \t|| Test Loss: 0.5400603149831196\n",
      "Epoch: 2521 \t|| Train Loss: 0.22289223310087078 \t|| Test Loss: 0.5399501149858126\n",
      "Epoch: 2522 \t|| Train Loss: 0.2228573331026545 \t|| Test Loss: 0.5398399149885055\n",
      "Epoch: 2523 \t|| Train Loss: 0.2228224331044381 \t|| Test Loss: 0.5397297149911985\n",
      "Epoch: 2524 \t|| Train Loss: 0.22278753310622182 \t|| Test Loss: 0.5396195149938916\n",
      "Epoch: 2525 \t|| Train Loss: 0.22275263310800547 \t|| Test Loss: 0.5395093149965846\n",
      "Epoch: 2526 \t|| Train Loss: 0.22271773310978915 \t|| Test Loss: 0.5393991149992776\n",
      "Epoch: 2527 \t|| Train Loss: 0.22268283311157283 \t|| Test Loss: 0.5392889150019705\n",
      "Epoch: 2528 \t|| Train Loss: 0.22264793311335648 \t|| Test Loss: 0.5391787150046636\n",
      "Epoch: 2529 \t|| Train Loss: 0.2226130331151402 \t|| Test Loss: 0.5390685150073565\n",
      "Epoch: 2530 \t|| Train Loss: 0.22257813311692382 \t|| Test Loss: 0.5389583150100495\n",
      "Epoch: 2531 \t|| Train Loss: 0.22254323311870747 \t|| Test Loss: 0.5388481150127425\n",
      "Epoch: 2532 \t|| Train Loss: 0.22250833312049118 \t|| Test Loss: 0.5387379150154357\n",
      "Epoch: 2533 \t|| Train Loss: 0.2224734331222748 \t|| Test Loss: 0.5386277150181286\n",
      "Epoch: 2534 \t|| Train Loss: 0.2224385331240585 \t|| Test Loss: 0.5385175150208216\n",
      "Epoch: 2535 \t|| Train Loss: 0.22240363312584216 \t|| Test Loss: 0.5384073150235146\n",
      "Epoch: 2536 \t|| Train Loss: 0.22236873312762584 \t|| Test Loss: 0.5382971150262077\n",
      "Epoch: 2537 \t|| Train Loss: 0.22233383312940952 \t|| Test Loss: 0.5381869150289006\n",
      "Epoch: 2538 \t|| Train Loss: 0.22229893313119317 \t|| Test Loss: 0.5380767150315937\n",
      "Epoch: 2539 \t|| Train Loss: 0.22226403313297682 \t|| Test Loss: 0.5379665150342866\n",
      "Epoch: 2540 \t|| Train Loss: 0.2222291331347605 \t|| Test Loss: 0.5378563150369797\n",
      "Epoch: 2541 \t|| Train Loss: 0.22219423313654416 \t|| Test Loss: 0.5377461150396726\n",
      "Epoch: 2542 \t|| Train Loss: 0.22215933313832786 \t|| Test Loss: 0.5376359150423656\n",
      "Epoch: 2543 \t|| Train Loss: 0.2221244331401115 \t|| Test Loss: 0.5375257150450586\n",
      "Epoch: 2544 \t|| Train Loss: 0.2220895331418952 \t|| Test Loss: 0.5374155150477516\n",
      "Epoch: 2545 \t|| Train Loss: 0.22205463314367888 \t|| Test Loss: 0.5373053150504447\n",
      "Epoch: 2546 \t|| Train Loss: 0.22201973314546253 \t|| Test Loss: 0.5371951150531377\n",
      "Epoch: 2547 \t|| Train Loss: 0.22198483314724618 \t|| Test Loss: 0.5370849150558307\n",
      "Epoch: 2548 \t|| Train Loss: 0.22194993314902986 \t|| Test Loss: 0.5369747150585238\n",
      "Epoch: 2549 \t|| Train Loss: 0.22191503315081357 \t|| Test Loss: 0.5368645150612167\n",
      "Epoch: 2550 \t|| Train Loss: 0.2218801331525972 \t|| Test Loss: 0.5367543150639097\n",
      "Epoch: 2551 \t|| Train Loss: 0.22184523315438084 \t|| Test Loss: 0.5366441150666027\n",
      "Epoch: 2552 \t|| Train Loss: 0.22181033315616455 \t|| Test Loss: 0.5365339150692957\n",
      "Epoch: 2553 \t|| Train Loss: 0.22177543315794818 \t|| Test Loss: 0.5364237150719886\n",
      "Epoch: 2554 \t|| Train Loss: 0.22174053315973188 \t|| Test Loss: 0.5363135150746818\n",
      "Epoch: 2555 \t|| Train Loss: 0.22170563316151554 \t|| Test Loss: 0.5362033150773747\n",
      "Epoch: 2556 \t|| Train Loss: 0.22167073316329916 \t|| Test Loss: 0.5360931150800677\n",
      "Epoch: 2557 \t|| Train Loss: 0.2216358331650829 \t|| Test Loss: 0.5359829150827607\n",
      "Epoch: 2558 \t|| Train Loss: 0.22160093316686655 \t|| Test Loss: 0.5358727150854538\n",
      "Epoch: 2559 \t|| Train Loss: 0.2215660331686502 \t|| Test Loss: 0.5357625150881468\n",
      "Epoch: 2560 \t|| Train Loss: 0.22153113317043394 \t|| Test Loss: 0.5356523150908398\n",
      "Epoch: 2561 \t|| Train Loss: 0.22149623317221753 \t|| Test Loss: 0.5355421150935327\n",
      "Epoch: 2562 \t|| Train Loss: 0.22146133317400124 \t|| Test Loss: 0.5354319150962258\n",
      "Epoch: 2563 \t|| Train Loss: 0.22142643317578486 \t|| Test Loss: 0.5353217150989188\n",
      "Epoch: 2564 \t|| Train Loss: 0.22139153317756857 \t|| Test Loss: 0.5352115151016118\n",
      "Epoch: 2565 \t|| Train Loss: 0.22135663317935222 \t|| Test Loss: 0.5351013151043048\n",
      "Epoch: 2566 \t|| Train Loss: 0.2213217331811359 \t|| Test Loss: 0.5349911151069978\n",
      "Epoch: 2567 \t|| Train Loss: 0.22128683318291956 \t|| Test Loss: 0.5348809151096908\n",
      "Epoch: 2568 \t|| Train Loss: 0.2212519331847032 \t|| Test Loss: 0.5347707151123838\n",
      "Epoch: 2569 \t|| Train Loss: 0.22121703318648694 \t|| Test Loss: 0.5346605151150768\n",
      "Epoch: 2570 \t|| Train Loss: 0.22118213318827054 \t|| Test Loss: 0.5345503151177697\n",
      "Epoch: 2571 \t|| Train Loss: 0.22114723319005422 \t|| Test Loss: 0.5344401151204629\n",
      "Epoch: 2572 \t|| Train Loss: 0.22111233319183793 \t|| Test Loss: 0.5343299151231558\n",
      "Epoch: 2573 \t|| Train Loss: 0.2210774331936216 \t|| Test Loss: 0.5342197151258489\n",
      "Epoch: 2574 \t|| Train Loss: 0.22104253319540526 \t|| Test Loss: 0.5341095151285418\n",
      "Epoch: 2575 \t|| Train Loss: 0.2210076331971889 \t|| Test Loss: 0.5339993151312349\n",
      "Epoch: 2576 \t|| Train Loss: 0.2209727331989726 \t|| Test Loss: 0.5338891151339279\n",
      "Epoch: 2577 \t|| Train Loss: 0.22093783320075627 \t|| Test Loss: 0.5337789151366208\n",
      "Epoch: 2578 \t|| Train Loss: 0.22090293320253993 \t|| Test Loss: 0.5336687151393138\n",
      "Epoch: 2579 \t|| Train Loss: 0.22086803320432363 \t|| Test Loss: 0.5335585151420069\n",
      "Epoch: 2580 \t|| Train Loss: 0.22083313320610723 \t|| Test Loss: 0.5334483151446998\n",
      "Epoch: 2581 \t|| Train Loss: 0.22079823320789096 \t|| Test Loss: 0.5333381151473929\n",
      "Epoch: 2582 \t|| Train Loss: 0.22076333320967462 \t|| Test Loss: 0.5332279151500858\n",
      "Epoch: 2583 \t|| Train Loss: 0.2207284332114583 \t|| Test Loss: 0.533117715152779\n",
      "Epoch: 2584 \t|| Train Loss: 0.22069353321324195 \t|| Test Loss: 0.533007515155472\n",
      "Epoch: 2585 \t|| Train Loss: 0.2206586332150256 \t|| Test Loss: 0.5328973151581649\n",
      "Epoch: 2586 \t|| Train Loss: 0.22062373321680928 \t|| Test Loss: 0.5327871151608579\n",
      "Epoch: 2587 \t|| Train Loss: 0.22058883321859293 \t|| Test Loss: 0.532676915163551\n",
      "Epoch: 2588 \t|| Train Loss: 0.22055393322037667 \t|| Test Loss: 0.5325667151662439\n",
      "Epoch: 2589 \t|| Train Loss: 0.22051903322216027 \t|| Test Loss: 0.5324565151689369\n",
      "Epoch: 2590 \t|| Train Loss: 0.22048413322394395 \t|| Test Loss: 0.5323463151716299\n",
      "Epoch: 2591 \t|| Train Loss: 0.22044923322572765 \t|| Test Loss: 0.532236115174323\n",
      "Epoch: 2592 \t|| Train Loss: 0.22041433322751125 \t|| Test Loss: 0.5321259151770159\n",
      "Epoch: 2593 \t|| Train Loss: 0.22037943322929499 \t|| Test Loss: 0.5320157151797089\n",
      "Epoch: 2594 \t|| Train Loss: 0.22034453323107864 \t|| Test Loss: 0.5319055151824019\n",
      "Epoch: 2595 \t|| Train Loss: 0.22030963323286232 \t|| Test Loss: 0.531795315185095\n",
      "Epoch: 2596 \t|| Train Loss: 0.22027473323464603 \t|| Test Loss: 0.5316851151877879\n",
      "Epoch: 2597 \t|| Train Loss: 0.22023983323642965 \t|| Test Loss: 0.531574915190481\n",
      "Epoch: 2598 \t|| Train Loss: 0.2202049332382133 \t|| Test Loss: 0.531464715193174\n",
      "Epoch: 2599 \t|| Train Loss: 0.22017003323999695 \t|| Test Loss: 0.531354515195867\n",
      "Epoch: 2600 \t|| Train Loss: 0.22013513324178063 \t|| Test Loss: 0.53124431519856\n",
      "Epoch: 2601 \t|| Train Loss: 0.22010023324356429 \t|| Test Loss: 0.531134115201253\n",
      "Epoch: 2602 \t|| Train Loss: 0.220065333245348 \t|| Test Loss: 0.531023915203946\n",
      "Epoch: 2603 \t|| Train Loss: 0.22003043324713167 \t|| Test Loss: 0.530913715206639\n",
      "Epoch: 2604 \t|| Train Loss: 0.21999553324891533 \t|| Test Loss: 0.530803515209332\n",
      "Epoch: 2605 \t|| Train Loss: 0.21996063325069898 \t|| Test Loss: 0.530693315212025\n",
      "Epoch: 2606 \t|| Train Loss: 0.21992573325248266 \t|| Test Loss: 0.530583115214718\n",
      "Epoch: 2607 \t|| Train Loss: 0.2198908332542663 \t|| Test Loss: 0.530472915217411\n",
      "Epoch: 2608 \t|| Train Loss: 0.21985593325605005 \t|| Test Loss: 0.530362715220104\n",
      "Epoch: 2609 \t|| Train Loss: 0.21982103325783364 \t|| Test Loss: 0.530252515222797\n",
      "Epoch: 2610 \t|| Train Loss: 0.21978613325961732 \t|| Test Loss: 0.53014231522549\n",
      "Epoch: 2611 \t|| Train Loss: 0.21975123326140097 \t|| Test Loss: 0.530032115228183\n",
      "Epoch: 2612 \t|| Train Loss: 0.21971633326318468 \t|| Test Loss: 0.5299219152308761\n",
      "Epoch: 2613 \t|| Train Loss: 0.21968143326496836 \t|| Test Loss: 0.5298117152335691\n",
      "Epoch: 2614 \t|| Train Loss: 0.21964653326675201 \t|| Test Loss: 0.5297015152362621\n",
      "Epoch: 2615 \t|| Train Loss: 0.21961163326853567 \t|| Test Loss: 0.5295913152389551\n",
      "Epoch: 2616 \t|| Train Loss: 0.21957673327031935 \t|| Test Loss: 0.5294811152416481\n",
      "Epoch: 2617 \t|| Train Loss: 0.219541833272103 \t|| Test Loss: 0.5293709152443411\n",
      "Epoch: 2618 \t|| Train Loss: 0.21950693327388668 \t|| Test Loss: 0.5292607152470341\n",
      "Epoch: 2619 \t|| Train Loss: 0.21947203327567033 \t|| Test Loss: 0.5291505152497271\n",
      "Epoch: 2620 \t|| Train Loss: 0.219437133277454 \t|| Test Loss: 0.5290403152524201\n",
      "Epoch: 2621 \t|| Train Loss: 0.21940223327923772 \t|| Test Loss: 0.5289301152551131\n",
      "Epoch: 2622 \t|| Train Loss: 0.21936733328102137 \t|| Test Loss: 0.5288199152578061\n",
      "Epoch: 2623 \t|| Train Loss: 0.21933243328280505 \t|| Test Loss: 0.5287097152604991\n",
      "Epoch: 2624 \t|| Train Loss: 0.2192975332845887 \t|| Test Loss: 0.528599515263192\n",
      "Epoch: 2625 \t|| Train Loss: 0.21926263328637238 \t|| Test Loss: 0.5284893152658852\n",
      "Epoch: 2626 \t|| Train Loss: 0.21922773328815603 \t|| Test Loss: 0.5283791152685782\n",
      "Epoch: 2627 \t|| Train Loss: 0.2191928332899397 \t|| Test Loss: 0.5282689152712712\n",
      "Epoch: 2628 \t|| Train Loss: 0.21915793329172337 \t|| Test Loss: 0.5281587152739642\n",
      "Epoch: 2629 \t|| Train Loss: 0.21912303329350707 \t|| Test Loss: 0.5280485152766572\n",
      "Epoch: 2630 \t|| Train Loss: 0.2190881332952907 \t|| Test Loss: 0.5279383152793502\n",
      "Epoch: 2631 \t|| Train Loss: 0.2190532332970744 \t|| Test Loss: 0.5278281152820432\n",
      "Epoch: 2632 \t|| Train Loss: 0.21901833329885806 \t|| Test Loss: 0.5277179152847362\n",
      "Epoch: 2633 \t|| Train Loss: 0.21898343330064174 \t|| Test Loss: 0.5276077152874292\n",
      "Epoch: 2634 \t|| Train Loss: 0.2189485333024254 \t|| Test Loss: 0.5274975152901221\n",
      "Epoch: 2635 \t|| Train Loss: 0.21891363330420907 \t|| Test Loss: 0.5273873152928152\n",
      "Epoch: 2636 \t|| Train Loss: 0.21887873330599272 \t|| Test Loss: 0.5272771152955081\n",
      "Epoch: 2637 \t|| Train Loss: 0.21884383330777638 \t|| Test Loss: 0.5271669152982013\n",
      "Epoch: 2638 \t|| Train Loss: 0.21880893330956006 \t|| Test Loss: 0.5270567153008943\n",
      "Epoch: 2639 \t|| Train Loss: 0.21877403331134376 \t|| Test Loss: 0.5269465153035873\n",
      "Epoch: 2640 \t|| Train Loss: 0.2187391333131274 \t|| Test Loss: 0.5268363153062802\n",
      "Epoch: 2641 \t|| Train Loss: 0.2187042333149111 \t|| Test Loss: 0.5267261153089733\n",
      "Epoch: 2642 \t|| Train Loss: 0.21866933331669475 \t|| Test Loss: 0.5266159153116663\n",
      "Epoch: 2643 \t|| Train Loss: 0.21863443331847837 \t|| Test Loss: 0.5265057153143593\n",
      "Epoch: 2644 \t|| Train Loss: 0.2185995333202621 \t|| Test Loss: 0.5263955153170523\n",
      "Epoch: 2645 \t|| Train Loss: 0.21856463332204573 \t|| Test Loss: 0.5262853153197453\n",
      "Epoch: 2646 \t|| Train Loss: 0.2185297333238294 \t|| Test Loss: 0.5261751153224382\n",
      "Epoch: 2647 \t|| Train Loss: 0.21849483332561306 \t|| Test Loss: 0.5260649153251312\n",
      "Epoch: 2648 \t|| Train Loss: 0.21845993332739672 \t|| Test Loss: 0.5259547153278243\n",
      "Epoch: 2649 \t|| Train Loss: 0.21842503332918045 \t|| Test Loss: 0.5258445153305172\n",
      "Epoch: 2650 \t|| Train Loss: 0.21839013333096408 \t|| Test Loss: 0.5257343153332104\n",
      "Epoch: 2651 \t|| Train Loss: 0.21835523333274778 \t|| Test Loss: 0.5256241153359033\n",
      "Epoch: 2652 \t|| Train Loss: 0.21832033333453144 \t|| Test Loss: 0.5255139153385964\n",
      "Epoch: 2653 \t|| Train Loss: 0.21828543333631506 \t|| Test Loss: 0.5254037153412893\n",
      "Epoch: 2654 \t|| Train Loss: 0.21825053333809877 \t|| Test Loss: 0.5252935153439824\n",
      "Epoch: 2655 \t|| Train Loss: 0.21821563333988242 \t|| Test Loss: 0.5251833153466754\n",
      "Epoch: 2656 \t|| Train Loss: 0.2181807333416661 \t|| Test Loss: 0.5250731153493684\n",
      "Epoch: 2657 \t|| Train Loss: 0.21814583334344978 \t|| Test Loss: 0.5249629153520613\n",
      "Epoch: 2658 \t|| Train Loss: 0.21811093334523343 \t|| Test Loss: 0.5248527153547544\n",
      "Epoch: 2659 \t|| Train Loss: 0.21807603334701708 \t|| Test Loss: 0.5247425153574472\n",
      "Epoch: 2660 \t|| Train Loss: 0.21804113334880082 \t|| Test Loss: 0.5246323153601402\n",
      "Epoch: 2661 \t|| Train Loss: 0.21800623335058447 \t|| Test Loss: 0.5245221153628333\n",
      "Epoch: 2662 \t|| Train Loss: 0.21797133335236812 \t|| Test Loss: 0.5244119153655262\n",
      "Epoch: 2663 \t|| Train Loss: 0.2179364333541518 \t|| Test Loss: 0.5243017153682193\n",
      "Epoch: 2664 \t|| Train Loss: 0.21790153335593548 \t|| Test Loss: 0.5241915153709122\n",
      "Epoch: 2665 \t|| Train Loss: 0.21786663335771914 \t|| Test Loss: 0.5240813153736054\n",
      "Epoch: 2666 \t|| Train Loss: 0.2178317333595028 \t|| Test Loss: 0.5239711153762985\n",
      "Epoch: 2667 \t|| Train Loss: 0.21779683336128644 \t|| Test Loss: 0.5238609153789913\n",
      "Epoch: 2668 \t|| Train Loss: 0.21776193336307018 \t|| Test Loss: 0.5237507153816843\n",
      "Epoch: 2669 \t|| Train Loss: 0.21772703336485383 \t|| Test Loss: 0.5236405153843774\n",
      "Epoch: 2670 \t|| Train Loss: 0.21769213336663745 \t|| Test Loss: 0.5235303153870705\n",
      "Epoch: 2671 \t|| Train Loss: 0.2176572333684211 \t|| Test Loss: 0.5234201153897634\n",
      "Epoch: 2672 \t|| Train Loss: 0.21762233337020476 \t|| Test Loss: 0.5233099153924563\n",
      "Epoch: 2673 \t|| Train Loss: 0.2175874333719885 \t|| Test Loss: 0.5231997153951494\n",
      "Epoch: 2674 \t|| Train Loss: 0.21755253337377214 \t|| Test Loss: 0.5230895153978423\n",
      "Epoch: 2675 \t|| Train Loss: 0.2175176333755558 \t|| Test Loss: 0.5229793154005354\n",
      "Epoch: 2676 \t|| Train Loss: 0.21748273337733953 \t|| Test Loss: 0.5228691154032283\n",
      "Epoch: 2677 \t|| Train Loss: 0.21744783337912316 \t|| Test Loss: 0.5227589154059215\n",
      "Epoch: 2678 \t|| Train Loss: 0.2174129333809068 \t|| Test Loss: 0.5226487154086146\n",
      "Epoch: 2679 \t|| Train Loss: 0.21737803338269052 \t|| Test Loss: 0.5225385154113074\n",
      "Epoch: 2680 \t|| Train Loss: 0.2173431333844742 \t|| Test Loss: 0.5224283154140004\n",
      "Epoch: 2681 \t|| Train Loss: 0.21730823338625785 \t|| Test Loss: 0.5223181154166935\n",
      "Epoch: 2682 \t|| Train Loss: 0.2172733333880415 \t|| Test Loss: 0.5222079154193864\n",
      "Epoch: 2683 \t|| Train Loss: 0.21723843338982524 \t|| Test Loss: 0.5220977154220795\n",
      "Epoch: 2684 \t|| Train Loss: 0.21720353339160886 \t|| Test Loss: 0.5219875154247724\n",
      "Epoch: 2685 \t|| Train Loss: 0.2171686333933925 \t|| Test Loss: 0.5218773154274655\n",
      "Epoch: 2686 \t|| Train Loss: 0.21713373339517617 \t|| Test Loss: 0.5217671154301585\n",
      "Epoch: 2687 \t|| Train Loss: 0.21709883339695982 \t|| Test Loss: 0.5216569154328514\n",
      "Epoch: 2688 \t|| Train Loss: 0.21706393339874355 \t|| Test Loss: 0.5215467154355444\n",
      "Epoch: 2689 \t|| Train Loss: 0.2170290334005272 \t|| Test Loss: 0.5214365154382374\n",
      "Epoch: 2690 \t|| Train Loss: 0.21699413340231083 \t|| Test Loss: 0.5213263154409306\n",
      "Epoch: 2691 \t|| Train Loss: 0.21695923340409456 \t|| Test Loss: 0.5212161154436235\n",
      "Epoch: 2692 \t|| Train Loss: 0.2169243334058782 \t|| Test Loss: 0.5211059154463165\n",
      "Epoch: 2693 \t|| Train Loss: 0.2168894334076618 \t|| Test Loss: 0.5209957154490095\n",
      "Epoch: 2694 \t|| Train Loss: 0.21685453340944552 \t|| Test Loss: 0.5208855154517025\n",
      "Epoch: 2695 \t|| Train Loss: 0.2168196334112292 \t|| Test Loss: 0.5207753154543955\n",
      "Epoch: 2696 \t|| Train Loss: 0.21678473341301285 \t|| Test Loss: 0.5206651154570885\n",
      "Epoch: 2697 \t|| Train Loss: 0.21674983341479653 \t|| Test Loss: 0.5205549154597815\n",
      "Epoch: 2698 \t|| Train Loss: 0.21671493341658019 \t|| Test Loss: 0.5204447154624745\n",
      "Epoch: 2699 \t|| Train Loss: 0.21668003341836384 \t|| Test Loss: 0.5203345154651675\n",
      "Epoch: 2700 \t|| Train Loss: 0.21664513342014752 \t|| Test Loss: 0.5202243154678605\n",
      "Epoch: 2701 \t|| Train Loss: 0.21661023342193123 \t|| Test Loss: 0.5201141154705535\n",
      "Epoch: 2702 \t|| Train Loss: 0.21657533342371488 \t|| Test Loss: 0.5200039154732465\n",
      "Epoch: 2703 \t|| Train Loss: 0.2165404334254985 \t|| Test Loss: 0.5198937154759395\n",
      "Epoch: 2704 \t|| Train Loss: 0.2165055334272822 \t|| Test Loss: 0.5197835154786326\n",
      "Epoch: 2705 \t|| Train Loss: 0.2164706334290659 \t|| Test Loss: 0.5196733154813256\n",
      "Epoch: 2706 \t|| Train Loss: 0.21643573343084954 \t|| Test Loss: 0.5195631154840186\n",
      "Epoch: 2707 \t|| Train Loss: 0.21640083343263322 \t|| Test Loss: 0.5194529154867116\n",
      "Epoch: 2708 \t|| Train Loss: 0.21636593343441687 \t|| Test Loss: 0.5193427154894046\n",
      "Epoch: 2709 \t|| Train Loss: 0.21633103343620058 \t|| Test Loss: 0.5192325154920976\n",
      "Epoch: 2710 \t|| Train Loss: 0.2162961334379842 \t|| Test Loss: 0.5191223154947906\n",
      "Epoch: 2711 \t|| Train Loss: 0.2162612334397679 \t|| Test Loss: 0.5190121154974836\n",
      "Epoch: 2712 \t|| Train Loss: 0.21622633344155157 \t|| Test Loss: 0.5189019155001766\n",
      "Epoch: 2713 \t|| Train Loss: 0.21619143344333525 \t|| Test Loss: 0.5187917155028695\n",
      "Epoch: 2714 \t|| Train Loss: 0.2161565334451189 \t|| Test Loss: 0.5186815155055626\n",
      "Epoch: 2715 \t|| Train Loss: 0.21612163344690258 \t|| Test Loss: 0.5185713155082555\n",
      "Epoch: 2716 \t|| Train Loss: 0.21608673344868629 \t|| Test Loss: 0.5184611155109486\n",
      "Epoch: 2717 \t|| Train Loss: 0.2160518334504699 \t|| Test Loss: 0.5183509155136417\n",
      "Epoch: 2718 \t|| Train Loss: 0.21601693345225362 \t|| Test Loss: 0.5182407155163347\n",
      "Epoch: 2719 \t|| Train Loss: 0.21598203345403727 \t|| Test Loss: 0.5181305155190276\n",
      "Epoch: 2720 \t|| Train Loss: 0.2159471334558209 \t|| Test Loss: 0.5180203155217207\n",
      "Epoch: 2721 \t|| Train Loss: 0.21591223345760455 \t|| Test Loss: 0.5179101155244137\n",
      "Epoch: 2722 \t|| Train Loss: 0.21587733345938825 \t|| Test Loss: 0.5177999155271067\n",
      "Epoch: 2723 \t|| Train Loss: 0.21584243346117188 \t|| Test Loss: 0.5176897155297997\n",
      "Epoch: 2724 \t|| Train Loss: 0.21580753346295561 \t|| Test Loss: 0.5175795155324927\n",
      "Epoch: 2725 \t|| Train Loss: 0.21577263346473927 \t|| Test Loss: 0.5174693155351856\n",
      "Epoch: 2726 \t|| Train Loss: 0.21573773346652292 \t|| Test Loss: 0.5173591155378787\n",
      "Epoch: 2727 \t|| Train Loss: 0.2157028334683066 \t|| Test Loss: 0.5172489155405716\n",
      "Epoch: 2728 \t|| Train Loss: 0.2156679334700903 \t|| Test Loss: 0.5171387155432646\n",
      "Epoch: 2729 \t|| Train Loss: 0.21563303347187396 \t|| Test Loss: 0.5170285155459577\n",
      "Epoch: 2730 \t|| Train Loss: 0.21559813347365758 \t|| Test Loss: 0.5169183155486506\n",
      "Epoch: 2731 \t|| Train Loss: 0.21556323347544123 \t|| Test Loss: 0.5168081155513436\n",
      "Epoch: 2732 \t|| Train Loss: 0.21552833347722494 \t|| Test Loss: 0.5166979155540367\n",
      "Epoch: 2733 \t|| Train Loss: 0.21549343347900862 \t|| Test Loss: 0.5165877155567298\n",
      "Epoch: 2734 \t|| Train Loss: 0.21545853348079227 \t|| Test Loss: 0.5164775155594228\n",
      "Epoch: 2735 \t|| Train Loss: 0.21542363348257595 \t|| Test Loss: 0.5163673155621157\n",
      "Epoch: 2736 \t|| Train Loss: 0.21538873348435966 \t|| Test Loss: 0.5162571155648087\n",
      "Epoch: 2737 \t|| Train Loss: 0.2153538334861433 \t|| Test Loss: 0.5161469155675016\n",
      "Epoch: 2738 \t|| Train Loss: 0.21531893348792694 \t|| Test Loss: 0.5160367155701947\n",
      "Epoch: 2739 \t|| Train Loss: 0.21528403348971065 \t|| Test Loss: 0.5159265155728878\n",
      "Epoch: 2740 \t|| Train Loss: 0.21524913349149427 \t|| Test Loss: 0.5158163155755807\n",
      "Epoch: 2741 \t|| Train Loss: 0.21521423349327798 \t|| Test Loss: 0.5157061155782737\n",
      "Epoch: 2742 \t|| Train Loss: 0.21517933349506163 \t|| Test Loss: 0.5155959155809667\n",
      "Epoch: 2743 \t|| Train Loss: 0.21514443349684526 \t|| Test Loss: 0.5154857155836596\n",
      "Epoch: 2744 \t|| Train Loss: 0.21510953349862896 \t|| Test Loss: 0.5153755155863528\n",
      "Epoch: 2745 \t|| Train Loss: 0.21507463350041264 \t|| Test Loss: 0.5152653155890459\n",
      "Epoch: 2746 \t|| Train Loss: 0.2150397335021963 \t|| Test Loss: 0.5151551155917388\n",
      "Epoch: 2747 \t|| Train Loss: 0.21500483350397998 \t|| Test Loss: 0.5150449155944317\n",
      "Epoch: 2748 \t|| Train Loss: 0.21496993350576363 \t|| Test Loss: 0.5149347155971248\n",
      "Epoch: 2749 \t|| Train Loss: 0.21493503350754728 \t|| Test Loss: 0.5148245155998177\n",
      "Epoch: 2750 \t|| Train Loss: 0.21490013350933096 \t|| Test Loss: 0.5147143156025108\n",
      "Epoch: 2751 \t|| Train Loss: 0.2148652335111146 \t|| Test Loss: 0.5146041156052037\n",
      "Epoch: 2752 \t|| Train Loss: 0.21483033351289832 \t|| Test Loss: 0.5144939156078968\n",
      "Epoch: 2753 \t|| Train Loss: 0.214795433514682 \t|| Test Loss: 0.5143837156105897\n",
      "Epoch: 2754 \t|| Train Loss: 0.21476053351646565 \t|| Test Loss: 0.5142735156132828\n",
      "Epoch: 2755 \t|| Train Loss: 0.21472563351824933 \t|| Test Loss: 0.5141633156159757\n",
      "Epoch: 2756 \t|| Train Loss: 0.21469073352003304 \t|| Test Loss: 0.5140531156186688\n",
      "Epoch: 2757 \t|| Train Loss: 0.21465583352181666 \t|| Test Loss: 0.5139429156213617\n",
      "Epoch: 2758 \t|| Train Loss: 0.21462093352360037 \t|| Test Loss: 0.5138327156240549\n",
      "Epoch: 2759 \t|| Train Loss: 0.21458603352538402 \t|| Test Loss: 0.5137225156267478\n",
      "Epoch: 2760 \t|| Train Loss: 0.2145511335271677 \t|| Test Loss: 0.5136123156294409\n",
      "Epoch: 2761 \t|| Train Loss: 0.21451623352895136 \t|| Test Loss: 0.5135021156321338\n",
      "Epoch: 2762 \t|| Train Loss: 0.214481333530735 \t|| Test Loss: 0.5133919156348268\n",
      "Epoch: 2763 \t|| Train Loss: 0.2144464335325187 \t|| Test Loss: 0.5132817156375199\n",
      "Epoch: 2764 \t|| Train Loss: 0.21441153353430234 \t|| Test Loss: 0.5131715156402128\n",
      "Epoch: 2765 \t|| Train Loss: 0.21437663353608602 \t|| Test Loss: 0.5130613156429058\n",
      "Epoch: 2766 \t|| Train Loss: 0.21434173353786973 \t|| Test Loss: 0.5129511156455988\n",
      "Epoch: 2767 \t|| Train Loss: 0.21430683353965335 \t|| Test Loss: 0.5128409156482918\n",
      "Epoch: 2768 \t|| Train Loss: 0.21427193354143706 \t|| Test Loss: 0.5127307156509848\n",
      "Epoch: 2769 \t|| Train Loss: 0.2142370335432207 \t|| Test Loss: 0.5126205156536778\n",
      "Epoch: 2770 \t|| Train Loss: 0.21420213354500434 \t|| Test Loss: 0.5125103156563708\n",
      "Epoch: 2771 \t|| Train Loss: 0.21416723354678804 \t|| Test Loss: 0.5124001156590638\n",
      "Epoch: 2772 \t|| Train Loss: 0.21413233354857172 \t|| Test Loss: 0.5122899156617569\n",
      "Epoch: 2773 \t|| Train Loss: 0.21409743355035538 \t|| Test Loss: 0.51217971566445\n",
      "Epoch: 2774 \t|| Train Loss: 0.21406253355213903 \t|| Test Loss: 0.5120695156671429\n",
      "Epoch: 2775 \t|| Train Loss: 0.2140276335539227 \t|| Test Loss: 0.5119593156698359\n",
      "Epoch: 2776 \t|| Train Loss: 0.21399273355570642 \t|| Test Loss: 0.5118491156725289\n",
      "Epoch: 2777 \t|| Train Loss: 0.21395783355749004 \t|| Test Loss: 0.5117389156752219\n",
      "Epoch: 2778 \t|| Train Loss: 0.2139229335592737 \t|| Test Loss: 0.5116287156779149\n",
      "Epoch: 2779 \t|| Train Loss: 0.21388803356105734 \t|| Test Loss: 0.5115185156806079\n",
      "Epoch: 2780 \t|| Train Loss: 0.21385313356284108 \t|| Test Loss: 0.5114083156833009\n",
      "Epoch: 2781 \t|| Train Loss: 0.21381823356462473 \t|| Test Loss: 0.5112981156859939\n",
      "Epoch: 2782 \t|| Train Loss: 0.21378333356640838 \t|| Test Loss: 0.5111879156886869\n",
      "Epoch: 2783 \t|| Train Loss: 0.21374843356819206 \t|| Test Loss: 0.5110777156913799\n",
      "Epoch: 2784 \t|| Train Loss: 0.21371353356997572 \t|| Test Loss: 0.510967515694073\n",
      "Epoch: 2785 \t|| Train Loss: 0.2136786335717594 \t|| Test Loss: 0.510857315696766\n",
      "Epoch: 2786 \t|| Train Loss: 0.21364373357354305 \t|| Test Loss: 0.510747115699459\n",
      "Epoch: 2787 \t|| Train Loss: 0.2136088335753267 \t|| Test Loss: 0.510636915702152\n",
      "Epoch: 2788 \t|| Train Loss: 0.21357393357711038 \t|| Test Loss: 0.510526715704845\n",
      "Epoch: 2789 \t|| Train Loss: 0.2135390335788941 \t|| Test Loss: 0.510416515707538\n",
      "Epoch: 2790 \t|| Train Loss: 0.2135041335806777 \t|| Test Loss: 0.510306315710231\n",
      "Epoch: 2791 \t|| Train Loss: 0.21346923358246142 \t|| Test Loss: 0.510196115712924\n",
      "Epoch: 2792 \t|| Train Loss: 0.2134343335842451 \t|| Test Loss: 0.510085915715617\n",
      "Epoch: 2793 \t|| Train Loss: 0.21339943358602875 \t|| Test Loss: 0.50997571571831\n",
      "Epoch: 2794 \t|| Train Loss: 0.2133645335878124 \t|| Test Loss: 0.509865515721003\n",
      "Epoch: 2795 \t|| Train Loss: 0.21332963358959614 \t|| Test Loss: 0.509755315723696\n",
      "Epoch: 2796 \t|| Train Loss: 0.21329473359137974 \t|| Test Loss: 0.509645115726389\n",
      "Epoch: 2797 \t|| Train Loss: 0.21325983359316342 \t|| Test Loss: 0.5095349157290819\n",
      "Epoch: 2798 \t|| Train Loss: 0.21322493359494712 \t|| Test Loss: 0.5094247157317751\n",
      "Epoch: 2799 \t|| Train Loss: 0.21319003359673078 \t|| Test Loss: 0.5093145157344681\n",
      "Epoch: 2800 \t|| Train Loss: 0.21315513359851446 \t|| Test Loss: 0.5092043157371611\n",
      "Epoch: 2801 \t|| Train Loss: 0.2131202336002981 \t|| Test Loss: 0.5090941157398541\n",
      "Epoch: 2802 \t|| Train Loss: 0.21308533360208176 \t|| Test Loss: 0.5089839157425471\n",
      "Epoch: 2803 \t|| Train Loss: 0.2130504336038655 \t|| Test Loss: 0.5088737157452401\n",
      "Epoch: 2804 \t|| Train Loss: 0.21301553360564912 \t|| Test Loss: 0.5087635157479331\n",
      "Epoch: 2805 \t|| Train Loss: 0.21298063360743277 \t|| Test Loss: 0.5086533157506261\n",
      "Epoch: 2806 \t|| Train Loss: 0.21294573360921648 \t|| Test Loss: 0.5085431157533191\n",
      "Epoch: 2807 \t|| Train Loss: 0.2129108336110001 \t|| Test Loss: 0.508432915756012\n",
      "Epoch: 2808 \t|| Train Loss: 0.2128759336127838 \t|| Test Loss: 0.5083227157587051\n",
      "Epoch: 2809 \t|| Train Loss: 0.21284103361456747 \t|| Test Loss: 0.508212515761398\n",
      "Epoch: 2810 \t|| Train Loss: 0.21280613361635115 \t|| Test Loss: 0.5081023157640912\n",
      "Epoch: 2811 \t|| Train Loss: 0.2127712336181348 \t|| Test Loss: 0.5079921157667842\n",
      "Epoch: 2812 \t|| Train Loss: 0.21273633361991848 \t|| Test Loss: 0.5078819157694772\n",
      "Epoch: 2813 \t|| Train Loss: 0.21270143362170213 \t|| Test Loss: 0.5077717157721702\n",
      "Epoch: 2814 \t|| Train Loss: 0.2126665336234858 \t|| Test Loss: 0.5076615157748632\n",
      "Epoch: 2815 \t|| Train Loss: 0.21263163362526952 \t|| Test Loss: 0.5075513157775562\n",
      "Epoch: 2816 \t|| Train Loss: 0.21259673362705317 \t|| Test Loss: 0.5074411157802492\n",
      "Epoch: 2817 \t|| Train Loss: 0.2125618336288368 \t|| Test Loss: 0.5073309157829421\n",
      "Epoch: 2818 \t|| Train Loss: 0.21252693363062045 \t|| Test Loss: 0.5072207157856352\n",
      "Epoch: 2819 \t|| Train Loss: 0.21249203363240415 \t|| Test Loss: 0.5071105157883281\n",
      "Epoch: 2820 \t|| Train Loss: 0.21245713363418783 \t|| Test Loss: 0.5070003157910211\n",
      "Epoch: 2821 \t|| Train Loss: 0.21242223363597149 \t|| Test Loss: 0.5068901157937142\n",
      "Epoch: 2822 \t|| Train Loss: 0.21238733363775517 \t|| Test Loss: 0.5067799157964071\n",
      "Epoch: 2823 \t|| Train Loss: 0.21235243363953882 \t|| Test Loss: 0.5066697157991003\n",
      "Epoch: 2824 \t|| Train Loss: 0.2123175336413225 \t|| Test Loss: 0.5065595158017931\n",
      "Epoch: 2825 \t|| Train Loss: 0.21228263364310615 \t|| Test Loss: 0.5064493158044862\n",
      "Epoch: 2826 \t|| Train Loss: 0.2122477336448898 \t|| Test Loss: 0.5063391158071792\n",
      "Epoch: 2827 \t|| Train Loss: 0.21221283364667348 \t|| Test Loss: 0.5062289158098723\n",
      "Epoch: 2828 \t|| Train Loss: 0.2121779336484572 \t|| Test Loss: 0.5061187158125652\n",
      "Epoch: 2829 \t|| Train Loss: 0.21214303365024084 \t|| Test Loss: 0.5060085158152583\n",
      "Epoch: 2830 \t|| Train Loss: 0.21210813365202447 \t|| Test Loss: 0.5058983158179512\n",
      "Epoch: 2831 \t|| Train Loss: 0.21207323365380817 \t|| Test Loss: 0.5057881158206442\n",
      "Epoch: 2832 \t|| Train Loss: 0.21203833365559183 \t|| Test Loss: 0.5056779158233372\n",
      "Epoch: 2833 \t|| Train Loss: 0.2120034336573755 \t|| Test Loss: 0.5055677158260302\n",
      "Epoch: 2834 \t|| Train Loss: 0.21196853365915916 \t|| Test Loss: 0.5054575158287232\n",
      "Epoch: 2835 \t|| Train Loss: 0.21193363366094284 \t|| Test Loss: 0.5053473158314163\n",
      "Epoch: 2836 \t|| Train Loss: 0.2118987336627265 \t|| Test Loss: 0.5052371158341092\n",
      "Epoch: 2837 \t|| Train Loss: 0.21186383366451017 \t|| Test Loss: 0.5051269158368022\n",
      "Epoch: 2838 \t|| Train Loss: 0.21182893366629382 \t|| Test Loss: 0.5050167158394953\n",
      "Epoch: 2839 \t|| Train Loss: 0.21179403366807753 \t|| Test Loss: 0.5049065158421883\n",
      "Epoch: 2840 \t|| Train Loss: 0.21175913366986115 \t|| Test Loss: 0.5047963158448813\n",
      "Epoch: 2841 \t|| Train Loss: 0.21172423367164486 \t|| Test Loss: 0.5046861158475743\n",
      "Epoch: 2842 \t|| Train Loss: 0.21168933367342851 \t|| Test Loss: 0.5045759158502673\n",
      "Epoch: 2843 \t|| Train Loss: 0.2116544336752122 \t|| Test Loss: 0.5044657158529603\n",
      "Epoch: 2844 \t|| Train Loss: 0.21161953367699587 \t|| Test Loss: 0.5043555158556533\n",
      "Epoch: 2845 \t|| Train Loss: 0.21158463367877953 \t|| Test Loss: 0.5042453158583463\n",
      "Epoch: 2846 \t|| Train Loss: 0.21154973368056318 \t|| Test Loss: 0.5041351158610393\n",
      "Epoch: 2847 \t|| Train Loss: 0.21151483368234686 \t|| Test Loss: 0.5040249158637323\n",
      "Epoch: 2848 \t|| Train Loss: 0.21147993368413057 \t|| Test Loss: 0.5039147158664253\n",
      "Epoch: 2849 \t|| Train Loss: 0.21144503368591422 \t|| Test Loss: 0.5038045158691183\n",
      "Epoch: 2850 \t|| Train Loss: 0.2114101336876979 \t|| Test Loss: 0.5036943158718113\n",
      "Epoch: 2851 \t|| Train Loss: 0.21137523368948155 \t|| Test Loss: 0.5035841158745044\n",
      "Epoch: 2852 \t|| Train Loss: 0.21134033369126523 \t|| Test Loss: 0.5034739158771973\n",
      "Epoch: 2853 \t|| Train Loss: 0.21130543369304888 \t|| Test Loss: 0.5033637158798904\n",
      "Epoch: 2854 \t|| Train Loss: 0.21127053369483254 \t|| Test Loss: 0.5032535158825834\n",
      "Epoch: 2855 \t|| Train Loss: 0.21123563369661622 \t|| Test Loss: 0.5031433158852764\n",
      "Epoch: 2856 \t|| Train Loss: 0.21120073369839987 \t|| Test Loss: 0.5030331158879694\n",
      "Epoch: 2857 \t|| Train Loss: 0.21116583370018355 \t|| Test Loss: 0.5029229158906624\n",
      "Epoch: 2858 \t|| Train Loss: 0.21113093370196725 \t|| Test Loss: 0.5028127158933554\n",
      "Epoch: 2859 \t|| Train Loss: 0.2110960337037509 \t|| Test Loss: 0.5027025158960484\n",
      "Epoch: 2860 \t|| Train Loss: 0.21106113370553453 \t|| Test Loss: 0.5025923158987413\n",
      "Epoch: 2861 \t|| Train Loss: 0.21102623370731824 \t|| Test Loss: 0.5024821159014344\n",
      "Epoch: 2862 \t|| Train Loss: 0.2109913337091019 \t|| Test Loss: 0.5023719159041274\n",
      "Epoch: 2863 \t|| Train Loss: 0.21095643371088552 \t|| Test Loss: 0.5022617159068203\n",
      "Epoch: 2864 \t|| Train Loss: 0.21092153371266922 \t|| Test Loss: 0.5021515159095133\n",
      "Epoch: 2865 \t|| Train Loss: 0.2108866337144529 \t|| Test Loss: 0.5020413159122064\n",
      "Epoch: 2866 \t|| Train Loss: 0.21085173371623656 \t|| Test Loss: 0.5019311159148995\n",
      "Epoch: 2867 \t|| Train Loss: 0.21081683371802024 \t|| Test Loss: 0.5018209159175925\n",
      "Epoch: 2868 \t|| Train Loss: 0.21078193371980394 \t|| Test Loss: 0.5017107159202854\n",
      "Epoch: 2869 \t|| Train Loss: 0.2107470337215876 \t|| Test Loss: 0.5016005159229785\n",
      "Epoch: 2870 \t|| Train Loss: 0.21071213372337128 \t|| Test Loss: 0.5014903159256715\n",
      "Epoch: 2871 \t|| Train Loss: 0.21067723372515493 \t|| Test Loss: 0.5013801159283644\n",
      "Epoch: 2872 \t|| Train Loss: 0.21064233372693858 \t|| Test Loss: 0.5012699159310575\n",
      "Epoch: 2873 \t|| Train Loss: 0.21060743372872232 \t|| Test Loss: 0.5011597159337505\n",
      "Epoch: 2874 \t|| Train Loss: 0.2105725337305059 \t|| Test Loss: 0.5010495159364435\n",
      "Epoch: 2875 \t|| Train Loss: 0.2105376337322896 \t|| Test Loss: 0.5009393159391365\n",
      "Epoch: 2876 \t|| Train Loss: 0.2105027337340733 \t|| Test Loss: 0.5008291159418294\n",
      "Epoch: 2877 \t|| Train Loss: 0.21046783373585692 \t|| Test Loss: 0.5007189159445224\n",
      "Epoch: 2878 \t|| Train Loss: 0.21043293373764063 \t|| Test Loss: 0.5006087159472156\n",
      "Epoch: 2879 \t|| Train Loss: 0.2103980337394243 \t|| Test Loss: 0.5004985159499085\n",
      "Epoch: 2880 \t|| Train Loss: 0.21036313374120796 \t|| Test Loss: 0.5003883159526015\n",
      "Epoch: 2881 \t|| Train Loss: 0.21032823374299162 \t|| Test Loss: 0.5002781159552945\n",
      "Epoch: 2882 \t|| Train Loss: 0.21029333374477527 \t|| Test Loss: 0.5001679159579875\n",
      "Epoch: 2883 \t|| Train Loss: 0.210258433746559 \t|| Test Loss: 0.5000577159606805\n",
      "Epoch: 2884 \t|| Train Loss: 0.21022353374834263 \t|| Test Loss: 0.4999475159633736\n",
      "Epoch: 2885 \t|| Train Loss: 0.21018863375012628 \t|| Test Loss: 0.4998373159660666\n",
      "Epoch: 2886 \t|| Train Loss: 0.21015373375190993 \t|| Test Loss: 0.4997271159687596\n",
      "Epoch: 2887 \t|| Train Loss: 0.21011883375369367 \t|| Test Loss: 0.4996169159714525\n",
      "Epoch: 2888 \t|| Train Loss: 0.21008393375547732 \t|| Test Loss: 0.49950671597414553\n",
      "Epoch: 2889 \t|| Train Loss: 0.21004903375726097 \t|| Test Loss: 0.49939651597683854\n",
      "Epoch: 2890 \t|| Train Loss: 0.2100141337590446 \t|| Test Loss: 0.49928631597953144\n",
      "Epoch: 2891 \t|| Train Loss: 0.20997923376082825 \t|| Test Loss: 0.49917611598222456\n",
      "Epoch: 2892 \t|| Train Loss: 0.20994433376261198 \t|| Test Loss: 0.49906591598491756\n",
      "Epoch: 2893 \t|| Train Loss: 0.20990943376439564 \t|| Test Loss: 0.49895571598761057\n",
      "Epoch: 2894 \t|| Train Loss: 0.2098745337661793 \t|| Test Loss: 0.4988455159903036\n",
      "Epoch: 2895 \t|| Train Loss: 0.20983963376796297 \t|| Test Loss: 0.4987353159929965\n",
      "Epoch: 2896 \t|| Train Loss: 0.20980473376974668 \t|| Test Loss: 0.4986251159956896\n",
      "Epoch: 2897 \t|| Train Loss: 0.2097698337715303 \t|| Test Loss: 0.4985149159983825\n",
      "Epoch: 2898 \t|| Train Loss: 0.209734933773314 \t|| Test Loss: 0.4984047160010755\n",
      "Epoch: 2899 \t|| Train Loss: 0.20970003377509766 \t|| Test Loss: 0.4982945160037685\n",
      "Epoch: 2900 \t|| Train Loss: 0.20966513377688134 \t|| Test Loss: 0.4981843160064615\n",
      "Epoch: 2901 \t|| Train Loss: 0.20963023377866502 \t|| Test Loss: 0.4980741160091545\n",
      "Epoch: 2902 \t|| Train Loss: 0.20959533378044867 \t|| Test Loss: 0.49796391601184753\n",
      "Epoch: 2903 \t|| Train Loss: 0.20956043378223232 \t|| Test Loss: 0.49785371601454054\n",
      "Epoch: 2904 \t|| Train Loss: 0.209525533784016 \t|| Test Loss: 0.49774351601723366\n",
      "Epoch: 2905 \t|| Train Loss: 0.20949063378579966 \t|| Test Loss: 0.49763331601992655\n",
      "Epoch: 2906 \t|| Train Loss: 0.20945573378758336 \t|| Test Loss: 0.49752311602261956\n",
      "Epoch: 2907 \t|| Train Loss: 0.209420833789367 \t|| Test Loss: 0.49741291602531257\n",
      "Epoch: 2908 \t|| Train Loss: 0.2093859337911507 \t|| Test Loss: 0.4973027160280057\n",
      "Epoch: 2909 \t|| Train Loss: 0.20935103379293435 \t|| Test Loss: 0.4971925160306986\n",
      "Epoch: 2910 \t|| Train Loss: 0.20931613379471797 \t|| Test Loss: 0.4970823160333916\n",
      "Epoch: 2911 \t|| Train Loss: 0.20928123379650168 \t|| Test Loss: 0.4969721160360846\n",
      "Epoch: 2912 \t|| Train Loss: 0.20924633379828536 \t|| Test Loss: 0.4968619160387776\n",
      "Epoch: 2913 \t|| Train Loss: 0.209211433800069 \t|| Test Loss: 0.4967517160414706\n",
      "Epoch: 2914 \t|| Train Loss: 0.2091765338018527 \t|| Test Loss: 0.4966415160441636\n",
      "Epoch: 2915 \t|| Train Loss: 0.20914163380363635 \t|| Test Loss: 0.49653131604685663\n",
      "Epoch: 2916 \t|| Train Loss: 0.20910673380542 \t|| Test Loss: 0.49642111604954964\n",
      "Epoch: 2917 \t|| Train Loss: 0.20907183380720368 \t|| Test Loss: 0.49631091605224265\n",
      "Epoch: 2918 \t|| Train Loss: 0.20903693380898739 \t|| Test Loss: 0.49620071605493565\n",
      "Epoch: 2919 \t|| Train Loss: 0.20900203381077107 \t|| Test Loss: 0.4960905160576286\n",
      "Epoch: 2920 \t|| Train Loss: 0.20896713381255472 \t|| Test Loss: 0.4959803160603218\n",
      "Epoch: 2921 \t|| Train Loss: 0.20893223381433837 \t|| Test Loss: 0.4958701160630147\n",
      "Epoch: 2922 \t|| Train Loss: 0.20889733381612205 \t|| Test Loss: 0.4957599160657077\n",
      "Epoch: 2923 \t|| Train Loss: 0.2088624338179057 \t|| Test Loss: 0.4956497160684007\n",
      "Epoch: 2924 \t|| Train Loss: 0.20882753381968938 \t|| Test Loss: 0.4955395160710937\n",
      "Epoch: 2925 \t|| Train Loss: 0.20879263382147303 \t|| Test Loss: 0.4954293160737867\n",
      "Epoch: 2926 \t|| Train Loss: 0.20875773382325674 \t|| Test Loss: 0.4953191160764797\n",
      "Epoch: 2927 \t|| Train Loss: 0.20872283382504037 \t|| Test Loss: 0.4952089160791727\n",
      "Epoch: 2928 \t|| Train Loss: 0.20868793382682407 \t|| Test Loss: 0.49509871608186573\n",
      "Epoch: 2929 \t|| Train Loss: 0.20865303382860773 \t|| Test Loss: 0.49498851608455874\n",
      "Epoch: 2930 \t|| Train Loss: 0.2086181338303914 \t|| Test Loss: 0.49487831608725175\n",
      "Epoch: 2931 \t|| Train Loss: 0.20858323383217506 \t|| Test Loss: 0.4947681160899447\n",
      "Epoch: 2932 \t|| Train Loss: 0.20854833383395874 \t|| Test Loss: 0.49465791609263776\n",
      "Epoch: 2933 \t|| Train Loss: 0.2085134338357424 \t|| Test Loss: 0.49454771609533077\n",
      "Epoch: 2934 \t|| Train Loss: 0.20847853383752607 \t|| Test Loss: 0.4944375160980238\n",
      "Epoch: 2935 \t|| Train Loss: 0.20844363383930978 \t|| Test Loss: 0.4943273161007168\n",
      "Epoch: 2936 \t|| Train Loss: 0.20840873384109343 \t|| Test Loss: 0.4942171161034098\n",
      "Epoch: 2937 \t|| Train Loss: 0.20837383384287705 \t|| Test Loss: 0.4941069161061028\n",
      "Epoch: 2938 \t|| Train Loss: 0.2083389338446608 \t|| Test Loss: 0.4939967161087958\n",
      "Epoch: 2939 \t|| Train Loss: 0.20830403384644444 \t|| Test Loss: 0.4938865161114888\n",
      "Epoch: 2940 \t|| Train Loss: 0.2082691338482281 \t|| Test Loss: 0.4937763161141818\n",
      "Epoch: 2941 \t|| Train Loss: 0.20823423385001175 \t|| Test Loss: 0.49366611611687483\n",
      "Epoch: 2942 \t|| Train Loss: 0.2081993338517954 \t|| Test Loss: 0.49355591611956784\n",
      "Epoch: 2943 \t|| Train Loss: 0.20816443385357908 \t|| Test Loss: 0.49344571612226085\n",
      "Epoch: 2944 \t|| Train Loss: 0.20812953385536276 \t|| Test Loss: 0.4933355161249538\n",
      "Epoch: 2945 \t|| Train Loss: 0.2080946338571464 \t|| Test Loss: 0.4932253161276468\n",
      "Epoch: 2946 \t|| Train Loss: 0.20805973385893012 \t|| Test Loss: 0.4931151161303398\n",
      "Epoch: 2947 \t|| Train Loss: 0.20802483386071374 \t|| Test Loss: 0.4930049161330328\n",
      "Epoch: 2948 \t|| Train Loss: 0.20798993386249745 \t|| Test Loss: 0.49289471613572583\n",
      "Epoch: 2949 \t|| Train Loss: 0.2079550338642811 \t|| Test Loss: 0.4927845161384189\n",
      "Epoch: 2950 \t|| Train Loss: 0.20792013386606473 \t|| Test Loss: 0.49267431614111185\n",
      "Epoch: 2951 \t|| Train Loss: 0.20788523386784843 \t|| Test Loss: 0.4925641161438049\n",
      "Epoch: 2952 \t|| Train Loss: 0.20785033386963211 \t|| Test Loss: 0.4924539161464979\n",
      "Epoch: 2953 \t|| Train Loss: 0.20781543387141577 \t|| Test Loss: 0.49234371614919076\n",
      "Epoch: 2954 \t|| Train Loss: 0.2077805338731995 \t|| Test Loss: 0.49223351615188377\n",
      "Epoch: 2955 \t|| Train Loss: 0.2077456338749831 \t|| Test Loss: 0.4921233161545769\n",
      "Epoch: 2956 \t|| Train Loss: 0.2077107338767668 \t|| Test Loss: 0.4920131161572699\n",
      "Epoch: 2957 \t|| Train Loss: 0.20767583387855043 \t|| Test Loss: 0.49190291615996296\n",
      "Epoch: 2958 \t|| Train Loss: 0.20764093388033414 \t|| Test Loss: 0.4917927161626559\n",
      "Epoch: 2959 \t|| Train Loss: 0.2076060338821178 \t|| Test Loss: 0.49168251616534897\n",
      "Epoch: 2960 \t|| Train Loss: 0.20757113388390147 \t|| Test Loss: 0.4915723161680419\n",
      "Epoch: 2961 \t|| Train Loss: 0.20753623388568512 \t|| Test Loss: 0.49146211617073493\n",
      "Epoch: 2962 \t|| Train Loss: 0.2075013338874688 \t|| Test Loss: 0.49135191617342794\n",
      "Epoch: 2963 \t|| Train Loss: 0.20746643388925246 \t|| Test Loss: 0.491241716176121\n",
      "Epoch: 2964 \t|| Train Loss: 0.2074315338910361 \t|| Test Loss: 0.49113151617881395\n",
      "Epoch: 2965 \t|| Train Loss: 0.2073966338928198 \t|| Test Loss: 0.49102131618150685\n",
      "Epoch: 2966 \t|| Train Loss: 0.2073617338946035 \t|| Test Loss: 0.49091111618419997\n",
      "Epoch: 2967 \t|| Train Loss: 0.20732683389638717 \t|| Test Loss: 0.490800916186893\n",
      "Epoch: 2968 \t|| Train Loss: 0.20729193389817077 \t|| Test Loss: 0.490690716189586\n",
      "Epoch: 2969 \t|| Train Loss: 0.20725703389995448 \t|| Test Loss: 0.4905805161922789\n",
      "Epoch: 2970 \t|| Train Loss: 0.20722213390173816 \t|| Test Loss: 0.490470316194972\n",
      "Epoch: 2971 \t|| Train Loss: 0.20718723390352184 \t|| Test Loss: 0.490360116197665\n",
      "Epoch: 2972 \t|| Train Loss: 0.2071523339053055 \t|| Test Loss: 0.4902499162003579\n",
      "Epoch: 2973 \t|| Train Loss: 0.20711743390708914 \t|| Test Loss: 0.4901397162030509\n",
      "Epoch: 2974 \t|| Train Loss: 0.2070825339088728 \t|| Test Loss: 0.4900295162057439\n",
      "Epoch: 2975 \t|| Train Loss: 0.20704763391065653 \t|| Test Loss: 0.48991931620843704\n",
      "Epoch: 2976 \t|| Train Loss: 0.20701273391244018 \t|| Test Loss: 0.48980911621112994\n",
      "Epoch: 2977 \t|| Train Loss: 0.2069778339142238 \t|| Test Loss: 0.48969891621382294\n",
      "Epoch: 2978 \t|| Train Loss: 0.20694293391600746 \t|| Test Loss: 0.48958871621651606\n",
      "Epoch: 2979 \t|| Train Loss: 0.20690803391779117 \t|| Test Loss: 0.48947851621920896\n",
      "Epoch: 2980 \t|| Train Loss: 0.20687313391957485 \t|| Test Loss: 0.48936831622190197\n",
      "Epoch: 2981 \t|| Train Loss: 0.2068382339213585 \t|| Test Loss: 0.4892581162245951\n",
      "Epoch: 2982 \t|| Train Loss: 0.20680333392314218 \t|| Test Loss: 0.489147916227288\n",
      "Epoch: 2983 \t|| Train Loss: 0.2067684339249259 \t|| Test Loss: 0.489037716229981\n",
      "Epoch: 2984 \t|| Train Loss: 0.2067335339267095 \t|| Test Loss: 0.488927516232674\n",
      "Epoch: 2985 \t|| Train Loss: 0.20669863392849322 \t|| Test Loss: 0.488817316235367\n",
      "Epoch: 2986 \t|| Train Loss: 0.20666373393027687 \t|| Test Loss: 0.48870711623806\n",
      "Epoch: 2987 \t|| Train Loss: 0.20662883393206055 \t|| Test Loss: 0.488596916240753\n",
      "Epoch: 2988 \t|| Train Loss: 0.2065939339338442 \t|| Test Loss: 0.48848671624344603\n",
      "Epoch: 2989 \t|| Train Loss: 0.20655903393562786 \t|| Test Loss: 0.48837651624613904\n",
      "Epoch: 2990 \t|| Train Loss: 0.20652413393741154 \t|| Test Loss: 0.48826631624883204\n",
      "Epoch: 2991 \t|| Train Loss: 0.20648923393919522 \t|| Test Loss: 0.48815611625152505\n",
      "Epoch: 2992 \t|| Train Loss: 0.20645433394097887 \t|| Test Loss: 0.48804591625421806\n",
      "Epoch: 2993 \t|| Train Loss: 0.20641943394276252 \t|| Test Loss: 0.48793571625691107\n",
      "Epoch: 2994 \t|| Train Loss: 0.20638453394454617 \t|| Test Loss: 0.4878255162596041\n",
      "Epoch: 2995 \t|| Train Loss: 0.20634963394632982 \t|| Test Loss: 0.4877153162622971\n",
      "Epoch: 2996 \t|| Train Loss: 0.20631473394811356 \t|| Test Loss: 0.4876051162649901\n",
      "Epoch: 2997 \t|| Train Loss: 0.20627983394989718 \t|| Test Loss: 0.4874949162676831\n",
      "Epoch: 2998 \t|| Train Loss: 0.20624493395168084 \t|| Test Loss: 0.4873847162703761\n",
      "Epoch: 2999 \t|| Train Loss: 0.20621003395346457 \t|| Test Loss: 0.4872745162730691\n",
      "Epoch: 3000 \t|| Train Loss: 0.20617513395524822 \t|| Test Loss: 0.4871643162757621\n",
      "Epoch: 3001 \t|| Train Loss: 0.20614023395703188 \t|| Test Loss: 0.48705411627845513\n",
      "Epoch: 3002 \t|| Train Loss: 0.20610533395881556 \t|| Test Loss: 0.48694391628114814\n",
      "Epoch: 3003 \t|| Train Loss: 0.20607043396059926 \t|| Test Loss: 0.48683371628384114\n",
      "Epoch: 3004 \t|| Train Loss: 0.2060355339623829 \t|| Test Loss: 0.4867235162865341\n",
      "Epoch: 3005 \t|| Train Loss: 0.20600063396416654 \t|| Test Loss: 0.48661331628922716\n",
      "Epoch: 3006 \t|| Train Loss: 0.20596573396595025 \t|| Test Loss: 0.48650311629192017\n",
      "Epoch: 3007 \t|| Train Loss: 0.20593083396773387 \t|| Test Loss: 0.4863929162946132\n",
      "Epoch: 3008 \t|| Train Loss: 0.20589593396951758 \t|| Test Loss: 0.4862827162973062\n",
      "Epoch: 3009 \t|| Train Loss: 0.20586103397130126 \t|| Test Loss: 0.4861725162999992\n",
      "Epoch: 3010 \t|| Train Loss: 0.20582613397308486 \t|| Test Loss: 0.48606231630269214\n",
      "Epoch: 3011 \t|| Train Loss: 0.2057912339748686 \t|| Test Loss: 0.4859521163053852\n",
      "Epoch: 3012 \t|| Train Loss: 0.20575633397665224 \t|| Test Loss: 0.4858419163080782\n",
      "Epoch: 3013 \t|| Train Loss: 0.2057214339784359 \t|| Test Loss: 0.4857317163107712\n",
      "Epoch: 3014 \t|| Train Loss: 0.20568653398021958 \t|| Test Loss: 0.48562151631346423\n",
      "Epoch: 3015 \t|| Train Loss: 0.20565163398200323 \t|| Test Loss: 0.4855113163161572\n",
      "Epoch: 3016 \t|| Train Loss: 0.20561673398378694 \t|| Test Loss: 0.4854011163188502\n",
      "Epoch: 3017 \t|| Train Loss: 0.20558183398557056 \t|| Test Loss: 0.4852909163215432\n",
      "Epoch: 3018 \t|| Train Loss: 0.20554693398735427 \t|| Test Loss: 0.4851807163242362\n",
      "Epoch: 3019 \t|| Train Loss: 0.20551203398913792 \t|| Test Loss: 0.4850705163269292\n",
      "Epoch: 3020 \t|| Train Loss: 0.2054771339909216 \t|| Test Loss: 0.4849603163296222\n",
      "Epoch: 3021 \t|| Train Loss: 0.20544223399270528 \t|| Test Loss: 0.48485011633231523\n",
      "Epoch: 3022 \t|| Train Loss: 0.2054073339944889 \t|| Test Loss: 0.4847399163350083\n",
      "Epoch: 3023 \t|| Train Loss: 0.20537243399627264 \t|| Test Loss: 0.48462971633770124\n",
      "Epoch: 3024 \t|| Train Loss: 0.20533753399805627 \t|| Test Loss: 0.4845195163403943\n",
      "Epoch: 3025 \t|| Train Loss: 0.20530263399983992 \t|| Test Loss: 0.48440931634308726\n",
      "Epoch: 3026 \t|| Train Loss: 0.20526773400162363 \t|| Test Loss: 0.48429911634578027\n",
      "Epoch: 3027 \t|| Train Loss: 0.2052328340034073 \t|| Test Loss: 0.4841889163484733\n",
      "Epoch: 3028 \t|| Train Loss: 0.20519793400519096 \t|| Test Loss: 0.4840787163511663\n",
      "Epoch: 3029 \t|| Train Loss: 0.20516303400697464 \t|| Test Loss: 0.4839685163538593\n",
      "Epoch: 3030 \t|| Train Loss: 0.2051281340087583 \t|| Test Loss: 0.4838583163565522\n",
      "Epoch: 3031 \t|| Train Loss: 0.20509323401054197 \t|| Test Loss: 0.4837481163592453\n",
      "Epoch: 3032 \t|| Train Loss: 0.20505833401232562 \t|| Test Loss: 0.4836379163619383\n",
      "Epoch: 3033 \t|| Train Loss: 0.20502343401410927 \t|| Test Loss: 0.4835277163646312\n",
      "Epoch: 3034 \t|| Train Loss: 0.20498853401589295 \t|| Test Loss: 0.48341751636732433\n",
      "Epoch: 3035 \t|| Train Loss: 0.20495363401767666 \t|| Test Loss: 0.48330731637001734\n",
      "Epoch: 3036 \t|| Train Loss: 0.2049187340194603 \t|| Test Loss: 0.48319711637271023\n",
      "Epoch: 3037 \t|| Train Loss: 0.20488383402124394 \t|| Test Loss: 0.48308691637540335\n",
      "Epoch: 3038 \t|| Train Loss: 0.20484893402302765 \t|| Test Loss: 0.48297671637809625\n",
      "Epoch: 3039 \t|| Train Loss: 0.20481403402481133 \t|| Test Loss: 0.48286651638078937\n",
      "Epoch: 3040 \t|| Train Loss: 0.20477913402659498 \t|| Test Loss: 0.48275631638348226\n",
      "Epoch: 3041 \t|| Train Loss: 0.20474423402837863 \t|| Test Loss: 0.4826461163861754\n",
      "Epoch: 3042 \t|| Train Loss: 0.2047093340301623 \t|| Test Loss: 0.4825359163888684\n",
      "Epoch: 3043 \t|| Train Loss: 0.20467443403194602 \t|| Test Loss: 0.4824257163915613\n",
      "Epoch: 3044 \t|| Train Loss: 0.20463953403372964 \t|| Test Loss: 0.4823155163942544\n",
      "Epoch: 3045 \t|| Train Loss: 0.2046046340355133 \t|| Test Loss: 0.4822053163969474\n",
      "Epoch: 3046 \t|| Train Loss: 0.204569734037297 \t|| Test Loss: 0.4820951163996404\n",
      "Epoch: 3047 \t|| Train Loss: 0.20453483403908068 \t|| Test Loss: 0.48198491640233343\n",
      "Epoch: 3048 \t|| Train Loss: 0.20449993404086433 \t|| Test Loss: 0.48187471640502644\n",
      "Epoch: 3049 \t|| Train Loss: 0.204465034042648 \t|| Test Loss: 0.48176451640771945\n",
      "Epoch: 3050 \t|| Train Loss: 0.20443013404443167 \t|| Test Loss: 0.48165431641041234\n",
      "Epoch: 3051 \t|| Train Loss: 0.20439523404621532 \t|| Test Loss: 0.48154411641310546\n",
      "Epoch: 3052 \t|| Train Loss: 0.204360334047999 \t|| Test Loss: 0.48143391641579847\n",
      "Epoch: 3053 \t|| Train Loss: 0.20432543404978265 \t|| Test Loss: 0.48132371641849137\n",
      "Epoch: 3054 \t|| Train Loss: 0.20429053405156633 \t|| Test Loss: 0.4812135164211844\n",
      "Epoch: 3055 \t|| Train Loss: 0.20425563405334998 \t|| Test Loss: 0.4811033164238775\n",
      "Epoch: 3056 \t|| Train Loss: 0.2042207340551337 \t|| Test Loss: 0.4809931164265704\n",
      "Epoch: 3057 \t|| Train Loss: 0.20418583405691731 \t|| Test Loss: 0.4808829164292634\n",
      "Epoch: 3058 \t|| Train Loss: 0.20415093405870102 \t|| Test Loss: 0.4807727164319564\n",
      "Epoch: 3059 \t|| Train Loss: 0.2041160340604847 \t|| Test Loss: 0.4806625164346494\n",
      "Epoch: 3060 \t|| Train Loss: 0.20408113406226835 \t|| Test Loss: 0.4805523164373424\n",
      "Epoch: 3061 \t|| Train Loss: 0.204046234064052 \t|| Test Loss: 0.4804421164400354\n",
      "Epoch: 3062 \t|| Train Loss: 0.2040113340658357 \t|| Test Loss: 0.48033191644272843\n",
      "Epoch: 3063 \t|| Train Loss: 0.20397643406761934 \t|| Test Loss: 0.48022171644542144\n",
      "Epoch: 3064 \t|| Train Loss: 0.20394153406940302 \t|| Test Loss: 0.48011151644811445\n",
      "Epoch: 3065 \t|| Train Loss: 0.20390663407118673 \t|| Test Loss: 0.48000131645080746\n",
      "Epoch: 3066 \t|| Train Loss: 0.20387173407297038 \t|| Test Loss: 0.47989111645350047\n",
      "Epoch: 3067 \t|| Train Loss: 0.20383683407475406 \t|| Test Loss: 0.4797809164561935\n",
      "Epoch: 3068 \t|| Train Loss: 0.2038019340765377 \t|| Test Loss: 0.4796707164588865\n",
      "Epoch: 3069 \t|| Train Loss: 0.20376703407832136 \t|| Test Loss: 0.4795605164615795\n",
      "Epoch: 3070 \t|| Train Loss: 0.2037321340801051 \t|| Test Loss: 0.4794503164642725\n",
      "Epoch: 3071 \t|| Train Loss: 0.20369723408188872 \t|| Test Loss: 0.4793401164669655\n",
      "Epoch: 3072 \t|| Train Loss: 0.20366233408367238 \t|| Test Loss: 0.4792299164696585\n",
      "Epoch: 3073 \t|| Train Loss: 0.20362743408545603 \t|| Test Loss: 0.4791197164723515\n",
      "Epoch: 3074 \t|| Train Loss: 0.20359253408723976 \t|| Test Loss: 0.47900951647504453\n",
      "Epoch: 3075 \t|| Train Loss: 0.20355763408902336 \t|| Test Loss: 0.47889931647773754\n",
      "Epoch: 3076 \t|| Train Loss: 0.20352273409080707 \t|| Test Loss: 0.47878911648043054\n",
      "Epoch: 3077 \t|| Train Loss: 0.2034878340925907 \t|| Test Loss: 0.47867891648312344\n",
      "Epoch: 3078 \t|| Train Loss: 0.2034529340943744 \t|| Test Loss: 0.47856871648581656\n",
      "Epoch: 3079 \t|| Train Loss: 0.20341803409615808 \t|| Test Loss: 0.47845851648850957\n",
      "Epoch: 3080 \t|| Train Loss: 0.20338313409794173 \t|| Test Loss: 0.4783483164912026\n",
      "Epoch: 3081 \t|| Train Loss: 0.20334823409972538 \t|| Test Loss: 0.47823811649389547\n",
      "Epoch: 3082 \t|| Train Loss: 0.20331333410150906 \t|| Test Loss: 0.4781279164965886\n",
      "Epoch: 3083 \t|| Train Loss: 0.20327843410329277 \t|| Test Loss: 0.4780177164992816\n",
      "Epoch: 3084 \t|| Train Loss: 0.2032435341050764 \t|| Test Loss: 0.4779075165019746\n",
      "Epoch: 3085 \t|| Train Loss: 0.20320863410686005 \t|| Test Loss: 0.4777973165046676\n",
      "Epoch: 3086 \t|| Train Loss: 0.2031737341086437 \t|| Test Loss: 0.4776871165073606\n",
      "Epoch: 3087 \t|| Train Loss: 0.20313883411042738 \t|| Test Loss: 0.4775769165100535\n",
      "Epoch: 3088 \t|| Train Loss: 0.2031039341122111 \t|| Test Loss: 0.47746671651274664\n",
      "Epoch: 3089 \t|| Train Loss: 0.20306903411399474 \t|| Test Loss: 0.47735651651543953\n",
      "Epoch: 3090 \t|| Train Loss: 0.20303413411577848 \t|| Test Loss: 0.47724631651813265\n",
      "Epoch: 3091 \t|| Train Loss: 0.2029992341175621 \t|| Test Loss: 0.47713611652082566\n",
      "Epoch: 3092 \t|| Train Loss: 0.20296433411934575 \t|| Test Loss: 0.47702591652351856\n",
      "Epoch: 3093 \t|| Train Loss: 0.2029294341211294 \t|| Test Loss: 0.47691571652621156\n",
      "Epoch: 3094 \t|| Train Loss: 0.20289453412291308 \t|| Test Loss: 0.47680551652890457\n",
      "Epoch: 3095 \t|| Train Loss: 0.2028596341246968 \t|| Test Loss: 0.4766953165315977\n",
      "Epoch: 3096 \t|| Train Loss: 0.20282473412648044 \t|| Test Loss: 0.4765851165342906\n",
      "Epoch: 3097 \t|| Train Loss: 0.20278983412826407 \t|| Test Loss: 0.4764749165369836\n",
      "Epoch: 3098 \t|| Train Loss: 0.20275493413004772 \t|| Test Loss: 0.4763647165396766\n",
      "Epoch: 3099 \t|| Train Loss: 0.20272003413183143 \t|| Test Loss: 0.4762545165423696\n",
      "Epoch: 3100 \t|| Train Loss: 0.20268513413361516 \t|| Test Loss: 0.4761443165450626\n",
      "Epoch: 3101 \t|| Train Loss: 0.20265023413539876 \t|| Test Loss: 0.4760341165477556\n",
      "Epoch: 3102 \t|| Train Loss: 0.20261533413718244 \t|| Test Loss: 0.47592391655044863\n",
      "Epoch: 3103 \t|| Train Loss: 0.20258043413896615 \t|| Test Loss: 0.47581371655314164\n",
      "Epoch: 3104 \t|| Train Loss: 0.20254553414074977 \t|| Test Loss: 0.47570351655583465\n",
      "Epoch: 3105 \t|| Train Loss: 0.20251063414253348 \t|| Test Loss: 0.47559331655852766\n",
      "Epoch: 3106 \t|| Train Loss: 0.20247573414431713 \t|| Test Loss: 0.47548311656122066\n",
      "Epoch: 3107 \t|| Train Loss: 0.2024408341461008 \t|| Test Loss: 0.4753729165639136\n",
      "Epoch: 3108 \t|| Train Loss: 0.20240593414788446 \t|| Test Loss: 0.4752627165666067\n",
      "Epoch: 3109 \t|| Train Loss: 0.20237103414966812 \t|| Test Loss: 0.47515251656929963\n",
      "Epoch: 3110 \t|| Train Loss: 0.2023361341514518 \t|| Test Loss: 0.4750423165719927\n",
      "Epoch: 3111 \t|| Train Loss: 0.20230123415323548 \t|| Test Loss: 0.4749321165746857\n",
      "Epoch: 3112 \t|| Train Loss: 0.20226633415501913 \t|| Test Loss: 0.47482191657737866\n",
      "Epoch: 3113 \t|| Train Loss: 0.20223143415680284 \t|| Test Loss: 0.47471171658007166\n",
      "Epoch: 3114 \t|| Train Loss: 0.20219653415858646 \t|| Test Loss: 0.4746015165827647\n",
      "Epoch: 3115 \t|| Train Loss: 0.2021616341603701 \t|| Test Loss: 0.47449131658545773\n",
      "Epoch: 3116 \t|| Train Loss: 0.20212673416215382 \t|| Test Loss: 0.4743811165881507\n",
      "Epoch: 3117 \t|| Train Loss: 0.2020918341639375 \t|| Test Loss: 0.4742709165908437\n",
      "Epoch: 3118 \t|| Train Loss: 0.20205693416572118 \t|| Test Loss: 0.4741607165935367\n",
      "Epoch: 3119 \t|| Train Loss: 0.20202203416750483 \t|| Test Loss: 0.4740505165962297\n",
      "Epoch: 3120 \t|| Train Loss: 0.20198713416928848 \t|| Test Loss: 0.4739403165989227\n",
      "Epoch: 3121 \t|| Train Loss: 0.20195223417107216 \t|| Test Loss: 0.4738301166016157\n",
      "Epoch: 3122 \t|| Train Loss: 0.20191733417285587 \t|| Test Loss: 0.4737199166043088\n",
      "Epoch: 3123 \t|| Train Loss: 0.20188243417463952 \t|| Test Loss: 0.47360971660700174\n",
      "Epoch: 3124 \t|| Train Loss: 0.20184753417642315 \t|| Test Loss: 0.47349951660969475\n",
      "Epoch: 3125 \t|| Train Loss: 0.2018126341782068 \t|| Test Loss: 0.47338931661238776\n",
      "Epoch: 3126 \t|| Train Loss: 0.2017777341799905 \t|| Test Loss: 0.4732791166150808\n",
      "Epoch: 3127 \t|| Train Loss: 0.2017428341817742 \t|| Test Loss: 0.47316891661777377\n",
      "Epoch: 3128 \t|| Train Loss: 0.20170793418355784 \t|| Test Loss: 0.4730587166204668\n",
      "Epoch: 3129 \t|| Train Loss: 0.20167303418534152 \t|| Test Loss: 0.4729485166231598\n",
      "Epoch: 3130 \t|| Train Loss: 0.20163813418712517 \t|| Test Loss: 0.4728383166258528\n",
      "Epoch: 3131 \t|| Train Loss: 0.20160323418890885 \t|| Test Loss: 0.4727281166285458\n",
      "Epoch: 3132 \t|| Train Loss: 0.2015683341906925 \t|| Test Loss: 0.47261791663123887\n",
      "Epoch: 3133 \t|| Train Loss: 0.2015334341924762 \t|| Test Loss: 0.4725077166339318\n",
      "Epoch: 3134 \t|| Train Loss: 0.20149853419425984 \t|| Test Loss: 0.4723975166366249\n",
      "Epoch: 3135 \t|| Train Loss: 0.20146363419604355 \t|| Test Loss: 0.47228731663931783\n",
      "Epoch: 3136 \t|| Train Loss: 0.2014287341978272 \t|| Test Loss: 0.4721771166420109\n",
      "Epoch: 3137 \t|| Train Loss: 0.20139383419961088 \t|| Test Loss: 0.47206691664470385\n",
      "Epoch: 3138 \t|| Train Loss: 0.20135893420139453 \t|| Test Loss: 0.47195671664739686\n",
      "Epoch: 3139 \t|| Train Loss: 0.2013240342031782 \t|| Test Loss: 0.47184651665008986\n",
      "Epoch: 3140 \t|| Train Loss: 0.20128913420496186 \t|| Test Loss: 0.47173631665278287\n",
      "Epoch: 3141 \t|| Train Loss: 0.2012542342067455 \t|| Test Loss: 0.4716261166554759\n",
      "Epoch: 3142 \t|| Train Loss: 0.2012193342085292 \t|| Test Loss: 0.4715159166581689\n",
      "Epoch: 3143 \t|| Train Loss: 0.2011844342103129 \t|| Test Loss: 0.4714057166608619\n",
      "Epoch: 3144 \t|| Train Loss: 0.20114953421209653 \t|| Test Loss: 0.4712955166635549\n",
      "Epoch: 3145 \t|| Train Loss: 0.20111463421388023 \t|| Test Loss: 0.4711853166662479\n",
      "Epoch: 3146 \t|| Train Loss: 0.20107973421566389 \t|| Test Loss: 0.4710751166689409\n",
      "Epoch: 3147 \t|| Train Loss: 0.20104483421744757 \t|| Test Loss: 0.4709649166716339\n",
      "Epoch: 3148 \t|| Train Loss: 0.20100993421923122 \t|| Test Loss: 0.47085471667432693\n",
      "Epoch: 3149 \t|| Train Loss: 0.20097503422101487 \t|| Test Loss: 0.47074451667701994\n",
      "Epoch: 3150 \t|| Train Loss: 0.20094013422279855 \t|| Test Loss: 0.47063431667971295\n",
      "Epoch: 3151 \t|| Train Loss: 0.20090523422458223 \t|| Test Loss: 0.47052411668240596\n",
      "Epoch: 3152 \t|| Train Loss: 0.20087033422636588 \t|| Test Loss: 0.47041391668509885\n",
      "Epoch: 3153 \t|| Train Loss: 0.2008354342281496 \t|| Test Loss: 0.470303716687792\n",
      "Epoch: 3154 \t|| Train Loss: 0.20080053422993321 \t|| Test Loss: 0.470193516690485\n",
      "Epoch: 3155 \t|| Train Loss: 0.20076563423171692 \t|| Test Loss: 0.470083316693178\n",
      "Epoch: 3156 \t|| Train Loss: 0.20073073423350057 \t|| Test Loss: 0.469973116695871\n",
      "Epoch: 3157 \t|| Train Loss: 0.2006958342352842 \t|| Test Loss: 0.469862916698564\n",
      "Epoch: 3158 \t|| Train Loss: 0.2006609342370679 \t|| Test Loss: 0.469752716701257\n",
      "Epoch: 3159 \t|| Train Loss: 0.20062603423885159 \t|| Test Loss: 0.46964251670395\n",
      "Epoch: 3160 \t|| Train Loss: 0.20059113424063524 \t|| Test Loss: 0.4695323167066429\n",
      "Epoch: 3161 \t|| Train Loss: 0.2005562342424189 \t|| Test Loss: 0.46942211670933603\n",
      "Epoch: 3162 \t|| Train Loss: 0.20052133424420257 \t|| Test Loss: 0.46931191671202893\n",
      "Epoch: 3163 \t|| Train Loss: 0.20048643424598628 \t|| Test Loss: 0.46920171671472205\n",
      "Epoch: 3164 \t|| Train Loss: 0.2004515342477699 \t|| Test Loss: 0.46909151671741506\n",
      "Epoch: 3165 \t|| Train Loss: 0.2004166342495536 \t|| Test Loss: 0.46898131672010807\n",
      "Epoch: 3166 \t|| Train Loss: 0.20038173425133726 \t|| Test Loss: 0.46887111672280096\n",
      "Epoch: 3167 \t|| Train Loss: 0.20034683425312094 \t|| Test Loss: 0.46876091672549397\n",
      "Epoch: 3168 \t|| Train Loss: 0.2003119342549046 \t|| Test Loss: 0.468650716728187\n",
      "Epoch: 3169 \t|| Train Loss: 0.20027703425668827 \t|| Test Loss: 0.46854051673088\n",
      "Epoch: 3170 \t|| Train Loss: 0.20024213425847198 \t|| Test Loss: 0.468430316733573\n",
      "Epoch: 3171 \t|| Train Loss: 0.20020723426025558 \t|| Test Loss: 0.468320116736266\n",
      "Epoch: 3172 \t|| Train Loss: 0.20017233426203926 \t|| Test Loss: 0.468209916738959\n",
      "Epoch: 3173 \t|| Train Loss: 0.20013743426382297 \t|| Test Loss: 0.468099716741652\n",
      "Epoch: 3174 \t|| Train Loss: 0.2001025342656066 \t|| Test Loss: 0.467989516744345\n",
      "Epoch: 3175 \t|| Train Loss: 0.2000676342673903 \t|| Test Loss: 0.46787931674703803\n",
      "Epoch: 3176 \t|| Train Loss: 0.20003273426917395 \t|| Test Loss: 0.46776911674973104\n",
      "Epoch: 3177 \t|| Train Loss: 0.19999783427095763 \t|| Test Loss: 0.46765891675242405\n",
      "Epoch: 3178 \t|| Train Loss: 0.19996293427274128 \t|| Test Loss: 0.46754871675511706\n",
      "Epoch: 3179 \t|| Train Loss: 0.19992803427452496 \t|| Test Loss: 0.46743851675781006\n",
      "Epoch: 3180 \t|| Train Loss: 0.19989313427630861 \t|| Test Loss: 0.46732831676050307\n",
      "Epoch: 3181 \t|| Train Loss: 0.19985832061125114 \t|| Test Loss: 0.4672868067628128\n",
      "Epoch: 3182 \t|| Train Loss: 0.1998246142796539 \t|| Test Loss: 0.4671766067655058\n",
      "Epoch: 3183 \t|| Train Loss: 0.19978971428143755 \t|| Test Loss: 0.4670664067681988\n",
      "Epoch: 3184 \t|| Train Loss: 0.19975579961574144 \t|| Test Loss: 0.46702489677050857\n",
      "Epoch: 3185 \t|| Train Loss: 0.19972119428478283 \t|| Test Loss: 0.4669146967732016\n",
      "Epoch: 3186 \t|| Train Loss: 0.19968689861867012 \t|| Test Loss: 0.4668731867755113\n",
      "Epoch: 3187 \t|| Train Loss: 0.1996526742881281 \t|| Test Loss: 0.4667629867782043\n",
      "Epoch: 3188 \t|| Train Loss: 0.1996179976215988 \t|| Test Loss: 0.46672147678051407\n",
      "Epoch: 3189 \t|| Train Loss: 0.19958415429147341 \t|| Test Loss: 0.466611276783207\n",
      "Epoch: 3190 \t|| Train Loss: 0.19954925429325704 \t|| Test Loss: 0.4665010767859002\n",
      "Epoch: 3191 \t|| Train Loss: 0.19951547662608907 \t|| Test Loss: 0.4664595667882098\n",
      "Epoch: 3192 \t|| Train Loss: 0.19948073429660235 \t|| Test Loss: 0.46634936679090283\n",
      "Epoch: 3193 \t|| Train Loss: 0.19944657562901774 \t|| Test Loss: 0.46630785679321257\n",
      "Epoch: 3194 \t|| Train Loss: 0.19941221429994757 \t|| Test Loss: 0.4661976567959056\n",
      "Epoch: 3195 \t|| Train Loss: 0.19937767463194644 \t|| Test Loss: 0.4661561467982153\n",
      "Epoch: 3196 \t|| Train Loss: 0.19934369430329288 \t|| Test Loss: 0.46604594680090833\n",
      "Epoch: 3197 \t|| Train Loss: 0.19930879430507656 \t|| Test Loss: 0.46593574680360134\n",
      "Epoch: 3198 \t|| Train Loss: 0.19927515363643672 \t|| Test Loss: 0.465894236805911\n",
      "Epoch: 3199 \t|| Train Loss: 0.1992402743084218 \t|| Test Loss: 0.4657840368086041\n",
      "Epoch: 3200 \t|| Train Loss: 0.1992062526393654 \t|| Test Loss: 0.4657425268109138\n",
      "Epoch: 3201 \t|| Train Loss: 0.1991717543117671 \t|| Test Loss: 0.46563232681360683\n",
      "Epoch: 3202 \t|| Train Loss: 0.19913735164229412 \t|| Test Loss: 0.4655908168159165\n",
      "Epoch: 3203 \t|| Train Loss: 0.19910323431511237 \t|| Test Loss: 0.4654806168186097\n",
      "Epoch: 3204 \t|| Train Loss: 0.19906845064522277 \t|| Test Loss: 0.46543910682091927\n",
      "Epoch: 3205 \t|| Train Loss: 0.19903471431845765 \t|| Test Loss: 0.46532890682361233\n",
      "Epoch: 3206 \t|| Train Loss: 0.1989998143202413 \t|| Test Loss: 0.46521870682630534\n",
      "Epoch: 3207 \t|| Train Loss: 0.19896592964971305 \t|| Test Loss: 0.4651771968286151\n",
      "Epoch: 3208 \t|| Train Loss: 0.19893129432358658 \t|| Test Loss: 0.4650669968313081\n",
      "Epoch: 3209 \t|| Train Loss: 0.19889702865264175 \t|| Test Loss: 0.46502548683361783\n",
      "Epoch: 3210 \t|| Train Loss: 0.19886277432693186 \t|| Test Loss: 0.46491528683631084\n",
      "Epoch: 3211 \t|| Train Loss: 0.19882812765557042 \t|| Test Loss: 0.4648737768386206\n",
      "Epoch: 3212 \t|| Train Loss: 0.19879425433027714 \t|| Test Loss: 0.4647635768413136\n",
      "Epoch: 3213 \t|| Train Loss: 0.1987593543320608 \t|| Test Loss: 0.4646533768440066\n",
      "Epoch: 3214 \t|| Train Loss: 0.19872560666006073 \t|| Test Loss: 0.46461186684631633\n",
      "Epoch: 3215 \t|| Train Loss: 0.19869083433540607 \t|| Test Loss: 0.46450166684900934\n",
      "Epoch: 3216 \t|| Train Loss: 0.1986567056629894 \t|| Test Loss: 0.4644601568513191\n",
      "Epoch: 3217 \t|| Train Loss: 0.19862231433875133 \t|| Test Loss: 0.4643499568540121\n",
      "Epoch: 3218 \t|| Train Loss: 0.19858780466591808 \t|| Test Loss: 0.46430844685632183\n",
      "Epoch: 3219 \t|| Train Loss: 0.19855379434209663 \t|| Test Loss: 0.46419824685901484\n",
      "Epoch: 3220 \t|| Train Loss: 0.19851890366884675 \t|| Test Loss: 0.4641567368613246\n",
      "Epoch: 3221 \t|| Train Loss: 0.1984852743454419 \t|| Test Loss: 0.4640465368640176\n",
      "Epoch: 3222 \t|| Train Loss: 0.19845037434722557 \t|| Test Loss: 0.4639363368667106\n",
      "Epoch: 3223 \t|| Train Loss: 0.19841638267333703 \t|| Test Loss: 0.46389482686902034\n",
      "Epoch: 3224 \t|| Train Loss: 0.19838185435057085 \t|| Test Loss: 0.46378462687171335\n",
      "Epoch: 3225 \t|| Train Loss: 0.19834748167626573 \t|| Test Loss: 0.46374311687402303\n",
      "Epoch: 3226 \t|| Train Loss: 0.19831333435391613 \t|| Test Loss: 0.46363291687671604\n",
      "Epoch: 3227 \t|| Train Loss: 0.1982785806791944 \t|| Test Loss: 0.46359140687902584\n",
      "Epoch: 3228 \t|| Train Loss: 0.19824481435726135 \t|| Test Loss: 0.46348120688171884\n",
      "Epoch: 3229 \t|| Train Loss: 0.19820991435904506 \t|| Test Loss: 0.46337100688441185\n",
      "Epoch: 3230 \t|| Train Loss: 0.1981760596836847 \t|| Test Loss: 0.4633294968867216\n",
      "Epoch: 3231 \t|| Train Loss: 0.19814139436239037 \t|| Test Loss: 0.46321929688941454\n",
      "Epoch: 3232 \t|| Train Loss: 0.19810715868661338 \t|| Test Loss: 0.46317778689172434\n",
      "Epoch: 3233 \t|| Train Loss: 0.19807287436573562 \t|| Test Loss: 0.46306758689441735\n",
      "Epoch: 3234 \t|| Train Loss: 0.19803825768954209 \t|| Test Loss: 0.4630260768967271\n",
      "Epoch: 3235 \t|| Train Loss: 0.19800435436908087 \t|| Test Loss: 0.4629158768994201\n",
      "Epoch: 3236 \t|| Train Loss: 0.19796945437086455 \t|| Test Loss: 0.4628056769021131\n",
      "Epoch: 3237 \t|| Train Loss: 0.19793573669403236 \t|| Test Loss: 0.46276416690442285\n",
      "Epoch: 3238 \t|| Train Loss: 0.1979009343742098 \t|| Test Loss: 0.46265396690711585\n",
      "Epoch: 3239 \t|| Train Loss: 0.19786683569696106 \t|| Test Loss: 0.4626124569094256\n",
      "Epoch: 3240 \t|| Train Loss: 0.1978324143775551 \t|| Test Loss: 0.4625022569121186\n",
      "Epoch: 3241 \t|| Train Loss: 0.19779793469988974 \t|| Test Loss: 0.46246074691442834\n",
      "Epoch: 3242 \t|| Train Loss: 0.19776389438090036 \t|| Test Loss: 0.46235054691712135\n",
      "Epoch: 3243 \t|| Train Loss: 0.19772903370281844 \t|| Test Loss: 0.4623090369194311\n",
      "Epoch: 3244 \t|| Train Loss: 0.19769537438424564 \t|| Test Loss: 0.4621988369221241\n",
      "Epoch: 3245 \t|| Train Loss: 0.1976604743860293 \t|| Test Loss: 0.46208863692481705\n",
      "Epoch: 3246 \t|| Train Loss: 0.19762651270730872 \t|| Test Loss: 0.46204712692712685\n",
      "Epoch: 3247 \t|| Train Loss: 0.1975919543893746 \t|| Test Loss: 0.46193692692981986\n",
      "Epoch: 3248 \t|| Train Loss: 0.1975576117102374 \t|| Test Loss: 0.4618954169321296\n",
      "Epoch: 3249 \t|| Train Loss: 0.19752343439271985 \t|| Test Loss: 0.4617852169348226\n",
      "Epoch: 3250 \t|| Train Loss: 0.19748871071316607 \t|| Test Loss: 0.46174370693713235\n",
      "Epoch: 3251 \t|| Train Loss: 0.19745491439606516 \t|| Test Loss: 0.46163350693982536\n",
      "Epoch: 3252 \t|| Train Loss: 0.19742001439784881 \t|| Test Loss: 0.46152330694251836\n",
      "Epoch: 3253 \t|| Train Loss: 0.19738618971765637 \t|| Test Loss: 0.4614817969448281\n",
      "Epoch: 3254 \t|| Train Loss: 0.19735149440119407 \t|| Test Loss: 0.4613715969475211\n",
      "Epoch: 3255 \t|| Train Loss: 0.19731728872058504 \t|| Test Loss: 0.46133008694983085\n",
      "Epoch: 3256 \t|| Train Loss: 0.19728297440453935 \t|| Test Loss: 0.46121988695252386\n",
      "Epoch: 3257 \t|| Train Loss: 0.1972483877235137 \t|| Test Loss: 0.46117837695483355\n",
      "Epoch: 3258 \t|| Train Loss: 0.19721445440788465 \t|| Test Loss: 0.46106817695752655\n",
      "Epoch: 3259 \t|| Train Loss: 0.1971795544096683 \t|| Test Loss: 0.4609579769602196\n",
      "Epoch: 3260 \t|| Train Loss: 0.19714586672800402 \t|| Test Loss: 0.46091646696252936\n",
      "Epoch: 3261 \t|| Train Loss: 0.19711103441301356 \t|| Test Loss: 0.46080626696522237\n",
      "Epoch: 3262 \t|| Train Loss: 0.19707696573093267 \t|| Test Loss: 0.4607647569675321\n",
      "Epoch: 3263 \t|| Train Loss: 0.19704251441635884 \t|| Test Loss: 0.4606545569702251\n",
      "Epoch: 3264 \t|| Train Loss: 0.19700806473386137 \t|| Test Loss: 0.46061304697253486\n",
      "Epoch: 3265 \t|| Train Loss: 0.19697399441970412 \t|| Test Loss: 0.4605028469752278\n",
      "Epoch: 3266 \t|| Train Loss: 0.19693916373679005 \t|| Test Loss: 0.4604613369775376\n",
      "Epoch: 3267 \t|| Train Loss: 0.1969054744230494 \t|| Test Loss: 0.4603511369802306\n",
      "Epoch: 3268 \t|| Train Loss: 0.19687057442483305 \t|| Test Loss: 0.4602409369829236\n",
      "Epoch: 3269 \t|| Train Loss: 0.19683664274128035 \t|| Test Loss: 0.46019942698523336\n",
      "Epoch: 3270 \t|| Train Loss: 0.19680205442817833 \t|| Test Loss: 0.46008922698792637\n",
      "Epoch: 3271 \t|| Train Loss: 0.19676774174420902 \t|| Test Loss: 0.4600477169902361\n",
      "Epoch: 3272 \t|| Train Loss: 0.19673353443152358 \t|| Test Loss: 0.45993751699292906\n",
      "Epoch: 3273 \t|| Train Loss: 0.19669884074713773 \t|| Test Loss: 0.45989600699523886\n",
      "Epoch: 3274 \t|| Train Loss: 0.1966650144348689 \t|| Test Loss: 0.4597858069979318\n",
      "Epoch: 3275 \t|| Train Loss: 0.19663011443665251 \t|| Test Loss: 0.4596756070006249\n",
      "Epoch: 3276 \t|| Train Loss: 0.196596319751628 \t|| Test Loss: 0.4596340970029346\n",
      "Epoch: 3277 \t|| Train Loss: 0.19656159443999782 \t|| Test Loss: 0.4595238970056276\n",
      "Epoch: 3278 \t|| Train Loss: 0.19652741875455665 \t|| Test Loss: 0.4594823870079373\n",
      "Epoch: 3279 \t|| Train Loss: 0.19649307444334313 \t|| Test Loss: 0.4593721870106304\n",
      "Epoch: 3280 \t|| Train Loss: 0.19645851775748535 \t|| Test Loss: 0.45933067701294006\n",
      "Epoch: 3281 \t|| Train Loss: 0.1964245544466884 \t|| Test Loss: 0.4592204770156331\n",
      "Epoch: 3282 \t|| Train Loss: 0.19638965444847203 \t|| Test Loss: 0.45911027701832613\n",
      "Epoch: 3283 \t|| Train Loss: 0.19635599676197565 \t|| Test Loss: 0.45906876702063587\n",
      "Epoch: 3284 \t|| Train Loss: 0.19632113445181731 \t|| Test Loss: 0.4589585670233289\n",
      "Epoch: 3285 \t|| Train Loss: 0.19628709576490433 \t|| Test Loss: 0.4589170570256386\n",
      "Epoch: 3286 \t|| Train Loss: 0.19625261445516257 \t|| Test Loss: 0.4588068570283316\n",
      "Epoch: 3287 \t|| Train Loss: 0.19621819476783303 \t|| Test Loss: 0.45876534703064137\n",
      "Epoch: 3288 \t|| Train Loss: 0.19618409445850787 \t|| Test Loss: 0.4586551470333344\n",
      "Epoch: 3289 \t|| Train Loss: 0.1961492937707617 \t|| Test Loss: 0.45861363703564406\n",
      "Epoch: 3290 \t|| Train Loss: 0.19611557446185315 \t|| Test Loss: 0.4585034370383371\n",
      "Epoch: 3291 \t|| Train Loss: 0.19608067446363683 \t|| Test Loss: 0.45839323704103013\n",
      "Epoch: 3292 \t|| Train Loss: 0.196046772775252 \t|| Test Loss: 0.4583517270433399\n",
      "Epoch: 3293 \t|| Train Loss: 0.19601215446698209 \t|| Test Loss: 0.4582415270460329\n",
      "Epoch: 3294 \t|| Train Loss: 0.19597787177818066 \t|| Test Loss: 0.4582000170483426\n",
      "Epoch: 3295 \t|| Train Loss: 0.19594363447032737 \t|| Test Loss: 0.4580898170510356\n",
      "Epoch: 3296 \t|| Train Loss: 0.19590897078110933 \t|| Test Loss: 0.4580483070533453\n",
      "Epoch: 3297 \t|| Train Loss: 0.19587511447367262 \t|| Test Loss: 0.4579381070560383\n",
      "Epoch: 3298 \t|| Train Loss: 0.19584021447545633 \t|| Test Loss: 0.4578279070587314\n",
      "Epoch: 3299 \t|| Train Loss: 0.19580644978559963 \t|| Test Loss: 0.4577863970610411\n",
      "Epoch: 3300 \t|| Train Loss: 0.19577169447880158 \t|| Test Loss: 0.45767619706373414\n",
      "Epoch: 3301 \t|| Train Loss: 0.1957375487885283 \t|| Test Loss: 0.4576346870660439\n",
      "Epoch: 3302 \t|| Train Loss: 0.19570317448214686 \t|| Test Loss: 0.45752448706873683\n",
      "Epoch: 3303 \t|| Train Loss: 0.195668647791457 \t|| Test Loss: 0.45748297707104657\n",
      "Epoch: 3304 \t|| Train Loss: 0.1956346544854921 \t|| Test Loss: 0.4573727770737396\n",
      "Epoch: 3305 \t|| Train Loss: 0.19559975448727582 \t|| Test Loss: 0.45726257707643264\n",
      "Epoch: 3306 \t|| Train Loss: 0.1955661267959473 \t|| Test Loss: 0.4572210670787424\n",
      "Epoch: 3307 \t|| Train Loss: 0.19553123449062104 \t|| Test Loss: 0.4571108670814354\n",
      "Epoch: 3308 \t|| Train Loss: 0.195497225798876 \t|| Test Loss: 0.45706935708374513\n",
      "Epoch: 3309 \t|| Train Loss: 0.19546271449396635 \t|| Test Loss: 0.45695915708643814\n",
      "Epoch: 3310 \t|| Train Loss: 0.19542832480180466 \t|| Test Loss: 0.4569176470887479\n",
      "Epoch: 3311 \t|| Train Loss: 0.1953941944973116 \t|| Test Loss: 0.4568074470914409\n",
      "Epoch: 3312 \t|| Train Loss: 0.19535942380473334 \t|| Test Loss: 0.45676593709375063\n",
      "Epoch: 3313 \t|| Train Loss: 0.1953256745006569 \t|| Test Loss: 0.45665573709644364\n",
      "Epoch: 3314 \t|| Train Loss: 0.19529077450244053 \t|| Test Loss: 0.45654553709913664\n",
      "Epoch: 3315 \t|| Train Loss: 0.19525690280922361 \t|| Test Loss: 0.4565040271014464\n",
      "Epoch: 3316 \t|| Train Loss: 0.19522225450578584 \t|| Test Loss: 0.4563938271041394\n",
      "Epoch: 3317 \t|| Train Loss: 0.1951880018121523 \t|| Test Loss: 0.45635231710644913\n",
      "Epoch: 3318 \t|| Train Loss: 0.19515373450913112 \t|| Test Loss: 0.4562421171091421\n",
      "Epoch: 3319 \t|| Train Loss: 0.195119100815081 \t|| Test Loss: 0.4562006071114519\n",
      "Epoch: 3320 \t|| Train Loss: 0.19508521451247635 \t|| Test Loss: 0.4560904071141449\n",
      "Epoch: 3321 \t|| Train Loss: 0.19505031451426005 \t|| Test Loss: 0.4559802071168379\n",
      "Epoch: 3322 \t|| Train Loss: 0.1950165798195713 \t|| Test Loss: 0.4559386971191476\n",
      "Epoch: 3323 \t|| Train Loss: 0.19498179451760528 \t|| Test Loss: 0.45582849712184065\n",
      "Epoch: 3324 \t|| Train Loss: 0.19494767882249997 \t|| Test Loss: 0.45578698712415033\n",
      "Epoch: 3325 \t|| Train Loss: 0.19491327452095059 \t|| Test Loss: 0.45567678712684334\n",
      "Epoch: 3326 \t|| Train Loss: 0.19487877782542867 \t|| Test Loss: 0.4556352771291531\n",
      "Epoch: 3327 \t|| Train Loss: 0.19484475452429587 \t|| Test Loss: 0.4555250771318461\n",
      "Epoch: 3328 \t|| Train Loss: 0.19480987682835735 \t|| Test Loss: 0.4554835671341559\n",
      "Epoch: 3329 \t|| Train Loss: 0.19477623452764115 \t|| Test Loss: 0.4553733671368489\n",
      "Epoch: 3330 \t|| Train Loss: 0.1947413345294248 \t|| Test Loss: 0.45526316713954185\n",
      "Epoch: 3331 \t|| Train Loss: 0.19470735583284765 \t|| Test Loss: 0.45522165714185164\n",
      "Epoch: 3332 \t|| Train Loss: 0.1946728145327701 \t|| Test Loss: 0.4551114571445446\n",
      "Epoch: 3333 \t|| Train Loss: 0.1946384548357763 \t|| Test Loss: 0.4550699471468544\n",
      "Epoch: 3334 \t|| Train Loss: 0.19460429453611536 \t|| Test Loss: 0.45495974714954734\n",
      "Epoch: 3335 \t|| Train Loss: 0.19456955383870497 \t|| Test Loss: 0.45491823715185714\n",
      "Epoch: 3336 \t|| Train Loss: 0.19453577453946064 \t|| Test Loss: 0.4548080371545501\n",
      "Epoch: 3337 \t|| Train Loss: 0.1945008745412443 \t|| Test Loss: 0.45469783715724316\n",
      "Epoch: 3338 \t|| Train Loss: 0.1944670328431953 \t|| Test Loss: 0.4546563271595529\n",
      "Epoch: 3339 \t|| Train Loss: 0.19443235454458957 \t|| Test Loss: 0.4545461271622459\n",
      "Epoch: 3340 \t|| Train Loss: 0.19439813184612395 \t|| Test Loss: 0.45450461716455565\n",
      "Epoch: 3341 \t|| Train Loss: 0.19436383454793482 \t|| Test Loss: 0.45439441716724865\n",
      "Epoch: 3342 \t|| Train Loss: 0.19432923084905268 \t|| Test Loss: 0.4543529071695584\n",
      "Epoch: 3343 \t|| Train Loss: 0.1942953145512801 \t|| Test Loss: 0.4542427071722514\n",
      "Epoch: 3344 \t|| Train Loss: 0.19426041455306375 \t|| Test Loss: 0.4541325071749444\n",
      "Epoch: 3345 \t|| Train Loss: 0.19422670985354293 \t|| Test Loss: 0.45409099717725415\n",
      "Epoch: 3346 \t|| Train Loss: 0.19419189455640906 \t|| Test Loss: 0.45398079717994716\n",
      "Epoch: 3347 \t|| Train Loss: 0.19415780885647163 \t|| Test Loss: 0.4539392871822569\n",
      "Epoch: 3348 \t|| Train Loss: 0.1941233745597543 \t|| Test Loss: 0.45382908718494985\n",
      "Epoch: 3349 \t|| Train Loss: 0.1940889078594003 \t|| Test Loss: 0.4537875771872596\n",
      "Epoch: 3350 \t|| Train Loss: 0.19405485456309962 \t|| Test Loss: 0.4536773771899526\n",
      "Epoch: 3351 \t|| Train Loss: 0.19402000686232898 \t|| Test Loss: 0.4536358671922624\n",
      "Epoch: 3352 \t|| Train Loss: 0.19398633456644485 \t|| Test Loss: 0.4535256671949554\n",
      "Epoch: 3353 \t|| Train Loss: 0.19395143456822858 \t|| Test Loss: 0.4534154671976484\n",
      "Epoch: 3354 \t|| Train Loss: 0.19391748586681926 \t|| Test Loss: 0.45337395719995816\n",
      "Epoch: 3355 \t|| Train Loss: 0.19388291457157383 \t|| Test Loss: 0.4532637572026511\n",
      "Epoch: 3356 \t|| Train Loss: 0.19384858486974793 \t|| Test Loss: 0.4532222472049609\n",
      "Epoch: 3357 \t|| Train Loss: 0.1938143945749191 \t|| Test Loss: 0.45311204720765386\n",
      "Epoch: 3358 \t|| Train Loss: 0.19377968387267663 \t|| Test Loss: 0.4530705372099636\n",
      "Epoch: 3359 \t|| Train Loss: 0.19374587457826437 \t|| Test Loss: 0.4529603372126566\n",
      "Epoch: 3360 \t|| Train Loss: 0.19371097458004805 \t|| Test Loss: 0.45285013721534967\n",
      "Epoch: 3361 \t|| Train Loss: 0.1936771628771669 \t|| Test Loss: 0.45280862721765935\n",
      "Epoch: 3362 \t|| Train Loss: 0.19364245458339333 \t|| Test Loss: 0.4526984272203524\n",
      "Epoch: 3363 \t|| Train Loss: 0.19360826188009564 \t|| Test Loss: 0.4526569172226621\n",
      "Epoch: 3364 \t|| Train Loss: 0.1935739345867386 \t|| Test Loss: 0.45254671722535517\n",
      "Epoch: 3365 \t|| Train Loss: 0.1935393608830243 \t|| Test Loss: 0.45250520722766485\n",
      "Epoch: 3366 \t|| Train Loss: 0.19350541459008386 \t|| Test Loss: 0.4523950072303579\n",
      "Epoch: 3367 \t|| Train Loss: 0.19347051459186754 \t|| Test Loss: 0.4522848072330509\n",
      "Epoch: 3368 \t|| Train Loss: 0.1934368398875146 \t|| Test Loss: 0.4522432972353606\n",
      "Epoch: 3369 \t|| Train Loss: 0.19340199459521284 \t|| Test Loss: 0.4521330972380536\n",
      "Epoch: 3370 \t|| Train Loss: 0.1933679388904433 \t|| Test Loss: 0.4520915872403634\n",
      "Epoch: 3371 \t|| Train Loss: 0.1933334745985581 \t|| Test Loss: 0.45198138724305637\n",
      "Epoch: 3372 \t|| Train Loss: 0.19329903789337194 \t|| Test Loss: 0.45193987724536616\n",
      "Epoch: 3373 \t|| Train Loss: 0.19326495460190335 \t|| Test Loss: 0.45182967724805917\n",
      "Epoch: 3374 \t|| Train Loss: 0.19323013689630064 \t|| Test Loss: 0.4517881672503689\n",
      "Epoch: 3375 \t|| Train Loss: 0.19319643460524863 \t|| Test Loss: 0.4516779672530619\n",
      "Epoch: 3376 \t|| Train Loss: 0.19316153460703228 \t|| Test Loss: 0.45156776725575487\n",
      "Epoch: 3377 \t|| Train Loss: 0.19312761590079092 \t|| Test Loss: 0.4515262572580646\n",
      "Epoch: 3378 \t|| Train Loss: 0.1930930146103776 \t|| Test Loss: 0.4514160572607576\n",
      "Epoch: 3379 \t|| Train Loss: 0.19305871490371965 \t|| Test Loss: 0.4513745472630674\n",
      "Epoch: 3380 \t|| Train Loss: 0.1930244946137228 \t|| Test Loss: 0.4512643472657604\n",
      "Epoch: 3381 \t|| Train Loss: 0.1929898139066483 \t|| Test Loss: 0.4512228372680701\n",
      "Epoch: 3382 \t|| Train Loss: 0.19295597461706812 \t|| Test Loss: 0.4511126372707631\n",
      "Epoch: 3383 \t|| Train Loss: 0.1929210746188518 \t|| Test Loss: 0.4510024372734561\n",
      "Epoch: 3384 \t|| Train Loss: 0.19288729291113857 \t|| Test Loss: 0.4509609272757659\n",
      "Epoch: 3385 \t|| Train Loss: 0.19285255462219708 \t|| Test Loss: 0.45085072727845893\n",
      "Epoch: 3386 \t|| Train Loss: 0.19281839191406724 \t|| Test Loss: 0.4508092172807686\n",
      "Epoch: 3387 \t|| Train Loss: 0.19278403462554233 \t|| Test Loss: 0.4506990172834616\n",
      "Epoch: 3388 \t|| Train Loss: 0.19274949091699595 \t|| Test Loss: 0.4506575072857714\n",
      "Epoch: 3389 \t|| Train Loss: 0.19271551462888764 \t|| Test Loss: 0.45054730728846437\n",
      "Epoch: 3390 \t|| Train Loss: 0.19268061463067127 \t|| Test Loss: 0.45043710729115743\n",
      "Epoch: 3391 \t|| Train Loss: 0.19264696992148622 \t|| Test Loss: 0.4503955972934672\n",
      "Epoch: 3392 \t|| Train Loss: 0.19261209463401657 \t|| Test Loss: 0.45028539729616013\n",
      "Epoch: 3393 \t|| Train Loss: 0.1925780689244149 \t|| Test Loss: 0.4502438872984699\n",
      "Epoch: 3394 \t|| Train Loss: 0.19254357463736185 \t|| Test Loss: 0.45013368730116293\n",
      "Epoch: 3395 \t|| Train Loss: 0.19250916792734357 \t|| Test Loss: 0.4500921773034727\n",
      "Epoch: 3396 \t|| Train Loss: 0.1924750546407071 \t|| Test Loss: 0.4499819773061656\n",
      "Epoch: 3397 \t|| Train Loss: 0.19244026693027227 \t|| Test Loss: 0.4499404673084754\n",
      "Epoch: 3398 \t|| Train Loss: 0.19240653464405238 \t|| Test Loss: 0.4498302673111684\n",
      "Epoch: 3399 \t|| Train Loss: 0.19237163464583606 \t|| Test Loss: 0.4497200673138614\n",
      "Epoch: 3400 \t|| Train Loss: 0.19233774593476258 \t|| Test Loss: 0.4496785573161712\n",
      "Epoch: 3401 \t|| Train Loss: 0.1923031146491813 \t|| Test Loss: 0.4495683573188642\n",
      "Epoch: 3402 \t|| Train Loss: 0.19226884493769125 \t|| Test Loss: 0.4495268473211739\n",
      "Epoch: 3403 \t|| Train Loss: 0.19223459465252662 \t|| Test Loss: 0.4494166473238669\n",
      "Epoch: 3404 \t|| Train Loss: 0.19219994394061993 \t|| Test Loss: 0.4493751373261766\n",
      "Epoch: 3405 \t|| Train Loss: 0.1921660746558719 \t|| Test Loss: 0.44926493732886963\n",
      "Epoch: 3406 \t|| Train Loss: 0.19213117465765556 \t|| Test Loss: 0.44915473733156264\n",
      "Epoch: 3407 \t|| Train Loss: 0.19209742294511023 \t|| Test Loss: 0.44911322733387243\n",
      "Epoch: 3408 \t|| Train Loss: 0.1920626546610008 \t|| Test Loss: 0.4490030273365654\n",
      "Epoch: 3409 \t|| Train Loss: 0.1920285219480389 \t|| Test Loss: 0.4489615173388751\n",
      "Epoch: 3410 \t|| Train Loss: 0.19199413466434606 \t|| Test Loss: 0.44885131734156813\n",
      "Epoch: 3411 \t|| Train Loss: 0.19195962095096758 \t|| Test Loss: 0.44880980734387793\n",
      "Epoch: 3412 \t|| Train Loss: 0.19192561466769137 \t|| Test Loss: 0.4486996073465709\n",
      "Epoch: 3413 \t|| Train Loss: 0.19189071995389625 \t|| Test Loss: 0.4486580973488806\n",
      "Epoch: 3414 \t|| Train Loss: 0.19185709467103665 \t|| Test Loss: 0.44854789735157363\n",
      "Epoch: 3415 \t|| Train Loss: 0.1918221946728203 \t|| Test Loss: 0.44843769735426664\n",
      "Epoch: 3416 \t|| Train Loss: 0.19178819895838656 \t|| Test Loss: 0.44839618735657644\n",
      "Epoch: 3417 \t|| Train Loss: 0.19175367467616555 \t|| Test Loss: 0.44828598735926944\n",
      "Epoch: 3418 \t|| Train Loss: 0.19171929796131526 \t|| Test Loss: 0.44824447736157913\n",
      "Epoch: 3419 \t|| Train Loss: 0.19168515467951086 \t|| Test Loss: 0.44813427736427214\n",
      "Epoch: 3420 \t|| Train Loss: 0.19165039696424394 \t|| Test Loss: 0.44809276736658193\n",
      "Epoch: 3421 \t|| Train Loss: 0.19161663468285614 \t|| Test Loss: 0.4479825673692749\n",
      "Epoch: 3422 \t|| Train Loss: 0.1915817346846398 \t|| Test Loss: 0.44787236737196795\n",
      "Epoch: 3423 \t|| Train Loss: 0.19154787596873418 \t|| Test Loss: 0.44783085737427764\n",
      "Epoch: 3424 \t|| Train Loss: 0.19151321468798507 \t|| Test Loss: 0.44772065737697064\n",
      "Epoch: 3425 \t|| Train Loss: 0.1914789749716629 \t|| Test Loss: 0.4476791473792804\n",
      "Epoch: 3426 \t|| Train Loss: 0.19144469469133035 \t|| Test Loss: 0.4475689473819734\n",
      "Epoch: 3427 \t|| Train Loss: 0.19141007397459156 \t|| Test Loss: 0.44752743738428313\n",
      "Epoch: 3428 \t|| Train Loss: 0.19137617469467563 \t|| Test Loss: 0.44741723738697614\n",
      "Epoch: 3429 \t|| Train Loss: 0.19134127469645928 \t|| Test Loss: 0.44730703738966915\n",
      "Epoch: 3430 \t|| Train Loss: 0.19130755297908184 \t|| Test Loss: 0.44726552739197895\n",
      "Epoch: 3431 \t|| Train Loss: 0.19127275469980456 \t|| Test Loss: 0.4471553273946719\n",
      "Epoch: 3432 \t|| Train Loss: 0.19123865198201054 \t|| Test Loss: 0.44711381739698164\n",
      "Epoch: 3433 \t|| Train Loss: 0.19120423470314982 \t|| Test Loss: 0.44700361739967465\n",
      "Epoch: 3434 \t|| Train Loss: 0.1911697509849392 \t|| Test Loss: 0.4469621074019844\n",
      "Epoch: 3435 \t|| Train Loss: 0.1911357147064951 \t|| Test Loss: 0.4468519074046774\n",
      "Epoch: 3436 \t|| Train Loss: 0.1911008499878679 \t|| Test Loss: 0.44681039740698714\n",
      "Epoch: 3437 \t|| Train Loss: 0.19106719470984038 \t|| Test Loss: 0.44670019740968014\n",
      "Epoch: 3438 \t|| Train Loss: 0.19103229471162406 \t|| Test Loss: 0.4465899974123732\n",
      "Epoch: 3439 \t|| Train Loss: 0.1909983289923582 \t|| Test Loss: 0.4465484874146829\n",
      "Epoch: 3440 \t|| Train Loss: 0.19096377471496934 \t|| Test Loss: 0.44643828741737596\n",
      "Epoch: 3441 \t|| Train Loss: 0.1909294279952869 \t|| Test Loss: 0.44639677741968564\n",
      "Epoch: 3442 \t|| Train Loss: 0.19089525471831462 \t|| Test Loss: 0.44628657742237865\n",
      "Epoch: 3443 \t|| Train Loss: 0.19086052699821557 \t|| Test Loss: 0.4462450674246884\n",
      "Epoch: 3444 \t|| Train Loss: 0.19082673472165987 \t|| Test Loss: 0.44613486742738145\n",
      "Epoch: 3445 \t|| Train Loss: 0.19079183472344352 \t|| Test Loss: 0.4460246674300744\n",
      "Epoch: 3446 \t|| Train Loss: 0.19075800600270584 \t|| Test Loss: 0.44598315743238415\n",
      "Epoch: 3447 \t|| Train Loss: 0.19072331472678883 \t|| Test Loss: 0.4458729574350772\n",
      "Epoch: 3448 \t|| Train Loss: 0.19068910500563455 \t|| Test Loss: 0.4458314474373869\n",
      "Epoch: 3449 \t|| Train Loss: 0.1906547947301341 \t|| Test Loss: 0.4457212474400799\n",
      "Epoch: 3450 \t|| Train Loss: 0.19062020400856322 \t|| Test Loss: 0.44567973744238965\n",
      "Epoch: 3451 \t|| Train Loss: 0.19058627473347936 \t|| Test Loss: 0.4455695374450827\n",
      "Epoch: 3452 \t|| Train Loss: 0.19055137473526304 \t|| Test Loss: 0.44545933744777566\n",
      "Epoch: 3453 \t|| Train Loss: 0.19051768301305352 \t|| Test Loss: 0.4454178274500854\n",
      "Epoch: 3454 \t|| Train Loss: 0.19048285473860832 \t|| Test Loss: 0.4453076274527784\n",
      "Epoch: 3455 \t|| Train Loss: 0.19044878201598217 \t|| Test Loss: 0.44526611745508815\n",
      "Epoch: 3456 \t|| Train Loss: 0.19041433474195357 \t|| Test Loss: 0.44515591745778116\n",
      "Epoch: 3457 \t|| Train Loss: 0.19037988101891085 \t|| Test Loss: 0.4451144074600909\n",
      "Epoch: 3458 \t|| Train Loss: 0.19034581474529885 \t|| Test Loss: 0.4450042074627839\n",
      "Epoch: 3459 \t|| Train Loss: 0.19031098002183952 \t|| Test Loss: 0.44496269746509365\n",
      "Epoch: 3460 \t|| Train Loss: 0.19027729474864413 \t|| Test Loss: 0.44485249746778666\n",
      "Epoch: 3461 \t|| Train Loss: 0.19024239475042778 \t|| Test Loss: 0.44474229747047966\n",
      "Epoch: 3462 \t|| Train Loss: 0.19020845902632982 \t|| Test Loss: 0.4447007874727894\n",
      "Epoch: 3463 \t|| Train Loss: 0.19017387475377312 \t|| Test Loss: 0.4445905874754824\n",
      "Epoch: 3464 \t|| Train Loss: 0.1901395580292585 \t|| Test Loss: 0.44454907747779215\n",
      "Epoch: 3465 \t|| Train Loss: 0.19010535475711837 \t|| Test Loss: 0.4444388774804852\n",
      "Epoch: 3466 \t|| Train Loss: 0.19007065703218723 \t|| Test Loss: 0.4443973674827949\n",
      "Epoch: 3467 \t|| Train Loss: 0.19003683476046365 \t|| Test Loss: 0.44428716748548797\n",
      "Epoch: 3468 \t|| Train Loss: 0.1900019347622473 \t|| Test Loss: 0.4441769674881809\n",
      "Epoch: 3469 \t|| Train Loss: 0.18996813603667745 \t|| Test Loss: 0.4441354574904907\n",
      "Epoch: 3470 \t|| Train Loss: 0.18993341476559258 \t|| Test Loss: 0.4440252574931837\n",
      "Epoch: 3471 \t|| Train Loss: 0.18989923503960618 \t|| Test Loss: 0.44398374749549346\n",
      "Epoch: 3472 \t|| Train Loss: 0.18986489476893784 \t|| Test Loss: 0.44387354749818647\n",
      "Epoch: 3473 \t|| Train Loss: 0.18983033404253485 \t|| Test Loss: 0.44383203750049616\n",
      "Epoch: 3474 \t|| Train Loss: 0.18979637477228312 \t|| Test Loss: 0.44372183750318916\n",
      "Epoch: 3475 \t|| Train Loss: 0.1897614747740668 \t|| Test Loss: 0.4436116375058822\n",
      "Epoch: 3476 \t|| Train Loss: 0.18972781304702516 \t|| Test Loss: 0.4435701275081919\n",
      "Epoch: 3477 \t|| Train Loss: 0.18969295477741205 \t|| Test Loss: 0.443459927510885\n",
      "Epoch: 3478 \t|| Train Loss: 0.18965891204995383 \t|| Test Loss: 0.44341841751319466\n",
      "Epoch: 3479 \t|| Train Loss: 0.18962443478075733 \t|| Test Loss: 0.44330821751588767\n",
      "Epoch: 3480 \t|| Train Loss: 0.18959001105288248 \t|| Test Loss: 0.4432667075181974\n",
      "Epoch: 3481 \t|| Train Loss: 0.1895559147841026 \t|| Test Loss: 0.4431565075208905\n",
      "Epoch: 3482 \t|| Train Loss: 0.18952111005581118 \t|| Test Loss: 0.44311499752320016\n",
      "Epoch: 3483 \t|| Train Loss: 0.18948739478744786 \t|| Test Loss: 0.44300479752589317\n",
      "Epoch: 3484 \t|| Train Loss: 0.18945249478923157 \t|| Test Loss: 0.4428945975285862\n",
      "Epoch: 3485 \t|| Train Loss: 0.18941858906030148 \t|| Test Loss: 0.442853087530896\n",
      "Epoch: 3486 \t|| Train Loss: 0.18938397479257682 \t|| Test Loss: 0.4427428875335889\n",
      "Epoch: 3487 \t|| Train Loss: 0.18934968806323016 \t|| Test Loss: 0.44270137753589867\n",
      "Epoch: 3488 \t|| Train Loss: 0.1893154547959221 \t|| Test Loss: 0.4425911775385917\n",
      "Epoch: 3489 \t|| Train Loss: 0.18928078706615886 \t|| Test Loss: 0.4425496675409014\n",
      "Epoch: 3490 \t|| Train Loss: 0.18924693479926735 \t|| Test Loss: 0.4424394675435944\n",
      "Epoch: 3491 \t|| Train Loss: 0.18921203480105103 \t|| Test Loss: 0.44232926754628743\n",
      "Epoch: 3492 \t|| Train Loss: 0.18917826607064914 \t|| Test Loss: 0.44228775754859717\n",
      "Epoch: 3493 \t|| Train Loss: 0.1891435148043963 \t|| Test Loss: 0.4421775575512902\n",
      "Epoch: 3494 \t|| Train Loss: 0.18910936507357784 \t|| Test Loss: 0.4421360475535999\n",
      "Epoch: 3495 \t|| Train Loss: 0.18907499480774156 \t|| Test Loss: 0.44202584755629293\n",
      "Epoch: 3496 \t|| Train Loss: 0.18904046407650651 \t|| Test Loss: 0.4419843375586027\n",
      "Epoch: 3497 \t|| Train Loss: 0.18900647481108684 \t|| Test Loss: 0.4418741375612957\n",
      "Epoch: 3498 \t|| Train Loss: 0.18897157481287055 \t|| Test Loss: 0.4417639375639887\n",
      "Epoch: 3499 \t|| Train Loss: 0.18893794308099682 \t|| Test Loss: 0.4417224275662984\n",
      "Epoch: 3500 \t|| Train Loss: 0.18890305481621578 \t|| Test Loss: 0.4416122275689915\n",
      "Epoch: 3501 \t|| Train Loss: 0.1888690420839255 \t|| Test Loss: 0.4415707175713012\n",
      "Epoch: 3502 \t|| Train Loss: 0.18883453481956106 \t|| Test Loss: 0.4414605175739942\n",
      "Epoch: 3503 \t|| Train Loss: 0.18880014108685414 \t|| Test Loss: 0.4414190075763039\n",
      "Epoch: 3504 \t|| Train Loss: 0.1887660148229063 \t|| Test Loss: 0.441308807578997\n",
      "Epoch: 3505 \t|| Train Loss: 0.18873124008978287 \t|| Test Loss: 0.4412672975813067\n",
      "Epoch: 3506 \t|| Train Loss: 0.18869749482625162 \t|| Test Loss: 0.4411570975839997\n",
      "Epoch: 3507 \t|| Train Loss: 0.1886625948280353 \t|| Test Loss: 0.4410468975866927\n",
      "Epoch: 3508 \t|| Train Loss: 0.18862871909427312 \t|| Test Loss: 0.4410053875890023\n",
      "Epoch: 3509 \t|| Train Loss: 0.18859407483138052 \t|| Test Loss: 0.44089518759169544\n",
      "Epoch: 3510 \t|| Train Loss: 0.18855981809720182 \t|| Test Loss: 0.4408536775940052\n",
      "Epoch: 3511 \t|| Train Loss: 0.18852555483472583 \t|| Test Loss: 0.4407434775966982\n",
      "Epoch: 3512 \t|| Train Loss: 0.1884909171001305 \t|| Test Loss: 0.4407019675990079\n",
      "Epoch: 3513 \t|| Train Loss: 0.1884570348380711 \t|| Test Loss: 0.44059176760170093\n",
      "Epoch: 3514 \t|| Train Loss: 0.18842213483985476 \t|| Test Loss: 0.44048156760439394\n",
      "Epoch: 3515 \t|| Train Loss: 0.1883883961046208 \t|| Test Loss: 0.4404400576067037\n",
      "Epoch: 3516 \t|| Train Loss: 0.18835361484320007 \t|| Test Loss: 0.4403298576093967\n",
      "Epoch: 3517 \t|| Train Loss: 0.18831949510754947 \t|| Test Loss: 0.44028834761170643\n",
      "Epoch: 3518 \t|| Train Loss: 0.18828509484654532 \t|| Test Loss: 0.44017814761439944\n",
      "Epoch: 3519 \t|| Train Loss: 0.18825059411047818 \t|| Test Loss: 0.4401366376167092\n",
      "Epoch: 3520 \t|| Train Loss: 0.1882165748498906 \t|| Test Loss: 0.4400264376194022\n",
      "Epoch: 3521 \t|| Train Loss: 0.18818169311340682 \t|| Test Loss: 0.43998492762171193\n",
      "Epoch: 3522 \t|| Train Loss: 0.18814805485323588 \t|| Test Loss: 0.43987472762440494\n",
      "Epoch: 3523 \t|| Train Loss: 0.18811315485501953 \t|| Test Loss: 0.43976452762709795\n",
      "Epoch: 3524 \t|| Train Loss: 0.1880791721178971 \t|| Test Loss: 0.4397230176294077\n",
      "Epoch: 3525 \t|| Train Loss: 0.1880446348583648 \t|| Test Loss: 0.4396128176321007\n",
      "Epoch: 3526 \t|| Train Loss: 0.1880102711208258 \t|| Test Loss: 0.43957130763441044\n",
      "Epoch: 3527 \t|| Train Loss: 0.1879761148617101 \t|| Test Loss: 0.4394611076371035\n",
      "Epoch: 3528 \t|| Train Loss: 0.1879413701237545 \t|| Test Loss: 0.43941959763941324\n",
      "Epoch: 3529 \t|| Train Loss: 0.1879075948650554 \t|| Test Loss: 0.4393093976421062\n",
      "Epoch: 3530 \t|| Train Loss: 0.18787269486683905 \t|| Test Loss: 0.4391991976447992\n",
      "Epoch: 3531 \t|| Train Loss: 0.18783884912824478 \t|| Test Loss: 0.43915768764710894\n",
      "Epoch: 3532 \t|| Train Loss: 0.18780417487018433 \t|| Test Loss: 0.43904748764980195\n",
      "Epoch: 3533 \t|| Train Loss: 0.18776994813117345 \t|| Test Loss: 0.4390059776521117\n",
      "Epoch: 3534 \t|| Train Loss: 0.18773565487352956 \t|| Test Loss: 0.4388957776548047\n",
      "Epoch: 3535 \t|| Train Loss: 0.18770104713410213 \t|| Test Loss: 0.43885426765711444\n",
      "Epoch: 3536 \t|| Train Loss: 0.18766713487687486 \t|| Test Loss: 0.43874406765980745\n",
      "Epoch: 3537 \t|| Train Loss: 0.18763223487865854 \t|| Test Loss: 0.43863386766250045\n",
      "Epoch: 3538 \t|| Train Loss: 0.18759852613859243 \t|| Test Loss: 0.4385923576648102\n",
      "Epoch: 3539 \t|| Train Loss: 0.18756371488200382 \t|| Test Loss: 0.4384821576675032\n",
      "Epoch: 3540 \t|| Train Loss: 0.18752962514152113 \t|| Test Loss: 0.43844064766981294\n",
      "Epoch: 3541 \t|| Train Loss: 0.1874951948853491 \t|| Test Loss: 0.438330447672506\n",
      "Epoch: 3542 \t|| Train Loss: 0.18746072414444978 \t|| Test Loss: 0.4382889376748157\n",
      "Epoch: 3543 \t|| Train Loss: 0.18742667488869436 \t|| Test Loss: 0.4381787376775087\n",
      "Epoch: 3544 \t|| Train Loss: 0.18739182314737848 \t|| Test Loss: 0.43813722767981844\n",
      "Epoch: 3545 \t|| Train Loss: 0.18735815489203964 \t|| Test Loss: 0.4380270276825115\n",
      "Epoch: 3546 \t|| Train Loss: 0.1873232548938233 \t|| Test Loss: 0.43791682768520446\n",
      "Epoch: 3547 \t|| Train Loss: 0.18728930215186876 \t|| Test Loss: 0.4378753176875142\n",
      "Epoch: 3548 \t|| Train Loss: 0.18725473489716857 \t|| Test Loss: 0.4377651176902072\n",
      "Epoch: 3549 \t|| Train Loss: 0.18722040115479746 \t|| Test Loss: 0.43772360769251695\n",
      "Epoch: 3550 \t|| Train Loss: 0.18718621490051385 \t|| Test Loss: 0.43761340769520995\n",
      "Epoch: 3551 \t|| Train Loss: 0.18715150015772614 \t|| Test Loss: 0.4375718976975197\n",
      "Epoch: 3552 \t|| Train Loss: 0.1871176949038591 \t|| Test Loss: 0.4374616977002127\n",
      "Epoch: 3553 \t|| Train Loss: 0.18708279490564278 \t|| Test Loss: 0.4373514977029057\n",
      "Epoch: 3554 \t|| Train Loss: 0.18704897916221644 \t|| Test Loss: 0.43730998770521545\n",
      "Epoch: 3555 \t|| Train Loss: 0.18701427490898803 \t|| Test Loss: 0.43719978770790846\n",
      "Epoch: 3556 \t|| Train Loss: 0.1869800781651451 \t|| Test Loss: 0.4371582777102182\n",
      "Epoch: 3557 \t|| Train Loss: 0.1869457549123333 \t|| Test Loss: 0.4370480777129112\n",
      "Epoch: 3558 \t|| Train Loss: 0.18691117716807376 \t|| Test Loss: 0.43700656771522095\n",
      "Epoch: 3559 \t|| Train Loss: 0.18687723491567862 \t|| Test Loss: 0.43689636771791396\n",
      "Epoch: 3560 \t|| Train Loss: 0.18684233491746227 \t|| Test Loss: 0.43678616772060697\n",
      "Epoch: 3561 \t|| Train Loss: 0.1868086561725641 \t|| Test Loss: 0.4367446577229167\n",
      "Epoch: 3562 \t|| Train Loss: 0.18677381492080752 \t|| Test Loss: 0.4366344577256097\n",
      "Epoch: 3563 \t|| Train Loss: 0.18673975517549277 \t|| Test Loss: 0.43659294772791946\n",
      "Epoch: 3564 \t|| Train Loss: 0.18670529492415283 \t|| Test Loss: 0.43648274773061246\n",
      "Epoch: 3565 \t|| Train Loss: 0.18667085417842144 \t|| Test Loss: 0.4364412377329222\n",
      "Epoch: 3566 \t|| Train Loss: 0.1866367749274981 \t|| Test Loss: 0.4363310377356152\n",
      "Epoch: 3567 \t|| Train Loss: 0.18660195318135014 \t|| Test Loss: 0.436289527737925\n",
      "Epoch: 3568 \t|| Train Loss: 0.18656825493084336 \t|| Test Loss: 0.43617932774061796\n",
      "Epoch: 3569 \t|| Train Loss: 0.18653335493262704 \t|| Test Loss: 0.43606912774331097\n",
      "Epoch: 3570 \t|| Train Loss: 0.1864994321858404 \t|| Test Loss: 0.4360276177456207\n",
      "Epoch: 3571 \t|| Train Loss: 0.18646483493597232 \t|| Test Loss: 0.4359174177483137\n",
      "Epoch: 3572 \t|| Train Loss: 0.18643053118876912 \t|| Test Loss: 0.4358759077506235\n",
      "Epoch: 3573 \t|| Train Loss: 0.18639631493931758 \t|| Test Loss: 0.43576570775331647\n",
      "Epoch: 3574 \t|| Train Loss: 0.18636163019169777 \t|| Test Loss: 0.4357241977556262\n",
      "Epoch: 3575 \t|| Train Loss: 0.18632779494266286 \t|| Test Loss: 0.4356139977583192\n",
      "Epoch: 3576 \t|| Train Loss: 0.1862928949444465 \t|| Test Loss: 0.4355037977610122\n",
      "Epoch: 3577 \t|| Train Loss: 0.1862591091961881 \t|| Test Loss: 0.43546228776332196\n",
      "Epoch: 3578 \t|| Train Loss: 0.18622437494779182 \t|| Test Loss: 0.435352087766015\n",
      "Epoch: 3579 \t|| Train Loss: 0.18619020819911675 \t|| Test Loss: 0.4353105777683247\n",
      "Epoch: 3580 \t|| Train Loss: 0.18615585495113707 \t|| Test Loss: 0.4352003777710177\n",
      "Epoch: 3581 \t|| Train Loss: 0.18612130720204542 \t|| Test Loss: 0.43515886777332746\n",
      "Epoch: 3582 \t|| Train Loss: 0.18608733495448232 \t|| Test Loss: 0.43504866777602047\n",
      "Epoch: 3583 \t|| Train Loss: 0.186052434956266 \t|| Test Loss: 0.4349384677787135\n",
      "Epoch: 3584 \t|| Train Loss: 0.18601878620653572 \t|| Test Loss: 0.4348969577810232\n",
      "Epoch: 3585 \t|| Train Loss: 0.1859839149596113 \t|| Test Loss: 0.4347867577837162\n",
      "Epoch: 3586 \t|| Train Loss: 0.1859498852094644 \t|| Test Loss: 0.43474524778602597\n",
      "Epoch: 3587 \t|| Train Loss: 0.18591539496295656 \t|| Test Loss: 0.434635047788719\n",
      "Epoch: 3588 \t|| Train Loss: 0.1858809842123931 \t|| Test Loss: 0.4345935377910287\n",
      "Epoch: 3589 \t|| Train Loss: 0.18584687496630184 \t|| Test Loss: 0.4344833377937217\n",
      "Epoch: 3590 \t|| Train Loss: 0.18581208321532178 \t|| Test Loss: 0.43444182779603147\n",
      "Epoch: 3591 \t|| Train Loss: 0.1857783549696471 \t|| Test Loss: 0.4343316277987245\n",
      "Epoch: 3592 \t|| Train Loss: 0.1857434549714308 \t|| Test Loss: 0.4342214278014175\n",
      "Epoch: 3593 \t|| Train Loss: 0.18570956221981205 \t|| Test Loss: 0.4341799178037272\n",
      "Epoch: 3594 \t|| Train Loss: 0.18567493497477608 \t|| Test Loss: 0.43406971780642023\n",
      "Epoch: 3595 \t|| Train Loss: 0.18564066122274075 \t|| Test Loss: 0.43402820780872997\n",
      "Epoch: 3596 \t|| Train Loss: 0.1856064149781213 \t|| Test Loss: 0.433918007811423\n",
      "Epoch: 3597 \t|| Train Loss: 0.1855717602256694 \t|| Test Loss: 0.4338764978137327\n",
      "Epoch: 3598 \t|| Train Loss: 0.1855378949814666 \t|| Test Loss: 0.4337662978164257\n",
      "Epoch: 3599 \t|| Train Loss: 0.1855029949832503 \t|| Test Loss: 0.43365609781911874\n",
      "Epoch: 3600 \t|| Train Loss: 0.18546923923015973 \t|| Test Loss: 0.4336145878214285\n",
      "Epoch: 3601 \t|| Train Loss: 0.18543447498659554 \t|| Test Loss: 0.43350438782412154\n",
      "Epoch: 3602 \t|| Train Loss: 0.18540033823308838 \t|| Test Loss: 0.4334628778264312\n",
      "Epoch: 3603 \t|| Train Loss: 0.18536595498994082 \t|| Test Loss: 0.43335267782912423\n",
      "Epoch: 3604 \t|| Train Loss: 0.18533143723601708 \t|| Test Loss: 0.43331116783143403\n",
      "Epoch: 3605 \t|| Train Loss: 0.1852974349932861 \t|| Test Loss: 0.433200967834127\n",
      "Epoch: 3606 \t|| Train Loss: 0.18526253623894579 \t|| Test Loss: 0.4331594578364367\n",
      "Epoch: 3607 \t|| Train Loss: 0.18522891499663138 \t|| Test Loss: 0.43304925783912973\n",
      "Epoch: 3608 \t|| Train Loss: 0.18519401499841504 \t|| Test Loss: 0.43293905784182274\n",
      "Epoch: 3609 \t|| Train Loss: 0.18516001524343606 \t|| Test Loss: 0.43289754784413254\n",
      "Epoch: 3610 \t|| Train Loss: 0.18512549500176032 \t|| Test Loss: 0.43278734784682554\n",
      "Epoch: 3611 \t|| Train Loss: 0.1850911142463647 \t|| Test Loss: 0.43274583784913523\n",
      "Epoch: 3612 \t|| Train Loss: 0.1850569750051056 \t|| Test Loss: 0.43263563785182824\n",
      "Epoch: 3613 \t|| Train Loss: 0.1850222132492934 \t|| Test Loss: 0.432594127854138\n",
      "Epoch: 3614 \t|| Train Loss: 0.18498845500845085 \t|| Test Loss: 0.432483927856831\n",
      "Epoch: 3615 \t|| Train Loss: 0.18495355501023453 \t|| Test Loss: 0.432373727859524\n",
      "Epoch: 3616 \t|| Train Loss: 0.18491969225378374 \t|| Test Loss: 0.43233221786183373\n",
      "Epoch: 3617 \t|| Train Loss: 0.1848850350135798 \t|| Test Loss: 0.43222201786452674\n",
      "Epoch: 3618 \t|| Train Loss: 0.18485079125671236 \t|| Test Loss: 0.4321805078668365\n",
      "Epoch: 3619 \t|| Train Loss: 0.18481651501692506 \t|| Test Loss: 0.4320703078695295\n",
      "Epoch: 3620 \t|| Train Loss: 0.18478189025964104 \t|| Test Loss: 0.43202879787183923\n",
      "Epoch: 3621 \t|| Train Loss: 0.18474799502027034 \t|| Test Loss: 0.43191859787453224\n",
      "Epoch: 3622 \t|| Train Loss: 0.18471309502205405 \t|| Test Loss: 0.43180839787722525\n",
      "Epoch: 3623 \t|| Train Loss: 0.18467936926413137 \t|| Test Loss: 0.431766887879535\n",
      "Epoch: 3624 \t|| Train Loss: 0.18464457502539927 \t|| Test Loss: 0.431656687882228\n",
      "Epoch: 3625 \t|| Train Loss: 0.18461046826706004 \t|| Test Loss: 0.43161517788453774\n",
      "Epoch: 3626 \t|| Train Loss: 0.18457605502874458 \t|| Test Loss: 0.43150497788723075\n",
      "Epoch: 3627 \t|| Train Loss: 0.18454156726998874 \t|| Test Loss: 0.4314634678895405\n",
      "Epoch: 3628 \t|| Train Loss: 0.18450753503208986 \t|| Test Loss: 0.4313532678922335\n",
      "Epoch: 3629 \t|| Train Loss: 0.18447266627291742 \t|| Test Loss: 0.43131175789454324\n",
      "Epoch: 3630 \t|| Train Loss: 0.1844390150354351 \t|| Test Loss: 0.43120155789723624\n",
      "Epoch: 3631 \t|| Train Loss: 0.1844041150372188 \t|| Test Loss: 0.43109135789992925\n",
      "Epoch: 3632 \t|| Train Loss: 0.1843701452774077 \t|| Test Loss: 0.431049847902239\n",
      "Epoch: 3633 \t|| Train Loss: 0.18433559504056407 \t|| Test Loss: 0.4309396479049319\n",
      "Epoch: 3634 \t|| Train Loss: 0.18430124428033637 \t|| Test Loss: 0.43089813790724174\n",
      "Epoch: 3635 \t|| Train Loss: 0.18426707504390932 \t|| Test Loss: 0.43078793790993475\n",
      "Epoch: 3636 \t|| Train Loss: 0.18423234328326507 \t|| Test Loss: 0.4307464279122445\n",
      "Epoch: 3637 \t|| Train Loss: 0.18419855504725463 \t|| Test Loss: 0.4306362279149375\n",
      "Epoch: 3638 \t|| Train Loss: 0.18416365504903828 \t|| Test Loss: 0.4305260279176304\n",
      "Epoch: 3639 \t|| Train Loss: 0.18412982228775535 \t|| Test Loss: 0.43048451791994025\n",
      "Epoch: 3640 \t|| Train Loss: 0.18409513505238356 \t|| Test Loss: 0.43037431792263325\n",
      "Epoch: 3641 \t|| Train Loss: 0.18406092129068402 \t|| Test Loss: 0.430332807924943\n",
      "Epoch: 3642 \t|| Train Loss: 0.18402661505572884 \t|| Test Loss: 0.430222607927636\n",
      "Epoch: 3643 \t|| Train Loss: 0.18399202029361272 \t|| Test Loss: 0.43018109792994574\n",
      "Epoch: 3644 \t|| Train Loss: 0.18395809505907407 \t|| Test Loss: 0.43007089793263875\n",
      "Epoch: 3645 \t|| Train Loss: 0.18392319506085777 \t|| Test Loss: 0.42996069793533176\n",
      "Epoch: 3646 \t|| Train Loss: 0.183889499298103 \t|| Test Loss: 0.4299191879376415\n",
      "Epoch: 3647 \t|| Train Loss: 0.18385467506420305 \t|| Test Loss: 0.4298089879403345\n",
      "Epoch: 3648 \t|| Train Loss: 0.18382059830103167 \t|| Test Loss: 0.42976747794264425\n",
      "Epoch: 3649 \t|| Train Loss: 0.1837861550675483 \t|| Test Loss: 0.42965727794533726\n",
      "Epoch: 3650 \t|| Train Loss: 0.18375169730396035 \t|| Test Loss: 0.4296157679476469\n",
      "Epoch: 3651 \t|| Train Loss: 0.18371763507089361 \t|| Test Loss: 0.42950556795034\n",
      "Epoch: 3652 \t|| Train Loss: 0.18368279630688905 \t|| Test Loss: 0.42946405795264975\n",
      "Epoch: 3653 \t|| Train Loss: 0.18364911507423887 \t|| Test Loss: 0.42935385795534275\n",
      "Epoch: 3654 \t|| Train Loss: 0.18361421507602252 \t|| Test Loss: 0.42924365795803576\n",
      "Epoch: 3655 \t|| Train Loss: 0.18358027531137933 \t|| Test Loss: 0.4292021479603455\n",
      "Epoch: 3656 \t|| Train Loss: 0.1835456950793678 \t|| Test Loss: 0.4290919479630385\n",
      "Epoch: 3657 \t|| Train Loss: 0.18351137431430803 \t|| Test Loss: 0.42905043796534825\n",
      "Epoch: 3658 \t|| Train Loss: 0.18347717508271305 \t|| Test Loss: 0.42894023796804126\n",
      "Epoch: 3659 \t|| Train Loss: 0.1834424733172367 \t|| Test Loss: 0.428898727970351\n",
      "Epoch: 3660 \t|| Train Loss: 0.18340865508605836 \t|| Test Loss: 0.4287885279730439\n",
      "Epoch: 3661 \t|| Train Loss: 0.18337375508784204 \t|| Test Loss: 0.4286783279757369\n",
      "Epoch: 3662 \t|| Train Loss: 0.183339952321727 \t|| Test Loss: 0.42863681797804676\n",
      "Epoch: 3663 \t|| Train Loss: 0.18330523509118732 \t|| Test Loss: 0.4285266179807398\n",
      "Epoch: 3664 \t|| Train Loss: 0.18327105132465568 \t|| Test Loss: 0.4284851079830495\n",
      "Epoch: 3665 \t|| Train Loss: 0.1832367150945326 \t|| Test Loss: 0.4283749079857424\n",
      "Epoch: 3666 \t|| Train Loss: 0.18320215032758436 \t|| Test Loss: 0.42833339798805226\n",
      "Epoch: 3667 \t|| Train Loss: 0.18316819509787785 \t|| Test Loss: 0.42822319799074526\n",
      "Epoch: 3668 \t|| Train Loss: 0.1831332950996615 \t|| Test Loss: 0.42811299799343827\n",
      "Epoch: 3669 \t|| Train Loss: 0.1830996293320747 \t|| Test Loss: 0.428071487995748\n",
      "Epoch: 3670 \t|| Train Loss: 0.18306477510300678 \t|| Test Loss: 0.427961287998441\n",
      "Epoch: 3671 \t|| Train Loss: 0.1830307283350033 \t|| Test Loss: 0.42791977800075076\n",
      "Epoch: 3672 \t|| Train Loss: 0.18299625510635203 \t|| Test Loss: 0.42780957800344366\n",
      "Epoch: 3673 \t|| Train Loss: 0.18296182733793204 \t|| Test Loss: 0.4277680680057535\n",
      "Epoch: 3674 \t|| Train Loss: 0.18292773510969734 \t|| Test Loss: 0.4276578680084465\n",
      "Epoch: 3675 \t|| Train Loss: 0.18289292634086068 \t|| Test Loss: 0.42761635801075626\n",
      "Epoch: 3676 \t|| Train Loss: 0.1828592151130426 \t|| Test Loss: 0.42750615801344927\n",
      "Epoch: 3677 \t|| Train Loss: 0.18282431511482627 \t|| Test Loss: 0.4273959580161423\n",
      "Epoch: 3678 \t|| Train Loss: 0.182790405345351 \t|| Test Loss: 0.427354448018452\n",
      "Epoch: 3679 \t|| Train Loss: 0.18275579511817155 \t|| Test Loss: 0.427244248021145\n",
      "Epoch: 3680 \t|| Train Loss: 0.18272150434827966 \t|| Test Loss: 0.42720273802345476\n",
      "Epoch: 3681 \t|| Train Loss: 0.1826872751215168 \t|| Test Loss: 0.42709253802614777\n",
      "Epoch: 3682 \t|| Train Loss: 0.18265260335120834 \t|| Test Loss: 0.4270510280284575\n",
      "Epoch: 3683 \t|| Train Loss: 0.18261875512486211 \t|| Test Loss: 0.4269408280311505\n",
      "Epoch: 3684 \t|| Train Loss: 0.18258385512664577 \t|| Test Loss: 0.42683062803384353\n",
      "Epoch: 3685 \t|| Train Loss: 0.18255008235569864 \t|| Test Loss: 0.42678911803615327\n",
      "Epoch: 3686 \t|| Train Loss: 0.18251533512999102 \t|| Test Loss: 0.4266789180388463\n",
      "Epoch: 3687 \t|| Train Loss: 0.1824811813586273 \t|| Test Loss: 0.426637408041156\n",
      "Epoch: 3688 \t|| Train Loss: 0.18244681513333633 \t|| Test Loss: 0.426527208043849\n",
      "Epoch: 3689 \t|| Train Loss: 0.182412280361556 \t|| Test Loss: 0.42648569804615877\n",
      "Epoch: 3690 \t|| Train Loss: 0.1823782951366816 \t|| Test Loss: 0.4263754980488518\n",
      "Epoch: 3691 \t|| Train Loss: 0.18234339513846526 \t|| Test Loss: 0.4262652980515448\n",
      "Epoch: 3692 \t|| Train Loss: 0.18230975936604626 \t|| Test Loss: 0.4262237880538545\n",
      "Epoch: 3693 \t|| Train Loss: 0.18227487514181054 \t|| Test Loss: 0.42611358805654753\n",
      "Epoch: 3694 \t|| Train Loss: 0.18224085836897497 \t|| Test Loss: 0.4260720780588573\n",
      "Epoch: 3695 \t|| Train Loss: 0.18220635514515582 \t|| Test Loss: 0.4259618780615503\n",
      "Epoch: 3696 \t|| Train Loss: 0.18217195737190361 \t|| Test Loss: 0.42592036806386\n",
      "Epoch: 3697 \t|| Train Loss: 0.18213783514850107 \t|| Test Loss: 0.42581016806655303\n",
      "Epoch: 3698 \t|| Train Loss: 0.18210305637483234 \t|| Test Loss: 0.42576865806886277\n",
      "Epoch: 3699 \t|| Train Loss: 0.18206931515184638 \t|| Test Loss: 0.4256584580715558\n",
      "Epoch: 3700 \t|| Train Loss: 0.18203441515363003 \t|| Test Loss: 0.4255482580742488\n",
      "Epoch: 3701 \t|| Train Loss: 0.18200053537932265 \t|| Test Loss: 0.4255067480765585\n",
      "Epoch: 3702 \t|| Train Loss: 0.1819658951569753 \t|| Test Loss: 0.4253965480792514\n",
      "Epoch: 3703 \t|| Train Loss: 0.1819316343822513 \t|| Test Loss: 0.4253550380815613\n",
      "Epoch: 3704 \t|| Train Loss: 0.18189737516032056 \t|| Test Loss: 0.4252448380842543\n",
      "Epoch: 3705 \t|| Train Loss: 0.18186273338517997 \t|| Test Loss: 0.425203328086564\n",
      "Epoch: 3706 \t|| Train Loss: 0.18182885516366584 \t|| Test Loss: 0.4250931280892569\n",
      "Epoch: 3707 \t|| Train Loss: 0.18179395516544952 \t|| Test Loss: 0.42498292809195004\n",
      "Epoch: 3708 \t|| Train Loss: 0.18176021238967027 \t|| Test Loss: 0.4249414180942598\n",
      "Epoch: 3709 \t|| Train Loss: 0.18172543516879477 \t|| Test Loss: 0.4248312180969528\n",
      "Epoch: 3710 \t|| Train Loss: 0.18169131139259895 \t|| Test Loss: 0.42478970809926253\n",
      "Epoch: 3711 \t|| Train Loss: 0.18165691517214008 \t|| Test Loss: 0.4246795081019554\n",
      "Epoch: 3712 \t|| Train Loss: 0.18162241039552762 \t|| Test Loss: 0.42463799810426517\n",
      "Epoch: 3713 \t|| Train Loss: 0.1815883951754853 \t|| Test Loss: 0.4245277981069583\n",
      "Epoch: 3714 \t|| Train Loss: 0.18155350939845633 \t|| Test Loss: 0.42448628810926803\n",
      "Epoch: 3715 \t|| Train Loss: 0.1815198751788306 \t|| Test Loss: 0.42437608811196104\n",
      "Epoch: 3716 \t|| Train Loss: 0.1814849751806143 \t|| Test Loss: 0.42426588811465404\n",
      "Epoch: 3717 \t|| Train Loss: 0.18145098840294663 \t|| Test Loss: 0.4242243781169638\n",
      "Epoch: 3718 \t|| Train Loss: 0.18141645518395957 \t|| Test Loss: 0.4241141781196568\n",
      "Epoch: 3719 \t|| Train Loss: 0.1813820874058753 \t|| Test Loss: 0.42407266812196653\n",
      "Epoch: 3720 \t|| Train Loss: 0.18134793518730485 \t|| Test Loss: 0.42396246812465954\n",
      "Epoch: 3721 \t|| Train Loss: 0.18131318640880398 \t|| Test Loss: 0.4239209581269693\n",
      "Epoch: 3722 \t|| Train Loss: 0.1812794151906501 \t|| Test Loss: 0.4238107581296623\n",
      "Epoch: 3723 \t|| Train Loss: 0.18124451519243379 \t|| Test Loss: 0.4237005581323553\n",
      "Epoch: 3724 \t|| Train Loss: 0.18121066541329428 \t|| Test Loss: 0.42365904813466504\n",
      "Epoch: 3725 \t|| Train Loss: 0.18117599519577904 \t|| Test Loss: 0.42354884813735794\n",
      "Epoch: 3726 \t|| Train Loss: 0.18114176441622293 \t|| Test Loss: 0.4235073381396678\n",
      "Epoch: 3727 \t|| Train Loss: 0.18110747519912432 \t|| Test Loss: 0.4233971381423608\n",
      "Epoch: 3728 \t|| Train Loss: 0.18107286341915163 \t|| Test Loss: 0.42335562814467054\n",
      "Epoch: 3729 \t|| Train Loss: 0.1810389552024696 \t|| Test Loss: 0.42324542814736354\n",
      "Epoch: 3730 \t|| Train Loss: 0.18100405520425325 \t|| Test Loss: 0.42313522815005655\n",
      "Epoch: 3731 \t|| Train Loss: 0.1809703424236419 \t|| Test Loss: 0.4230937181523663\n",
      "Epoch: 3732 \t|| Train Loss: 0.18093553520759856 \t|| Test Loss: 0.4229835181550593\n",
      "Epoch: 3733 \t|| Train Loss: 0.1809014414265706 \t|| Test Loss: 0.42294200815736893\n",
      "Epoch: 3734 \t|| Train Loss: 0.1808670152109438 \t|| Test Loss: 0.42283180816006205\n",
      "Epoch: 3735 \t|| Train Loss: 0.18083254042949928 \t|| Test Loss: 0.4227902981623718\n",
      "Epoch: 3736 \t|| Train Loss: 0.1807984952142891 \t|| Test Loss: 0.4226800981650648\n",
      "Epoch: 3737 \t|| Train Loss: 0.18076363943242796 \t|| Test Loss: 0.42263858816737454\n",
      "Epoch: 3738 \t|| Train Loss: 0.18072997521763434 \t|| Test Loss: 0.42252838817006755\n",
      "Epoch: 3739 \t|| Train Loss: 0.18069507521941802 \t|| Test Loss: 0.42241818817276044\n",
      "Epoch: 3740 \t|| Train Loss: 0.1806611184369183 \t|| Test Loss: 0.4223766781750703\n",
      "Epoch: 3741 \t|| Train Loss: 0.1806265552227633 \t|| Test Loss: 0.4222664781777632\n",
      "Epoch: 3742 \t|| Train Loss: 0.18059221743984694 \t|| Test Loss: 0.42222496818007305\n",
      "Epoch: 3743 \t|| Train Loss: 0.18055803522610855 \t|| Test Loss: 0.42211476818276594\n",
      "Epoch: 3744 \t|| Train Loss: 0.18052331644277558 \t|| Test Loss: 0.4220732581850757\n",
      "Epoch: 3745 \t|| Train Loss: 0.18048951522945386 \t|| Test Loss: 0.4219630581877688\n",
      "Epoch: 3746 \t|| Train Loss: 0.18045461523123751 \t|| Test Loss: 0.4218528581904618\n",
      "Epoch: 3747 \t|| Train Loss: 0.1804207954472659 \t|| Test Loss: 0.42181134819277155\n",
      "Epoch: 3748 \t|| Train Loss: 0.18038609523458282 \t|| Test Loss: 0.42170114819546445\n",
      "Epoch: 3749 \t|| Train Loss: 0.1803518944501946 \t|| Test Loss: 0.4216596381977743\n",
      "Epoch: 3750 \t|| Train Loss: 0.18031757523792807 \t|| Test Loss: 0.4215494382004673\n",
      "Epoch: 3751 \t|| Train Loss: 0.1802829934531233 \t|| Test Loss: 0.42150792820277705\n",
      "Epoch: 3752 \t|| Train Loss: 0.18024905524127335 \t|| Test Loss: 0.42139772820547006\n",
      "Epoch: 3753 \t|| Train Loss: 0.180214155243057 \t|| Test Loss: 0.42128752820816306\n",
      "Epoch: 3754 \t|| Train Loss: 0.18018047245761357 \t|| Test Loss: 0.4212460182104728\n",
      "Epoch: 3755 \t|| Train Loss: 0.18014563524640229 \t|| Test Loss: 0.4211358182131658\n",
      "Epoch: 3756 \t|| Train Loss: 0.18011157146054224 \t|| Test Loss: 0.42109430821547555\n",
      "Epoch: 3757 \t|| Train Loss: 0.18007711524974757 \t|| Test Loss: 0.42098410821816856\n",
      "Epoch: 3758 \t|| Train Loss: 0.18004267046347092 \t|| Test Loss: 0.4209425982204783\n",
      "Epoch: 3759 \t|| Train Loss: 0.18000859525309282 \t|| Test Loss: 0.4208323982231713\n",
      "Epoch: 3760 \t|| Train Loss: 0.17997376946639962 \t|| Test Loss: 0.42079088822548094\n",
      "Epoch: 3761 \t|| Train Loss: 0.1799400752564381 \t|| Test Loss: 0.42068068822817406\n",
      "Epoch: 3762 \t|| Train Loss: 0.17990517525822175 \t|| Test Loss: 0.42057048823086696\n",
      "Epoch: 3763 \t|| Train Loss: 0.17987124847088992 \t|| Test Loss: 0.4205289782331768\n",
      "Epoch: 3764 \t|| Train Loss: 0.17983665526156706 \t|| Test Loss: 0.4204187782358697\n",
      "Epoch: 3765 \t|| Train Loss: 0.17980234747381857 \t|| Test Loss: 0.42037726823817956\n",
      "Epoch: 3766 \t|| Train Loss: 0.17976813526491237 \t|| Test Loss: 0.42026706824087245\n",
      "Epoch: 3767 \t|| Train Loss: 0.17973344647674727 \t|| Test Loss: 0.4202255582431822\n",
      "Epoch: 3768 \t|| Train Loss: 0.17969961526825756 \t|| Test Loss: 0.4201153582458752\n",
      "Epoch: 3769 \t|| Train Loss: 0.1796647152700413 \t|| Test Loss: 0.4200051582485682\n",
      "Epoch: 3770 \t|| Train Loss: 0.17963092548123755 \t|| Test Loss: 0.41996364825087795\n",
      "Epoch: 3771 \t|| Train Loss: 0.17959619527338652 \t|| Test Loss: 0.41985344825357107\n",
      "Epoch: 3772 \t|| Train Loss: 0.17956202448416622 \t|| Test Loss: 0.4198119382558807\n",
      "Epoch: 3773 \t|| Train Loss: 0.1795276752767318 \t|| Test Loss: 0.4197017382585738\n",
      "Epoch: 3774 \t|| Train Loss: 0.17949312348709492 \t|| Test Loss: 0.41966022826088345\n",
      "Epoch: 3775 \t|| Train Loss: 0.1794591552800771 \t|| Test Loss: 0.41955002826357657\n",
      "Epoch: 3776 \t|| Train Loss: 0.1794242552818608 \t|| Test Loss: 0.4194398282662696\n",
      "Epoch: 3777 \t|| Train Loss: 0.17939060249158523 \t|| Test Loss: 0.4193983182685793\n",
      "Epoch: 3778 \t|| Train Loss: 0.17935573528520604 \t|| Test Loss: 0.4192881182712723\n",
      "Epoch: 3779 \t|| Train Loss: 0.17932170149451393 \t|| Test Loss: 0.41924660827358207\n",
      "Epoch: 3780 \t|| Train Loss: 0.17928721528855132 \t|| Test Loss: 0.41913640827627496\n",
      "Epoch: 3781 \t|| Train Loss: 0.17925280049744258 \t|| Test Loss: 0.4190948982785848\n",
      "Epoch: 3782 \t|| Train Loss: 0.17921869529189655 \t|| Test Loss: 0.4189846982812777\n",
      "Epoch: 3783 \t|| Train Loss: 0.17918389950037122 \t|| Test Loss: 0.41894318828358756\n",
      "Epoch: 3784 \t|| Train Loss: 0.17915017529524185 \t|| Test Loss: 0.41883298828628057\n",
      "Epoch: 3785 \t|| Train Loss: 0.17911527529702553 \t|| Test Loss: 0.41872278828897347\n",
      "Epoch: 3786 \t|| Train Loss: 0.17908137850486155 \t|| Test Loss: 0.4186812782912833\n",
      "Epoch: 3787 \t|| Train Loss: 0.1790467553003708 \t|| Test Loss: 0.4185710782939762\n",
      "Epoch: 3788 \t|| Train Loss: 0.17901247750779023 \t|| Test Loss: 0.41852956829628596\n",
      "Epoch: 3789 \t|| Train Loss: 0.17897823530371607 \t|| Test Loss: 0.41841936829897897\n",
      "Epoch: 3790 \t|| Train Loss: 0.1789435765107189 \t|| Test Loss: 0.4183778583012888\n",
      "Epoch: 3791 \t|| Train Loss: 0.17890971530706135 \t|| Test Loss: 0.4182676583039818\n",
      "Epoch: 3792 \t|| Train Loss: 0.178874815308845 \t|| Test Loss: 0.41815745830667483\n",
      "Epoch: 3793 \t|| Train Loss: 0.17884105551520918 \t|| Test Loss: 0.41811594830898446\n",
      "Epoch: 3794 \t|| Train Loss: 0.17880629531219028 \t|| Test Loss: 0.41800574831167747\n",
      "Epoch: 3795 \t|| Train Loss: 0.1787721545181379 \t|| Test Loss: 0.4179642383139873\n",
      "Epoch: 3796 \t|| Train Loss: 0.17873777531553556 \t|| Test Loss: 0.41785403831668033\n",
      "Epoch: 3797 \t|| Train Loss: 0.17870325352106659 \t|| Test Loss: 0.41781252831898996\n",
      "Epoch: 3798 \t|| Train Loss: 0.17866925531888084 \t|| Test Loss: 0.41770232832168297\n",
      "Epoch: 3799 \t|| Train Loss: 0.1786343553206645 \t|| Test Loss: 0.4175921283243761\n",
      "Epoch: 3800 \t|| Train Loss: 0.17860073252555686 \t|| Test Loss: 0.41755061832668583\n",
      "Epoch: 3801 \t|| Train Loss: 0.17856583532400977 \t|| Test Loss: 0.4174404183293787\n",
      "Epoch: 3802 \t|| Train Loss: 0.17853183152848554 \t|| Test Loss: 0.4173989083316886\n",
      "Epoch: 3803 \t|| Train Loss: 0.17849731532735508 \t|| Test Loss: 0.4172887083343815\n",
      "Epoch: 3804 \t|| Train Loss: 0.17846293053141418 \t|| Test Loss: 0.4172471983366913\n",
      "Epoch: 3805 \t|| Train Loss: 0.17842879533070036 \t|| Test Loss: 0.41713699833938434\n",
      "Epoch: 3806 \t|| Train Loss: 0.1783940295343429 \t|| Test Loss: 0.4170954883416941\n",
      "Epoch: 3807 \t|| Train Loss: 0.17836027533404558 \t|| Test Loss: 0.4169852883443871\n",
      "Epoch: 3808 \t|| Train Loss: 0.17832537533582926 \t|| Test Loss: 0.4168750883470801\n",
      "Epoch: 3809 \t|| Train Loss: 0.1782915085388332 \t|| Test Loss: 0.41683357834938983\n",
      "Epoch: 3810 \t|| Train Loss: 0.1782568553391745 \t|| Test Loss: 0.41672337835208284\n",
      "Epoch: 3811 \t|| Train Loss: 0.1782226075417619 \t|| Test Loss: 0.41668186835439247\n",
      "Epoch: 3812 \t|| Train Loss: 0.17818833534251982 \t|| Test Loss: 0.4165716683570856\n",
      "Epoch: 3813 \t|| Train Loss: 0.17815370654469057 \t|| Test Loss: 0.41653015835939533\n",
      "Epoch: 3814 \t|| Train Loss: 0.17811981534586507 \t|| Test Loss: 0.41641995836208834\n",
      "Epoch: 3815 \t|| Train Loss: 0.17808491534764875 \t|| Test Loss: 0.41630975836478123\n",
      "Epoch: 3816 \t|| Train Loss: 0.17805118554918087 \t|| Test Loss: 0.416268248367091\n",
      "Epoch: 3817 \t|| Train Loss: 0.17801639535099403 \t|| Test Loss: 0.416158048369784\n",
      "Epoch: 3818 \t|| Train Loss: 0.17798228455210954 \t|| Test Loss: 0.4161165383720937\n",
      "Epoch: 3819 \t|| Train Loss: 0.1779478753543393 \t|| Test Loss: 0.41600633837478684\n",
      "Epoch: 3820 \t|| Train Loss: 0.17791338355503822 \t|| Test Loss: 0.4159648283770965\n",
      "Epoch: 3821 \t|| Train Loss: 0.17787935535768457 \t|| Test Loss: 0.4158546283797896\n",
      "Epoch: 3822 \t|| Train Loss: 0.1778444825579669 \t|| Test Loss: 0.41581311838209933\n",
      "Epoch: 3823 \t|| Train Loss: 0.17781083536102985 \t|| Test Loss: 0.41570291838479234\n",
      "Epoch: 3824 \t|| Train Loss: 0.17777593536281353 \t|| Test Loss: 0.41559271838748535\n",
      "Epoch: 3825 \t|| Train Loss: 0.17774196156245717 \t|| Test Loss: 0.415551208389795\n",
      "Epoch: 3826 \t|| Train Loss: 0.1777074153661588 \t|| Test Loss: 0.415441008392488\n",
      "Epoch: 3827 \t|| Train Loss: 0.17767306056538587 \t|| Test Loss: 0.41539949839479784\n",
      "Epoch: 3828 \t|| Train Loss: 0.17763889536950406 \t|| Test Loss: 0.41528929839749085\n",
      "Epoch: 3829 \t|| Train Loss: 0.17760415956831455 \t|| Test Loss: 0.4152477883998005\n",
      "Epoch: 3830 \t|| Train Loss: 0.17757037537284934 \t|| Test Loss: 0.4151375884024936\n",
      "Epoch: 3831 \t|| Train Loss: 0.17753547537463302 \t|| Test Loss: 0.4150273884051865\n",
      "Epoch: 3832 \t|| Train Loss: 0.17750163857280482 \t|| Test Loss: 0.41498587840749634\n",
      "Epoch: 3833 \t|| Train Loss: 0.17746695537797832 \t|| Test Loss: 0.41487567841018924\n",
      "Epoch: 3834 \t|| Train Loss: 0.17743273757573355 \t|| Test Loss: 0.4148341684124991\n",
      "Epoch: 3835 \t|| Train Loss: 0.17739843538132355 \t|| Test Loss: 0.4147239684151921\n",
      "Epoch: 3836 \t|| Train Loss: 0.1773638365786622 \t|| Test Loss: 0.41468245841750184\n",
      "Epoch: 3837 \t|| Train Loss: 0.17732991538466886 \t|| Test Loss: 0.41457225842019485\n",
      "Epoch: 3838 \t|| Train Loss: 0.17729501538645248 \t|| Test Loss: 0.41446205842288786\n",
      "Epoch: 3839 \t|| Train Loss: 0.17726131558315247 \t|| Test Loss: 0.4144205484251975\n",
      "Epoch: 3840 \t|| Train Loss: 0.1772264953897978 \t|| Test Loss: 0.4143103484278905\n",
      "Epoch: 3841 \t|| Train Loss: 0.17719241458608115 \t|| Test Loss: 0.41426883843020024\n",
      "Epoch: 3842 \t|| Train Loss: 0.17715797539314307 \t|| Test Loss: 0.41415863843289324\n",
      "Epoch: 3843 \t|| Train Loss: 0.17712351358900985 \t|| Test Loss: 0.414117128435203\n",
      "Epoch: 3844 \t|| Train Loss: 0.17708945539648832 \t|| Test Loss: 0.4140069284378961\n",
      "Epoch: 3845 \t|| Train Loss: 0.17705461259193853 \t|| Test Loss: 0.41396541844020573\n",
      "Epoch: 3846 \t|| Train Loss: 0.1770209353998336 \t|| Test Loss: 0.41385521844289874\n",
      "Epoch: 3847 \t|| Train Loss: 0.17698603540161728 \t|| Test Loss: 0.41374501844559186\n",
      "Epoch: 3848 \t|| Train Loss: 0.17695209159642883 \t|| Test Loss: 0.4137035084479015\n",
      "Epoch: 3849 \t|| Train Loss: 0.17691751540496253 \t|| Test Loss: 0.4135933084505946\n",
      "Epoch: 3850 \t|| Train Loss: 0.17688319059935748 \t|| Test Loss: 0.41355179845290435\n",
      "Epoch: 3851 \t|| Train Loss: 0.1768489954083078 \t|| Test Loss: 0.41344159845559736\n",
      "Epoch: 3852 \t|| Train Loss: 0.17681428960228618 \t|| Test Loss: 0.413400088457907\n",
      "Epoch: 3853 \t|| Train Loss: 0.1767804754116531 \t|| Test Loss: 0.4132898884606\n",
      "Epoch: 3854 \t|| Train Loss: 0.17674557541343675 \t|| Test Loss: 0.413179688463293\n",
      "Epoch: 3855 \t|| Train Loss: 0.17671176860677648 \t|| Test Loss: 0.41313817846560275\n",
      "Epoch: 3856 \t|| Train Loss: 0.17667705541678203 \t|| Test Loss: 0.41302797846829586\n",
      "Epoch: 3857 \t|| Train Loss: 0.17664286760970516 \t|| Test Loss: 0.4129864684706055\n",
      "Epoch: 3858 \t|| Train Loss: 0.1766085354201273 \t|| Test Loss: 0.4128762684732985\n",
      "Epoch: 3859 \t|| Train Loss: 0.17657396661263386 \t|| Test Loss: 0.41283475847560824\n",
      "Epoch: 3860 \t|| Train Loss: 0.17654001542347258 \t|| Test Loss: 0.41272455847830136\n",
      "Epoch: 3861 \t|| Train Loss: 0.17650511542525624 \t|| Test Loss: 0.41261435848099426\n",
      "Epoch: 3862 \t|| Train Loss: 0.17647144561712413 \t|| Test Loss: 0.412572848483304\n",
      "Epoch: 3863 \t|| Train Loss: 0.17643659542860152 \t|| Test Loss: 0.412462648485997\n",
      "Epoch: 3864 \t|| Train Loss: 0.1764025446200528 \t|| Test Loss: 0.41242113848830675\n",
      "Epoch: 3865 \t|| Train Loss: 0.1763680754319468 \t|| Test Loss: 0.41231093849099976\n",
      "Epoch: 3866 \t|| Train Loss: 0.17633364362298148 \t|| Test Loss: 0.4122694284933095\n",
      "Epoch: 3867 \t|| Train Loss: 0.1762995554352921 \t|| Test Loss: 0.4121592284960026\n",
      "Epoch: 3868 \t|| Train Loss: 0.17626474262591016 \t|| Test Loss: 0.41211771849831236\n",
      "Epoch: 3869 \t|| Train Loss: 0.17623103543863733 \t|| Test Loss: 0.41200751850100537\n",
      "Epoch: 3870 \t|| Train Loss: 0.17619613544042098 \t|| Test Loss: 0.41189731850369826\n",
      "Epoch: 3871 \t|| Train Loss: 0.17616222163040046 \t|| Test Loss: 0.4118558085060081\n",
      "Epoch: 3872 \t|| Train Loss: 0.1761276154437663 \t|| Test Loss: 0.411745608508701\n",
      "Epoch: 3873 \t|| Train Loss: 0.17609332063332914 \t|| Test Loss: 0.41170409851101075\n",
      "Epoch: 3874 \t|| Train Loss: 0.17605909544711157 \t|| Test Loss: 0.41159389851370376\n",
      "Epoch: 3875 \t|| Train Loss: 0.17602441963625787 \t|| Test Loss: 0.4115523885160135\n",
      "Epoch: 3876 \t|| Train Loss: 0.1759905754504568 \t|| Test Loss: 0.4114421885187066\n",
      "Epoch: 3877 \t|| Train Loss: 0.1759556754522405 \t|| Test Loss: 0.4113319885213995\n",
      "Epoch: 3878 \t|| Train Loss: 0.17592189864074811 \t|| Test Loss: 0.41129047852370937\n",
      "Epoch: 3879 \t|| Train Loss: 0.17588715545558578 \t|| Test Loss: 0.41118027852640227\n",
      "Epoch: 3880 \t|| Train Loss: 0.17585299764367682 \t|| Test Loss: 0.4111387685287121\n",
      "Epoch: 3881 \t|| Train Loss: 0.17581863545893106 \t|| Test Loss: 0.4110285685314051\n",
      "Epoch: 3882 \t|| Train Loss: 0.1757840966466055 \t|| Test Loss: 0.41098705853371487\n",
      "Epoch: 3883 \t|| Train Loss: 0.1757501154622763 \t|| Test Loss: 0.4108768585364079\n",
      "Epoch: 3884 \t|| Train Loss: 0.17571521546406 \t|| Test Loss: 0.4107666585391009\n",
      "Epoch: 3885 \t|| Train Loss: 0.17568157565109577 \t|| Test Loss: 0.4107251485414105\n",
      "Epoch: 3886 \t|| Train Loss: 0.17564669546740527 \t|| Test Loss: 0.4106149485441035\n",
      "Epoch: 3887 \t|| Train Loss: 0.17561267465402444 \t|| Test Loss: 0.41057343854641326\n",
      "Epoch: 3888 \t|| Train Loss: 0.17557817547075055 \t|| Test Loss: 0.4104632385491064\n",
      "Epoch: 3889 \t|| Train Loss: 0.17554377365695317 \t|| Test Loss: 0.410421728551416\n",
      "Epoch: 3890 \t|| Train Loss: 0.1755096554740958 \t|| Test Loss: 0.410311528554109\n",
      "Epoch: 3891 \t|| Train Loss: 0.17547487265988182 \t|| Test Loss: 0.41027001855641887\n",
      "Epoch: 3892 \t|| Train Loss: 0.1754411354774411 \t|| Test Loss: 0.41015981855911177\n",
      "Epoch: 3893 \t|| Train Loss: 0.17540623547922476 \t|| Test Loss: 0.4100496185618048\n",
      "Epoch: 3894 \t|| Train Loss: 0.17537235166437212 \t|| Test Loss: 0.4100081085641145\n",
      "Epoch: 3895 \t|| Train Loss: 0.17533771548257004 \t|| Test Loss: 0.4098979085668075\n",
      "Epoch: 3896 \t|| Train Loss: 0.1753034506673008 \t|| Test Loss: 0.40985639856911726\n",
      "Epoch: 3897 \t|| Train Loss: 0.1752691954859153 \t|| Test Loss: 0.40974619857181027\n",
      "Epoch: 3898 \t|| Train Loss: 0.1752345496702295 \t|| Test Loss: 0.4097046885741201\n",
      "Epoch: 3899 \t|| Train Loss: 0.1752006754892606 \t|| Test Loss: 0.409594488576813\n",
      "Epoch: 3900 \t|| Train Loss: 0.17516577549104423 \t|| Test Loss: 0.40948428857950603\n",
      "Epoch: 3901 \t|| Train Loss: 0.17513202867471978 \t|| Test Loss: 0.40944277858181577\n",
      "Epoch: 3902 \t|| Train Loss: 0.17509725549438954 \t|| Test Loss: 0.4093325785845088\n",
      "Epoch: 3903 \t|| Train Loss: 0.17506312767764845 \t|| Test Loss: 0.40929106858681863\n",
      "Epoch: 3904 \t|| Train Loss: 0.17502873549773482 \t|| Test Loss: 0.4091808685895115\n",
      "Epoch: 3905 \t|| Train Loss: 0.17499422668057715 \t|| Test Loss: 0.4091393585918214\n",
      "Epoch: 3906 \t|| Train Loss: 0.17496021550108007 \t|| Test Loss: 0.4090291585945144\n",
      "Epoch: 3907 \t|| Train Loss: 0.1749253256835058 \t|| Test Loss: 0.4089876485968241\n",
      "Epoch: 3908 \t|| Train Loss: 0.17489169550442535 \t|| Test Loss: 0.408877448599517\n",
      "Epoch: 3909 \t|| Train Loss: 0.17485679550620903 \t|| Test Loss: 0.40876724860221003\n",
      "Epoch: 3910 \t|| Train Loss: 0.1748228046879961 \t|| Test Loss: 0.4087257386045199\n",
      "Epoch: 3911 \t|| Train Loss: 0.17478827550955428 \t|| Test Loss: 0.4086155386072129\n",
      "Epoch: 3912 \t|| Train Loss: 0.17475390369092478 \t|| Test Loss: 0.40857402860952263\n",
      "Epoch: 3913 \t|| Train Loss: 0.1747197555128996 \t|| Test Loss: 0.40846382861221564\n",
      "Epoch: 3914 \t|| Train Loss: 0.17468500269385348 \t|| Test Loss: 0.40842231861452527\n",
      "Epoch: 3915 \t|| Train Loss: 0.1746512355162448 \t|| Test Loss: 0.4083121186172184\n",
      "Epoch: 3916 \t|| Train Loss: 0.1746163355180285 \t|| Test Loss: 0.4082019186199113\n",
      "Epoch: 3917 \t|| Train Loss: 0.17458248169834373 \t|| Test Loss: 0.408160408622221\n",
      "Epoch: 3918 \t|| Train Loss: 0.17454781552137377 \t|| Test Loss: 0.40805020862491403\n",
      "Epoch: 3919 \t|| Train Loss: 0.17451358070127246 \t|| Test Loss: 0.4080086986272239\n",
      "Epoch: 3920 \t|| Train Loss: 0.17447929552471905 \t|| Test Loss: 0.4078984986299169\n",
      "Epoch: 3921 \t|| Train Loss: 0.17444467970420113 \t|| Test Loss: 0.4078569886322265\n",
      "Epoch: 3922 \t|| Train Loss: 0.1744107755280643 \t|| Test Loss: 0.40774678863491953\n",
      "Epoch: 3923 \t|| Train Loss: 0.17437587552984798 \t|| Test Loss: 0.40763658863761254\n",
      "Epoch: 3924 \t|| Train Loss: 0.1743421587086914 \t|| Test Loss: 0.4075950786399223\n",
      "Epoch: 3925 \t|| Train Loss: 0.17430735553319326 \t|| Test Loss: 0.4074848786426153\n",
      "Epoch: 3926 \t|| Train Loss: 0.17427325771162008 \t|| Test Loss: 0.40744336864492503\n",
      "Epoch: 3927 \t|| Train Loss: 0.17423883553653857 \t|| Test Loss: 0.40733316864761804\n",
      "Epoch: 3928 \t|| Train Loss: 0.17420435671454876 \t|| Test Loss: 0.4072916586499278\n",
      "Epoch: 3929 \t|| Train Loss: 0.17417031553988382 \t|| Test Loss: 0.4071814586526208\n",
      "Epoch: 3930 \t|| Train Loss: 0.17413545571747746 \t|| Test Loss: 0.40713994865493053\n",
      "Epoch: 3931 \t|| Train Loss: 0.17410179554322908 \t|| Test Loss: 0.40702974865762354\n",
      "Epoch: 3932 \t|| Train Loss: 0.17406689554501273 \t|| Test Loss: 0.40691954866031654\n",
      "Epoch: 3933 \t|| Train Loss: 0.17403293472196776 \t|| Test Loss: 0.4068780386626263\n",
      "Epoch: 3934 \t|| Train Loss: 0.17399837554835806 \t|| Test Loss: 0.4067678386653193\n",
      "Epoch: 3935 \t|| Train Loss: 0.17396403372489644 \t|| Test Loss: 0.40672632866762914\n",
      "Epoch: 3936 \t|| Train Loss: 0.17392985555170332 \t|| Test Loss: 0.40661612867032215\n",
      "Epoch: 3937 \t|| Train Loss: 0.17389513272782514 \t|| Test Loss: 0.4065746186726318\n",
      "Epoch: 3938 \t|| Train Loss: 0.17386133555504857 \t|| Test Loss: 0.4064644186753248\n",
      "Epoch: 3939 \t|| Train Loss: 0.17382643555683225 \t|| Test Loss: 0.4063542186780178\n",
      "Epoch: 3940 \t|| Train Loss: 0.1737926117323154 \t|| Test Loss: 0.40631270868032754\n",
      "Epoch: 3941 \t|| Train Loss: 0.17375791556017756 \t|| Test Loss: 0.40620250868302055\n",
      "Epoch: 3942 \t|| Train Loss: 0.17372371073524406 \t|| Test Loss: 0.4061609986853303\n",
      "Epoch: 3943 \t|| Train Loss: 0.17368939556352278 \t|| Test Loss: 0.4060507986880234\n",
      "Epoch: 3944 \t|| Train Loss: 0.17365480973817277 \t|| Test Loss: 0.40600928869033304\n",
      "Epoch: 3945 \t|| Train Loss: 0.17362087556686806 \t|| Test Loss: 0.40589908869302604\n",
      "Epoch: 3946 \t|| Train Loss: 0.17358597556865174 \t|| Test Loss: 0.40578888869571905\n",
      "Epoch: 3947 \t|| Train Loss: 0.1735522887426631 \t|| Test Loss: 0.4057473786980288\n",
      "Epoch: 3948 \t|| Train Loss: 0.17351745557199702 \t|| Test Loss: 0.4056371787007218\n",
      "Epoch: 3949 \t|| Train Loss: 0.17348338774559174 \t|| Test Loss: 0.40559566870303154\n",
      "Epoch: 3950 \t|| Train Loss: 0.1734489355753423 \t|| Test Loss: 0.40548546870572455\n",
      "Epoch: 3951 \t|| Train Loss: 0.17341448674852042 \t|| Test Loss: 0.4054439587080343\n",
      "Epoch: 3952 \t|| Train Loss: 0.17338041557868755 \t|| Test Loss: 0.4053337587107274\n",
      "Epoch: 3953 \t|| Train Loss: 0.17334558575144912 \t|| Test Loss: 0.40529224871303704\n",
      "Epoch: 3954 \t|| Train Loss: 0.17331189558203283 \t|| Test Loss: 0.40518204871573005\n",
      "Epoch: 3955 \t|| Train Loss: 0.1732769955838165 \t|| Test Loss: 0.40507184871842306\n",
      "Epoch: 3956 \t|| Train Loss: 0.17324306475593937 \t|| Test Loss: 0.4050303387207329\n",
      "Epoch: 3957 \t|| Train Loss: 0.17320847558716176 \t|| Test Loss: 0.4049201387234258\n",
      "Epoch: 3958 \t|| Train Loss: 0.1731741637588681 \t|| Test Loss: 0.40487862872573555\n",
      "Epoch: 3959 \t|| Train Loss: 0.17313995559050704 \t|| Test Loss: 0.40476842872842855\n",
      "Epoch: 3960 \t|| Train Loss: 0.17310526276179675 \t|| Test Loss: 0.4047269187307383\n",
      "Epoch: 3961 \t|| Train Loss: 0.17307143559385235 \t|| Test Loss: 0.4046167187334314\n",
      "Epoch: 3962 \t|| Train Loss: 0.17303653559563598 \t|| Test Loss: 0.4045065187361243\n",
      "Epoch: 3963 \t|| Train Loss: 0.17300274176628708 \t|| Test Loss: 0.40446500873843405\n",
      "Epoch: 3964 \t|| Train Loss: 0.17296801559898126 \t|| Test Loss: 0.40435480874112706\n",
      "Epoch: 3965 \t|| Train Loss: 0.17293384076921572 \t|| Test Loss: 0.4043132987434368\n",
      "Epoch: 3966 \t|| Train Loss: 0.17289949560232656 \t|| Test Loss: 0.4042030987461298\n",
      "Epoch: 3967 \t|| Train Loss: 0.1728649397721444 \t|| Test Loss: 0.40416158874843966\n",
      "Epoch: 3968 \t|| Train Loss: 0.17283097560567184 \t|| Test Loss: 0.40405138875113256\n",
      "Epoch: 3969 \t|| Train Loss: 0.1727960756074555 \t|| Test Loss: 0.40394118875382556\n",
      "Epoch: 3970 \t|| Train Loss: 0.17276241877663473 \t|| Test Loss: 0.4038996787561353\n",
      "Epoch: 3971 \t|| Train Loss: 0.17272755561080078 \t|| Test Loss: 0.4037894787588283\n",
      "Epoch: 3972 \t|| Train Loss: 0.17269351777956338 \t|| Test Loss: 0.40374796876113805\n",
      "Epoch: 3973 \t|| Train Loss: 0.17265903561414603 \t|| Test Loss: 0.40363776876383106\n",
      "Epoch: 3974 \t|| Train Loss: 0.17262461678249208 \t|| Test Loss: 0.4035962587661408\n",
      "Epoch: 3975 \t|| Train Loss: 0.1725905156174913 \t|| Test Loss: 0.4034860587688339\n",
      "Epoch: 3976 \t|| Train Loss: 0.17255571578542073 \t|| Test Loss: 0.40344454877114355\n",
      "Epoch: 3977 \t|| Train Loss: 0.17252199562083656 \t|| Test Loss: 0.40333434877383656\n",
      "Epoch: 3978 \t|| Train Loss: 0.17248709562262027 \t|| Test Loss: 0.40322414877652957\n",
      "Epoch: 3979 \t|| Train Loss: 0.17245319478991106 \t|| Test Loss: 0.4031826387788393\n",
      "Epoch: 3980 \t|| Train Loss: 0.17241857562596552 \t|| Test Loss: 0.4030724387815324\n",
      "Epoch: 3981 \t|| Train Loss: 0.17238429379283973 \t|| Test Loss: 0.40303092878384206\n",
      "Epoch: 3982 \t|| Train Loss: 0.1723500556293108 \t|| Test Loss: 0.40292072878653507\n",
      "Epoch: 3983 \t|| Train Loss: 0.1723153927957684 \t|| Test Loss: 0.4028792187888448\n",
      "Epoch: 3984 \t|| Train Loss: 0.17228153563265608 \t|| Test Loss: 0.4027690187915378\n",
      "Epoch: 3985 \t|| Train Loss: 0.17224663563443976 \t|| Test Loss: 0.4026588187942308\n",
      "Epoch: 3986 \t|| Train Loss: 0.1722128718002587 \t|| Test Loss: 0.40261730879654056\n",
      "Epoch: 3987 \t|| Train Loss: 0.172178115637785 \t|| Test Loss: 0.40250710879923357\n",
      "Epoch: 3988 \t|| Train Loss: 0.17214397080318738 \t|| Test Loss: 0.4024655988015433\n",
      "Epoch: 3989 \t|| Train Loss: 0.17210959564113026 \t|| Test Loss: 0.4023553988042363\n",
      "Epoch: 3990 \t|| Train Loss: 0.17207506980611606 \t|| Test Loss: 0.40231388880654606\n",
      "Epoch: 3991 \t|| Train Loss: 0.17204107564447554 \t|| Test Loss: 0.40220368880923907\n",
      "Epoch: 3992 \t|| Train Loss: 0.17200617564625922 \t|| Test Loss: 0.4020934888119321\n",
      "Epoch: 3993 \t|| Train Loss: 0.17197254881060633 \t|| Test Loss: 0.4020519788142418\n",
      "Epoch: 3994 \t|| Train Loss: 0.17193765564960453 \t|| Test Loss: 0.4019417788169348\n",
      "Epoch: 3995 \t|| Train Loss: 0.17190364781353504 \t|| Test Loss: 0.40190026881924457\n",
      "Epoch: 3996 \t|| Train Loss: 0.17186913565294978 \t|| Test Loss: 0.4017900688219376\n",
      "Epoch: 3997 \t|| Train Loss: 0.1718347468164637 \t|| Test Loss: 0.4017485588242473\n",
      "Epoch: 3998 \t|| Train Loss: 0.17180061565629506 \t|| Test Loss: 0.4016383588269403\n",
      "Epoch: 3999 \t|| Train Loss: 0.17176584581939242 \t|| Test Loss: 0.40159684882925006\n",
      "Epoch: 4000 \t|| Train Loss: 0.17173209565964034 \t|| Test Loss: 0.40148664883194307\n",
      "Epoch: 4001 \t|| Train Loss: 0.171697195661424 \t|| Test Loss: 0.4013764488346361\n",
      "Epoch: 4002 \t|| Train Loss: 0.17166332482388266 \t|| Test Loss: 0.4013349388369458\n",
      "Epoch: 4003 \t|| Train Loss: 0.17162867566476928 \t|| Test Loss: 0.40122473883963883\n",
      "Epoch: 4004 \t|| Train Loss: 0.17159442382681137 \t|| Test Loss: 0.40118322884194857\n",
      "Epoch: 4005 \t|| Train Loss: 0.17156015566811456 \t|| Test Loss: 0.4010730288446416\n",
      "Epoch: 4006 \t|| Train Loss: 0.17152552282974004 \t|| Test Loss: 0.4010315188469513\n",
      "Epoch: 4007 \t|| Train Loss: 0.1714916356714598 \t|| Test Loss: 0.4009213188496443\n",
      "Epoch: 4008 \t|| Train Loss: 0.1714567356732435 \t|| Test Loss: 0.40081111885233744\n",
      "Epoch: 4009 \t|| Train Loss: 0.17142300183423034 \t|| Test Loss: 0.4007696088546471\n",
      "Epoch: 4010 \t|| Train Loss: 0.17138821567658877 \t|| Test Loss: 0.4006594088573401\n",
      "Epoch: 4011 \t|| Train Loss: 0.17135410083715902 \t|| Test Loss: 0.40061789885964993\n",
      "Epoch: 4012 \t|| Train Loss: 0.17131969567993402 \t|| Test Loss: 0.40050769886234294\n",
      "Epoch: 4013 \t|| Train Loss: 0.17128519984008772 \t|| Test Loss: 0.4004661888646526\n",
      "Epoch: 4014 \t|| Train Loss: 0.1712511756832793 \t|| Test Loss: 0.4003559888673456\n",
      "Epoch: 4015 \t|| Train Loss: 0.17121629884301642 \t|| Test Loss: 0.40031447886965543\n",
      "Epoch: 4016 \t|| Train Loss: 0.1711826556866246 \t|| Test Loss: 0.40020427887234833\n",
      "Epoch: 4017 \t|| Train Loss: 0.17114775568840826 \t|| Test Loss: 0.40009407887504134\n",
      "Epoch: 4018 \t|| Train Loss: 0.17111377784750667 \t|| Test Loss: 0.4000525688773511\n",
      "Epoch: 4019 \t|| Train Loss: 0.1710792356917535 \t|| Test Loss: 0.3999423688800441\n",
      "Epoch: 4020 \t|| Train Loss: 0.17104487685043535 \t|| Test Loss: 0.3999008588823538\n",
      "Epoch: 4021 \t|| Train Loss: 0.1710107156950988 \t|| Test Loss: 0.39979065888504683\n",
      "Epoch: 4022 \t|| Train Loss: 0.17097597585336405 \t|| Test Loss: 0.39974914888735663\n",
      "Epoch: 4023 \t|| Train Loss: 0.1709421956984441 \t|| Test Loss: 0.3996389488900496\n",
      "Epoch: 4024 \t|| Train Loss: 0.17090729570022772 \t|| Test Loss: 0.3995287488927426\n",
      "Epoch: 4025 \t|| Train Loss: 0.17087345485785432 \t|| Test Loss: 0.39948723889505233\n",
      "Epoch: 4026 \t|| Train Loss: 0.17083877570357303 \t|| Test Loss: 0.3993770388977454\n",
      "Epoch: 4027 \t|| Train Loss: 0.17080455386078303 \t|| Test Loss: 0.3993355289000551\n",
      "Epoch: 4028 \t|| Train Loss: 0.1707702557069183 \t|| Test Loss: 0.3992253289027481\n",
      "Epoch: 4029 \t|| Train Loss: 0.1707356528637117 \t|| Test Loss: 0.39918381890505783\n",
      "Epoch: 4030 \t|| Train Loss: 0.17070173571026356 \t|| Test Loss: 0.39907361890775084\n",
      "Epoch: 4031 \t|| Train Loss: 0.17066683571204724 \t|| Test Loss: 0.39896341891044385\n",
      "Epoch: 4032 \t|| Train Loss: 0.170633131868202 \t|| Test Loss: 0.3989219089127536\n",
      "Epoch: 4033 \t|| Train Loss: 0.17059831571539252 \t|| Test Loss: 0.3988117089154466\n",
      "Epoch: 4034 \t|| Train Loss: 0.17056423087113068 \t|| Test Loss: 0.39877019891775634\n",
      "Epoch: 4035 \t|| Train Loss: 0.17052979571873778 \t|| Test Loss: 0.3986599989204494\n",
      "Epoch: 4036 \t|| Train Loss: 0.17049532987405933 \t|| Test Loss: 0.3986184889227591\n",
      "Epoch: 4037 \t|| Train Loss: 0.17046127572208306 \t|| Test Loss: 0.39850828892545215\n",
      "Epoch: 4038 \t|| Train Loss: 0.17042642887698803 \t|| Test Loss: 0.3984667789277618\n",
      "Epoch: 4039 \t|| Train Loss: 0.17039275572542834 \t|| Test Loss: 0.39835657893045484\n",
      "Epoch: 4040 \t|| Train Loss: 0.17035785572721202 \t|| Test Loss: 0.39824637893314785\n",
      "Epoch: 4041 \t|| Train Loss: 0.17032390788147833 \t|| Test Loss: 0.3982048689354576\n",
      "Epoch: 4042 \t|| Train Loss: 0.17028933573055727 \t|| Test Loss: 0.3980946689381506\n",
      "Epoch: 4043 \t|| Train Loss: 0.170255006884407 \t|| Test Loss: 0.39805315894046034\n",
      "Epoch: 4044 \t|| Train Loss: 0.17022081573390255 \t|| Test Loss: 0.39794295894315335\n",
      "Epoch: 4045 \t|| Train Loss: 0.17018610588733568 \t|| Test Loss: 0.3979014489454631\n",
      "Epoch: 4046 \t|| Train Loss: 0.17015229573724783 \t|| Test Loss: 0.3977912489481561\n",
      "Epoch: 4047 \t|| Train Loss: 0.1701173957390315 \t|| Test Loss: 0.3976810489508491\n",
      "Epoch: 4048 \t|| Train Loss: 0.17008358489182598 \t|| Test Loss: 0.3976395389531589\n",
      "Epoch: 4049 \t|| Train Loss: 0.17004887574237676 \t|| Test Loss: 0.39752933895585185\n",
      "Epoch: 4050 \t|| Train Loss: 0.17001468389475466 \t|| Test Loss: 0.39748782895816165\n",
      "Epoch: 4051 \t|| Train Loss: 0.16998035574572207 \t|| Test Loss: 0.3973776289608546\n",
      "Epoch: 4052 \t|| Train Loss: 0.16994578289768336 \t|| Test Loss: 0.3973361189631644\n",
      "Epoch: 4053 \t|| Train Loss: 0.16991183574906732 \t|| Test Loss: 0.39722591896585735\n",
      "Epoch: 4054 \t|| Train Loss: 0.169876935750851 \t|| Test Loss: 0.3971157189685504\n",
      "Epoch: 4055 \t|| Train Loss: 0.16984326190217364 \t|| Test Loss: 0.3970742089708601\n",
      "Epoch: 4056 \t|| Train Loss: 0.16980841575419625 \t|| Test Loss: 0.3969640089735531\n",
      "Epoch: 4057 \t|| Train Loss: 0.16977436090510228 \t|| Test Loss: 0.39692249897586285\n",
      "Epoch: 4058 \t|| Train Loss: 0.16973989575754153 \t|| Test Loss: 0.39681229897855586\n",
      "Epoch: 4059 \t|| Train Loss: 0.169705459908031 \t|| Test Loss: 0.3967707889808656\n",
      "Epoch: 4060 \t|| Train Loss: 0.1696713757608868 \t|| Test Loss: 0.3966605889835586\n",
      "Epoch: 4061 \t|| Train Loss: 0.1696365589109597 \t|| Test Loss: 0.39661907898586835\n",
      "Epoch: 4062 \t|| Train Loss: 0.1696028557642321 \t|| Test Loss: 0.39650887898856135\n",
      "Epoch: 4063 \t|| Train Loss: 0.16956795576601574 \t|| Test Loss: 0.39639867899125436\n",
      "Epoch: 4064 \t|| Train Loss: 0.16953403791544996 \t|| Test Loss: 0.3963571689935641\n",
      "Epoch: 4065 \t|| Train Loss: 0.16949943576936105 \t|| Test Loss: 0.3962469689962571\n",
      "Epoch: 4066 \t|| Train Loss: 0.16946513691837867 \t|| Test Loss: 0.39620545899856685\n",
      "Epoch: 4067 \t|| Train Loss: 0.1694309157727063 \t|| Test Loss: 0.39609525900125986\n",
      "Epoch: 4068 \t|| Train Loss: 0.16939623592130734 \t|| Test Loss: 0.39605374900356954\n",
      "Epoch: 4069 \t|| Train Loss: 0.16936239577605156 \t|| Test Loss: 0.3959435490062626\n",
      "Epoch: 4070 \t|| Train Loss: 0.16932749577783526 \t|| Test Loss: 0.3958333490089556\n",
      "Epoch: 4071 \t|| Train Loss: 0.16929371492579762 \t|| Test Loss: 0.39579183901126536\n",
      "Epoch: 4072 \t|| Train Loss: 0.1692589757811805 \t|| Test Loss: 0.3956816390139583\n",
      "Epoch: 4073 \t|| Train Loss: 0.1692248139287263 \t|| Test Loss: 0.39564012901626805\n",
      "Epoch: 4074 \t|| Train Loss: 0.1691904557845258 \t|| Test Loss: 0.39552992901896106\n",
      "Epoch: 4075 \t|| Train Loss: 0.16915591293165497 \t|| Test Loss: 0.39548841902127085\n",
      "Epoch: 4076 \t|| Train Loss: 0.16912193578787105 \t|| Test Loss: 0.39537821902396386\n",
      "Epoch: 4077 \t|| Train Loss: 0.16908703578965473 \t|| Test Loss: 0.39526801902665687\n",
      "Epoch: 4078 \t|| Train Loss: 0.16905339193614527 \t|| Test Loss: 0.3952265090289666\n",
      "Epoch: 4079 \t|| Train Loss: 0.16901851579300003 \t|| Test Loss: 0.3951163090316596\n",
      "Epoch: 4080 \t|| Train Loss: 0.16898449093907397 \t|| Test Loss: 0.39507479903396936\n",
      "Epoch: 4081 \t|| Train Loss: 0.16894999579634531 \t|| Test Loss: 0.39496459903666237\n",
      "Epoch: 4082 \t|| Train Loss: 0.16891558994200265 \t|| Test Loss: 0.3949230890389721\n",
      "Epoch: 4083 \t|| Train Loss: 0.16888147579969054 \t|| Test Loss: 0.3948128890416651\n",
      "Epoch: 4084 \t|| Train Loss: 0.16884668894493132 \t|| Test Loss: 0.39477137904397486\n",
      "Epoch: 4085 \t|| Train Loss: 0.16881295580303585 \t|| Test Loss: 0.39466117904666786\n",
      "Epoch: 4086 \t|| Train Loss: 0.16877805580481947 \t|| Test Loss: 0.39455097904936093\n",
      "Epoch: 4087 \t|| Train Loss: 0.16874416794942163 \t|| Test Loss: 0.39450946905167067\n",
      "Epoch: 4088 \t|| Train Loss: 0.16870953580816478 \t|| Test Loss: 0.3943992690543636\n",
      "Epoch: 4089 \t|| Train Loss: 0.1686752669523503 \t|| Test Loss: 0.39435775905667336\n",
      "Epoch: 4090 \t|| Train Loss: 0.16864101581151006 \t|| Test Loss: 0.39424755905936637\n",
      "Epoch: 4091 \t|| Train Loss: 0.16860636595527895 \t|| Test Loss: 0.3942060490616761\n",
      "Epoch: 4092 \t|| Train Loss: 0.1685724958148553 \t|| Test Loss: 0.3940958490643691\n",
      "Epoch: 4093 \t|| Train Loss: 0.168537595816639 \t|| Test Loss: 0.3939856490670621\n",
      "Epoch: 4094 \t|| Train Loss: 0.16850384495976928 \t|| Test Loss: 0.3939441390693718\n",
      "Epoch: 4095 \t|| Train Loss: 0.16846907581998427 \t|| Test Loss: 0.3938339390720648\n",
      "Epoch: 4096 \t|| Train Loss: 0.16843494396269793 \t|| Test Loss: 0.3937924290743746\n",
      "Epoch: 4097 \t|| Train Loss: 0.16840055582332952 \t|| Test Loss: 0.3936822290770676\n",
      "Epoch: 4098 \t|| Train Loss: 0.16836604296562663 \t|| Test Loss: 0.3936407190793773\n",
      "Epoch: 4099 \t|| Train Loss: 0.1683320358266748 \t|| Test Loss: 0.3935305190820703\n",
      "Epoch: 4100 \t|| Train Loss: 0.16829714196855533 \t|| Test Loss: 0.3934890090843801\n",
      "Epoch: 4101 \t|| Train Loss: 0.16826351583002008 \t|| Test Loss: 0.3933788090870731\n",
      "Epoch: 4102 \t|| Train Loss: 0.16822861583180376 \t|| Test Loss: 0.39326860908976613\n",
      "Epoch: 4103 \t|| Train Loss: 0.16819462097304558 \t|| Test Loss: 0.3932270990920759\n",
      "Epoch: 4104 \t|| Train Loss: 0.16816009583514901 \t|| Test Loss: 0.3931168990947689\n",
      "Epoch: 4105 \t|| Train Loss: 0.16812571997597425 \t|| Test Loss: 0.39307538909707856\n",
      "Epoch: 4106 \t|| Train Loss: 0.1680915758384943 \t|| Test Loss: 0.39296518909977163\n",
      "Epoch: 4107 \t|| Train Loss: 0.16805681897890296 \t|| Test Loss: 0.3929236791020813\n",
      "Epoch: 4108 \t|| Train Loss: 0.16802305584183957 \t|| Test Loss: 0.3928134791047744\n",
      "Epoch: 4109 \t|| Train Loss: 0.16798815584362323 \t|| Test Loss: 0.3927032791074674\n",
      "Epoch: 4110 \t|| Train Loss: 0.16795429798339329 \t|| Test Loss: 0.3926617691097771\n",
      "Epoch: 4111 \t|| Train Loss: 0.1679196358469685 \t|| Test Loss: 0.3925515691124702\n",
      "Epoch: 4112 \t|| Train Loss: 0.16788539698632193 \t|| Test Loss: 0.3925100591147799\n",
      "Epoch: 4113 \t|| Train Loss: 0.16785111585031381 \t|| Test Loss: 0.3923998591174728\n",
      "Epoch: 4114 \t|| Train Loss: 0.16781649598925064 \t|| Test Loss: 0.3923583491197826\n",
      "Epoch: 4115 \t|| Train Loss: 0.16778259585365907 \t|| Test Loss: 0.39224814912247563\n",
      "Epoch: 4116 \t|| Train Loss: 0.16774769585544275 \t|| Test Loss: 0.39213794912516864\n",
      "Epoch: 4117 \t|| Train Loss: 0.1677139749937409 \t|| Test Loss: 0.39209643912747844\n",
      "Epoch: 4118 \t|| Train Loss: 0.16767917585878803 \t|| Test Loss: 0.39198623913017133\n",
      "Epoch: 4119 \t|| Train Loss: 0.16764507399666959 \t|| Test Loss: 0.39194472913248113\n",
      "Epoch: 4120 \t|| Train Loss: 0.1676106558621333 \t|| Test Loss: 0.39183452913517414\n",
      "Epoch: 4121 \t|| Train Loss: 0.16757617299959826 \t|| Test Loss: 0.3917930191374839\n",
      "Epoch: 4122 \t|| Train Loss: 0.16754213586547856 \t|| Test Loss: 0.3916828191401769\n",
      "Epoch: 4123 \t|| Train Loss: 0.16750727200252696 \t|| Test Loss: 0.3916413091424866\n",
      "Epoch: 4124 \t|| Train Loss: 0.1674736158688238 \t|| Test Loss: 0.39153110914517963\n",
      "Epoch: 4125 \t|| Train Loss: 0.1674387158706075 \t|| Test Loss: 0.39142090914787264\n",
      "Epoch: 4126 \t|| Train Loss: 0.16740475100701727 \t|| Test Loss: 0.3913793991501824\n",
      "Epoch: 4127 \t|| Train Loss: 0.1673701958739528 \t|| Test Loss: 0.3912691991528754\n",
      "Epoch: 4128 \t|| Train Loss: 0.1673358500099459 \t|| Test Loss: 0.3912276891551851\n",
      "Epoch: 4129 \t|| Train Loss: 0.16730167587729802 \t|| Test Loss: 0.3911174891578781\n",
      "Epoch: 4130 \t|| Train Loss: 0.1672669490128746 \t|| Test Loss: 0.3910759791601878\n",
      "Epoch: 4131 \t|| Train Loss: 0.1672331558806433 \t|| Test Loss: 0.3909657791628809\n",
      "Epoch: 4132 \t|| Train Loss: 0.167198255882427 \t|| Test Loss: 0.3908555791655739\n",
      "Epoch: 4133 \t|| Train Loss: 0.16716442801736492 \t|| Test Loss: 0.39081406916788364\n",
      "Epoch: 4134 \t|| Train Loss: 0.16712973588577226 \t|| Test Loss: 0.3907038691705766\n",
      "Epoch: 4135 \t|| Train Loss: 0.16709552702029357 \t|| Test Loss: 0.3906623591728864\n",
      "Epoch: 4136 \t|| Train Loss: 0.16706121588911754 \t|| Test Loss: 0.39055215917557934\n",
      "Epoch: 4137 \t|| Train Loss: 0.16702662602322227 \t|| Test Loss: 0.39051064917788914\n",
      "Epoch: 4138 \t|| Train Loss: 0.1669926958924628 \t|| Test Loss: 0.39040044918058214\n",
      "Epoch: 4139 \t|| Train Loss: 0.16695779589424647 \t|| Test Loss: 0.39029024918327515\n",
      "Epoch: 4140 \t|| Train Loss: 0.16692410502771254 \t|| Test Loss: 0.3902487391855849\n",
      "Epoch: 4141 \t|| Train Loss: 0.16688927589759178 \t|| Test Loss: 0.3901385391882779\n",
      "Epoch: 4142 \t|| Train Loss: 0.16685520403064125 \t|| Test Loss: 0.3900970291905876\n",
      "Epoch: 4143 \t|| Train Loss: 0.166820755900937 \t|| Test Loss: 0.38998682919328065\n",
      "Epoch: 4144 \t|| Train Loss: 0.1667863030335699 \t|| Test Loss: 0.3899453191955904\n",
      "Epoch: 4145 \t|| Train Loss: 0.1667522359042823 \t|| Test Loss: 0.3898351191982834\n",
      "Epoch: 4146 \t|| Train Loss: 0.1667174020364986 \t|| Test Loss: 0.38979360920059314\n",
      "Epoch: 4147 \t|| Train Loss: 0.1666837159076276 \t|| Test Loss: 0.38968340920328615\n",
      "Epoch: 4148 \t|| Train Loss: 0.16664881590941127 \t|| Test Loss: 0.3895732092059791\n",
      "Epoch: 4149 \t|| Train Loss: 0.16661488104098887 \t|| Test Loss: 0.3895316992082888\n",
      "Epoch: 4150 \t|| Train Loss: 0.16658029591275653 \t|| Test Loss: 0.3894214992109818\n",
      "Epoch: 4151 \t|| Train Loss: 0.16654598004391757 \t|| Test Loss: 0.3893799892132916\n",
      "Epoch: 4152 \t|| Train Loss: 0.1665117759161018 \t|| Test Loss: 0.38926978921598465\n",
      "Epoch: 4153 \t|| Train Loss: 0.16647707904684625 \t|| Test Loss: 0.3892282792182944\n",
      "Epoch: 4154 \t|| Train Loss: 0.1664432559194471 \t|| Test Loss: 0.3891180792209874\n",
      "Epoch: 4155 \t|| Train Loss: 0.16640835592123074 \t|| Test Loss: 0.3890078792236804\n",
      "Epoch: 4156 \t|| Train Loss: 0.16637455805133655 \t|| Test Loss: 0.38896636922599015\n",
      "Epoch: 4157 \t|| Train Loss: 0.166339835924576 \t|| Test Loss: 0.38885616922868316\n",
      "Epoch: 4158 \t|| Train Loss: 0.1663056570542652 \t|| Test Loss: 0.3888146592309929\n",
      "Epoch: 4159 \t|| Train Loss: 0.16627131592792127 \t|| Test Loss: 0.38870445923368585\n",
      "Epoch: 4160 \t|| Train Loss: 0.1662367560571939 \t|| Test Loss: 0.38866294923599565\n",
      "Epoch: 4161 \t|| Train Loss: 0.16620279593126655 \t|| Test Loss: 0.38855274923868865\n",
      "Epoch: 4162 \t|| Train Loss: 0.16616789593305023 \t|| Test Loss: 0.3884425492413816\n",
      "Epoch: 4163 \t|| Train Loss: 0.1661342350616842 \t|| Test Loss: 0.3884010392436914\n",
      "Epoch: 4164 \t|| Train Loss: 0.1660993759363955 \t|| Test Loss: 0.3882908392463844\n",
      "Epoch: 4165 \t|| Train Loss: 0.16606533406461288 \t|| Test Loss: 0.38824932924869415\n",
      "Epoch: 4166 \t|| Train Loss: 0.1660308559397408 \t|| Test Loss: 0.3881391292513871\n",
      "Epoch: 4167 \t|| Train Loss: 0.16599643306754158 \t|| Test Loss: 0.3880976192536969\n",
      "Epoch: 4168 \t|| Train Loss: 0.16596233594308601 \t|| Test Loss: 0.38798741925638985\n",
      "Epoch: 4169 \t|| Train Loss: 0.16592753207047023 \t|| Test Loss: 0.38794590925869965\n",
      "Epoch: 4170 \t|| Train Loss: 0.1658938159464313 \t|| Test Loss: 0.38783570926139266\n",
      "Epoch: 4171 \t|| Train Loss: 0.16585891594821497 \t|| Test Loss: 0.38772550926408567\n",
      "Epoch: 4172 \t|| Train Loss: 0.16582501107496056 \t|| Test Loss: 0.3876839992663954\n",
      "Epoch: 4173 \t|| Train Loss: 0.16579039595156025 \t|| Test Loss: 0.38757379926908836\n",
      "Epoch: 4174 \t|| Train Loss: 0.1657561100778892 \t|| Test Loss: 0.38753228927139816\n",
      "Epoch: 4175 \t|| Train Loss: 0.1657218759549055 \t|| Test Loss: 0.38742208927409116\n",
      "Epoch: 4176 \t|| Train Loss: 0.1656872090808179 \t|| Test Loss: 0.3873805792764009\n",
      "Epoch: 4177 \t|| Train Loss: 0.16565335595825081 \t|| Test Loss: 0.3872703792790939\n",
      "Epoch: 4178 \t|| Train Loss: 0.1656184559600345 \t|| Test Loss: 0.3871601792817869\n",
      "Epoch: 4179 \t|| Train Loss: 0.16558468808530818 \t|| Test Loss: 0.3871186692840966\n",
      "Epoch: 4180 \t|| Train Loss: 0.16554993596337975 \t|| Test Loss: 0.38700846928678967\n",
      "Epoch: 4181 \t|| Train Loss: 0.16551578708823683 \t|| Test Loss: 0.38696695928909935\n",
      "Epoch: 4182 \t|| Train Loss: 0.165481415966725 \t|| Test Loss: 0.3868567592917924\n",
      "Epoch: 4183 \t|| Train Loss: 0.16544688609116556 \t|| Test Loss: 0.38681524929410205\n",
      "Epoch: 4184 \t|| Train Loss: 0.1654128959700703 \t|| Test Loss: 0.38670504929679517\n",
      "Epoch: 4185 \t|| Train Loss: 0.16537799597185393 \t|| Test Loss: 0.3865948492994882\n",
      "Epoch: 4186 \t|| Train Loss: 0.16534436509565584 \t|| Test Loss: 0.3865533393017979\n",
      "Epoch: 4187 \t|| Train Loss: 0.16530947597519924 \t|| Test Loss: 0.3864431393044908\n",
      "Epoch: 4188 \t|| Train Loss: 0.16527546409858454 \t|| Test Loss: 0.38640162930680066\n",
      "Epoch: 4189 \t|| Train Loss: 0.16524095597854455 \t|| Test Loss: 0.3862914293094936\n",
      "Epoch: 4190 \t|| Train Loss: 0.16520656310151322 \t|| Test Loss: 0.38624991931180336\n",
      "Epoch: 4191 \t|| Train Loss: 0.1651724359818898 \t|| Test Loss: 0.38613971931449637\n",
      "Epoch: 4192 \t|| Train Loss: 0.1651376621044419 \t|| Test Loss: 0.3860982093168061\n",
      "Epoch: 4193 \t|| Train Loss: 0.16510391598523508 \t|| Test Loss: 0.3859880093194991\n",
      "Epoch: 4194 \t|| Train Loss: 0.16506901598701873 \t|| Test Loss: 0.3858778093221921\n",
      "Epoch: 4195 \t|| Train Loss: 0.1650351411089322 \t|| Test Loss: 0.3858362993245019\n",
      "Epoch: 4196 \t|| Train Loss: 0.165000495990364 \t|| Test Loss: 0.38572609932719487\n",
      "Epoch: 4197 \t|| Train Loss: 0.16496624011186084 \t|| Test Loss: 0.3856845893295046\n",
      "Epoch: 4198 \t|| Train Loss: 0.16493197599370926 \t|| Test Loss: 0.3855743893321976\n",
      "Epoch: 4199 \t|| Train Loss: 0.16489733911478952 \t|| Test Loss: 0.3855328793345074\n",
      "Epoch: 4200 \t|| Train Loss: 0.16486345599705454 \t|| Test Loss: 0.38542267933720037\n",
      "Epoch: 4201 \t|| Train Loss: 0.16482855599883822 \t|| Test Loss: 0.3853124793398934\n",
      "Epoch: 4202 \t|| Train Loss: 0.16479481811927982 \t|| Test Loss: 0.3852709693422031\n",
      "Epoch: 4203 \t|| Train Loss: 0.16476003600218353 \t|| Test Loss: 0.3851607693448961\n",
      "Epoch: 4204 \t|| Train Loss: 0.1647259171222085 \t|| Test Loss: 0.38511925934720587\n",
      "Epoch: 4205 \t|| Train Loss: 0.1646915160055288 \t|| Test Loss: 0.3850090593498989\n",
      "Epoch: 4206 \t|| Train Loss: 0.1646570161251372 \t|| Test Loss: 0.3849675493522086\n",
      "Epoch: 4207 \t|| Train Loss: 0.16462299600887403 \t|| Test Loss: 0.3848573493549017\n",
      "Epoch: 4208 \t|| Train Loss: 0.1645881151280659 \t|| Test Loss: 0.38481583935721136\n",
      "Epoch: 4209 \t|| Train Loss: 0.16455447601221934 \t|| Test Loss: 0.3847056393599044\n",
      "Epoch: 4210 \t|| Train Loss: 0.16451957601400297 \t|| Test Loss: 0.3845954393625974\n",
      "Epoch: 4211 \t|| Train Loss: 0.16448559413255617 \t|| Test Loss: 0.3845539293649072\n",
      "Epoch: 4212 \t|| Train Loss: 0.16445105601734827 \t|| Test Loss: 0.3844437293676002\n",
      "Epoch: 4213 \t|| Train Loss: 0.16441669313548485 \t|| Test Loss: 0.3844022193699099\n",
      "Epoch: 4214 \t|| Train Loss: 0.16438253602069355 \t|| Test Loss: 0.38429201937260293\n",
      "Epoch: 4215 \t|| Train Loss: 0.16434779213841355 \t|| Test Loss: 0.3842505093749127\n",
      "Epoch: 4216 \t|| Train Loss: 0.1643140160240388 \t|| Test Loss: 0.3841403093776057\n",
      "Epoch: 4217 \t|| Train Loss: 0.1642791160258225 \t|| Test Loss: 0.3840301093802987\n",
      "Epoch: 4218 \t|| Train Loss: 0.16424527114290383 \t|| Test Loss: 0.38398859938260843\n",
      "Epoch: 4219 \t|| Train Loss: 0.16421059602916777 \t|| Test Loss: 0.38387839938530144\n",
      "Epoch: 4220 \t|| Train Loss: 0.16417637014583253 \t|| Test Loss: 0.3838368893876112\n",
      "Epoch: 4221 \t|| Train Loss: 0.16414207603251302 \t|| Test Loss: 0.38372668939030413\n",
      "Epoch: 4222 \t|| Train Loss: 0.1641074691487612 \t|| Test Loss: 0.3836851793926139\n",
      "Epoch: 4223 \t|| Train Loss: 0.1640735560358583 \t|| Test Loss: 0.3835749793953069\n",
      "Epoch: 4224 \t|| Train Loss: 0.16403865603764198 \t|| Test Loss: 0.38346477939799983\n",
      "Epoch: 4225 \t|| Train Loss: 0.16400494815325145 \t|| Test Loss: 0.38342326940030963\n",
      "Epoch: 4226 \t|| Train Loss: 0.16397013604098726 \t|| Test Loss: 0.3833130694030027\n",
      "Epoch: 4227 \t|| Train Loss: 0.16393604715618015 \t|| Test Loss: 0.3832715594053123\n",
      "Epoch: 4228 \t|| Train Loss: 0.16390161604433257 \t|| Test Loss: 0.3831613594080054\n",
      "Epoch: 4229 \t|| Train Loss: 0.16386714615910886 \t|| Test Loss: 0.3831198494103151\n",
      "Epoch: 4230 \t|| Train Loss: 0.1638330960476778 \t|| Test Loss: 0.38300964941300814\n",
      "Epoch: 4231 \t|| Train Loss: 0.16379824516203753 \t|| Test Loss: 0.3829681394153179\n",
      "Epoch: 4232 \t|| Train Loss: 0.16376457605102307 \t|| Test Loss: 0.3828579394180109\n",
      "Epoch: 4233 \t|| Train Loss: 0.16372967605280678 \t|| Test Loss: 0.3827477394207039\n",
      "Epoch: 4234 \t|| Train Loss: 0.16369572416652783 \t|| Test Loss: 0.38270622942301363\n",
      "Epoch: 4235 \t|| Train Loss: 0.163661156056152 \t|| Test Loss: 0.38259602942570664\n",
      "Epoch: 4236 \t|| Train Loss: 0.16362682316945648 \t|| Test Loss: 0.3825545194280164\n",
      "Epoch: 4237 \t|| Train Loss: 0.1635926360594973 \t|| Test Loss: 0.3824443194307094\n",
      "Epoch: 4238 \t|| Train Loss: 0.16355792217238516 \t|| Test Loss: 0.3824028094330192\n",
      "Epoch: 4239 \t|| Train Loss: 0.16352411606284256 \t|| Test Loss: 0.3822926094357122\n",
      "Epoch: 4240 \t|| Train Loss: 0.16348921606462624 \t|| Test Loss: 0.38218240943840515\n",
      "Epoch: 4241 \t|| Train Loss: 0.16345540117687546 \t|| Test Loss: 0.38214089944071483\n",
      "Epoch: 4242 \t|| Train Loss: 0.16342069606797152 \t|| Test Loss: 0.38203069944340784\n",
      "Epoch: 4243 \t|| Train Loss: 0.16338650017980413 \t|| Test Loss: 0.3819891894457177\n",
      "Epoch: 4244 \t|| Train Loss: 0.16335217607131677 \t|| Test Loss: 0.3818789894484107\n",
      "Epoch: 4245 \t|| Train Loss: 0.16331759918273284 \t|| Test Loss: 0.38183747945072044\n",
      "Epoch: 4246 \t|| Train Loss: 0.16328365607466205 \t|| Test Loss: 0.38172727945341345\n",
      "Epoch: 4247 \t|| Train Loss: 0.16324875607644573 \t|| Test Loss: 0.38161707945610646\n",
      "Epoch: 4248 \t|| Train Loss: 0.16321507818722314 \t|| Test Loss: 0.3815755694584162\n",
      "Epoch: 4249 \t|| Train Loss: 0.16318023607979099 \t|| Test Loss: 0.3814653694611091\n",
      "Epoch: 4250 \t|| Train Loss: 0.16314617719015181 \t|| Test Loss: 0.38142385946341895\n",
      "Epoch: 4251 \t|| Train Loss: 0.16311171608313627 \t|| Test Loss: 0.38131365946611184\n",
      "Epoch: 4252 \t|| Train Loss: 0.1630772761930805 \t|| Test Loss: 0.3812721494684217\n",
      "Epoch: 4253 \t|| Train Loss: 0.16304319608648155 \t|| Test Loss: 0.3811619494711147\n",
      "Epoch: 4254 \t|| Train Loss: 0.16300837519600916 \t|| Test Loss: 0.3811204394734244\n",
      "Epoch: 4255 \t|| Train Loss: 0.1629746760898268 \t|| Test Loss: 0.38101023947611734\n",
      "Epoch: 4256 \t|| Train Loss: 0.1629397760916105 \t|| Test Loss: 0.3809000394788104\n",
      "Epoch: 4257 \t|| Train Loss: 0.16290585420049944 \t|| Test Loss: 0.38085852948112014\n",
      "Epoch: 4258 \t|| Train Loss: 0.16287125609495576 \t|| Test Loss: 0.38074832948381315\n",
      "Epoch: 4259 \t|| Train Loss: 0.16283695320342811 \t|| Test Loss: 0.3807068194861229\n",
      "Epoch: 4260 \t|| Train Loss: 0.16280273609830104 \t|| Test Loss: 0.3805966194888159\n",
      "Epoch: 4261 \t|| Train Loss: 0.16276805220635684 \t|| Test Loss: 0.38055510949112564\n",
      "Epoch: 4262 \t|| Train Loss: 0.1627342161016463 \t|| Test Loss: 0.3804449094938186\n",
      "Epoch: 4263 \t|| Train Loss: 0.16269931610342997 \t|| Test Loss: 0.38033470949651166\n",
      "Epoch: 4264 \t|| Train Loss: 0.16266553121084712 \t|| Test Loss: 0.3802931994988214\n",
      "Epoch: 4265 \t|| Train Loss: 0.16263079610677525 \t|| Test Loss: 0.3801829995015144\n",
      "Epoch: 4266 \t|| Train Loss: 0.16259663021377582 \t|| Test Loss: 0.38014148950382415\n",
      "Epoch: 4267 \t|| Train Loss: 0.1625622761101205 \t|| Test Loss: 0.3800312895065172\n",
      "Epoch: 4268 \t|| Train Loss: 0.1625277292167045 \t|| Test Loss: 0.37998977950882695\n",
      "Epoch: 4269 \t|| Train Loss: 0.16249375611346578 \t|| Test Loss: 0.3798795795115199\n",
      "Epoch: 4270 \t|| Train Loss: 0.16245885611524946 \t|| Test Loss: 0.37976937951421286\n",
      "Epoch: 4271 \t|| Train Loss: 0.16242520822119472 \t|| Test Loss: 0.37972786951652265\n",
      "Epoch: 4272 \t|| Train Loss: 0.16239033611859474 \t|| Test Loss: 0.37961766951921566\n",
      "Epoch: 4273 \t|| Train Loss: 0.16235630722412348 \t|| Test Loss: 0.3795761595215254\n",
      "Epoch: 4274 \t|| Train Loss: 0.16232181612194002 \t|| Test Loss: 0.37946595952421847\n",
      "Epoch: 4275 \t|| Train Loss: 0.16228740622705215 \t|| Test Loss: 0.3794244495265282\n",
      "Epoch: 4276 \t|| Train Loss: 0.16225329612528525 \t|| Test Loss: 0.3793142495292212\n",
      "Epoch: 4277 \t|| Train Loss: 0.1622185052299808 \t|| Test Loss: 0.37927273953153096\n",
      "Epoch: 4278 \t|| Train Loss: 0.16218477612863058 \t|| Test Loss: 0.37916253953422396\n",
      "Epoch: 4279 \t|| Train Loss: 0.16214987613041423 \t|| Test Loss: 0.37905233953691686\n",
      "Epoch: 4280 \t|| Train Loss: 0.1621159842344711 \t|| Test Loss: 0.3790108295392266\n",
      "Epoch: 4281 \t|| Train Loss: 0.16208135613375949 \t|| Test Loss: 0.3789006295419196\n",
      "Epoch: 4282 \t|| Train Loss: 0.16204708323739977 \t|| Test Loss: 0.37885911954422935\n",
      "Epoch: 4283 \t|| Train Loss: 0.1620128361371048 \t|| Test Loss: 0.37874891954692247\n",
      "Epoch: 4284 \t|| Train Loss: 0.16197818224032848 \t|| Test Loss: 0.3787074095492321\n",
      "Epoch: 4285 \t|| Train Loss: 0.16194431614045005 \t|| Test Loss: 0.37859720955192516\n",
      "Epoch: 4286 \t|| Train Loss: 0.16190941614223373 \t|| Test Loss: 0.37848700955461817\n",
      "Epoch: 4287 \t|| Train Loss: 0.16187566124481875 \t|| Test Loss: 0.3784454995569279\n",
      "Epoch: 4288 \t|| Train Loss: 0.161840896145579 \t|| Test Loss: 0.3783352995596209\n",
      "Epoch: 4289 \t|| Train Loss: 0.16180676024774746 \t|| Test Loss: 0.37829378956193066\n",
      "Epoch: 4290 \t|| Train Loss: 0.16177237614892426 \t|| Test Loss: 0.37818358956462367\n",
      "Epoch: 4291 \t|| Train Loss: 0.16173785925067613 \t|| Test Loss: 0.3781420795669334\n",
      "Epoch: 4292 \t|| Train Loss: 0.16170385615226954 \t|| Test Loss: 0.3780318795696264\n",
      "Epoch: 4293 \t|| Train Loss: 0.1616689582536048 \t|| Test Loss: 0.37799036957193616\n",
      "Epoch: 4294 \t|| Train Loss: 0.16163533615561482 \t|| Test Loss: 0.3778801695746291\n",
      "Epoch: 4295 \t|| Train Loss: 0.16160043615739847 \t|| Test Loss: 0.3777699695773222\n",
      "Epoch: 4296 \t|| Train Loss: 0.1615664372580951 \t|| Test Loss: 0.3777284595796319\n",
      "Epoch: 4297 \t|| Train Loss: 0.16153191616074375 \t|| Test Loss: 0.3776182595823249\n",
      "Epoch: 4298 \t|| Train Loss: 0.16149753626102376 \t|| Test Loss: 0.3775767495846347\n",
      "Epoch: 4299 \t|| Train Loss: 0.16146339616408906 \t|| Test Loss: 0.37746654958732767\n",
      "Epoch: 4300 \t|| Train Loss: 0.16142863526395246 \t|| Test Loss: 0.37742503958963747\n",
      "Epoch: 4301 \t|| Train Loss: 0.1613948761674343 \t|| Test Loss: 0.3773148395923304\n",
      "Epoch: 4302 \t|| Train Loss: 0.161359976169218 \t|| Test Loss: 0.37720463959502337\n",
      "Epoch: 4303 \t|| Train Loss: 0.16132611426844276 \t|| Test Loss: 0.37716312959733317\n",
      "Epoch: 4304 \t|| Train Loss: 0.16129145617256327 \t|| Test Loss: 0.3770529296000262\n",
      "Epoch: 4305 \t|| Train Loss: 0.16125721327137144 \t|| Test Loss: 0.3770114196023359\n",
      "Epoch: 4306 \t|| Train Loss: 0.16122293617590855 \t|| Test Loss: 0.37690121960502887\n",
      "Epoch: 4307 \t|| Train Loss: 0.1611883122743001 \t|| Test Loss: 0.3768597096073387\n",
      "Epoch: 4308 \t|| Train Loss: 0.1611544161792538 \t|| Test Loss: 0.3767495096100317\n",
      "Epoch: 4309 \t|| Train Loss: 0.16111951618103748 \t|| Test Loss: 0.3766393096127246\n",
      "Epoch: 4310 \t|| Train Loss: 0.1610857912787904 \t|| Test Loss: 0.37659779961503437\n",
      "Epoch: 4311 \t|| Train Loss: 0.16105099618438273 \t|| Test Loss: 0.3764875996177274\n",
      "Epoch: 4312 \t|| Train Loss: 0.16101689028171912 \t|| Test Loss: 0.3764460896200372\n",
      "Epoch: 4313 \t|| Train Loss: 0.160982476187728 \t|| Test Loss: 0.3763358896227301\n",
      "Epoch: 4314 \t|| Train Loss: 0.16094798928464776 \t|| Test Loss: 0.3762943796250399\n",
      "Epoch: 4315 \t|| Train Loss: 0.1609139561910733 \t|| Test Loss: 0.3761841796277329\n",
      "Epoch: 4316 \t|| Train Loss: 0.16087908828757644 \t|| Test Loss: 0.3761426696300427\n",
      "Epoch: 4317 \t|| Train Loss: 0.16084543619441857 \t|| Test Loss: 0.3760324696327356\n",
      "Epoch: 4318 \t|| Train Loss: 0.16081053619620223 \t|| Test Loss: 0.37592226963542863\n",
      "Epoch: 4319 \t|| Train Loss: 0.16077656729206674 \t|| Test Loss: 0.3758807596377384\n",
      "Epoch: 4320 \t|| Train Loss: 0.1607420161995475 \t|| Test Loss: 0.37577055964043143\n",
      "Epoch: 4321 \t|| Train Loss: 0.16070766629499542 \t|| Test Loss: 0.3757290496427412\n",
      "Epoch: 4322 \t|| Train Loss: 0.16067349620289278 \t|| Test Loss: 0.3756188496454342\n",
      "Epoch: 4323 \t|| Train Loss: 0.16063876529792412 \t|| Test Loss: 0.375577339647744\n",
      "Epoch: 4324 \t|| Train Loss: 0.1606049762062381 \t|| Test Loss: 0.37546713965043693\n",
      "Epoch: 4325 \t|| Train Loss: 0.16057007620802172 \t|| Test Loss: 0.3753569396531299\n",
      "Epoch: 4326 \t|| Train Loss: 0.16053624430241437 \t|| Test Loss: 0.3753154296554397\n",
      "Epoch: 4327 \t|| Train Loss: 0.16050155621136702 \t|| Test Loss: 0.3752052296581327\n",
      "Epoch: 4328 \t|| Train Loss: 0.1604673433053431 \t|| Test Loss: 0.37516371966044243\n",
      "Epoch: 4329 \t|| Train Loss: 0.1604330362147123 \t|| Test Loss: 0.3750535196631355\n",
      "Epoch: 4330 \t|| Train Loss: 0.16039844230827177 \t|| Test Loss: 0.3750120096654452\n",
      "Epoch: 4331 \t|| Train Loss: 0.16036451621805753 \t|| Test Loss: 0.37490180966813813\n",
      "Epoch: 4332 \t|| Train Loss: 0.1603296162198412 \t|| Test Loss: 0.3747916096708312\n",
      "Epoch: 4333 \t|| Train Loss: 0.16029592131276205 \t|| Test Loss: 0.3747500996731409\n",
      "Epoch: 4334 \t|| Train Loss: 0.1602610962231865 \t|| Test Loss: 0.37463989967583394\n",
      "Epoch: 4335 \t|| Train Loss: 0.16022702031569072 \t|| Test Loss: 0.3745983896781437\n",
      "Epoch: 4336 \t|| Train Loss: 0.16019257622653177 \t|| Test Loss: 0.37448818968083664\n",
      "Epoch: 4337 \t|| Train Loss: 0.1601581193186194 \t|| Test Loss: 0.37444667968314643\n",
      "Epoch: 4338 \t|| Train Loss: 0.16012405622987705 \t|| Test Loss: 0.37433647968583944\n",
      "Epoch: 4339 \t|| Train Loss: 0.1600892183215481 \t|| Test Loss: 0.3742949696881491\n",
      "Epoch: 4340 \t|| Train Loss: 0.1600555362332223 \t|| Test Loss: 0.37418476969084213\n",
      "Epoch: 4341 \t|| Train Loss: 0.160020636235006 \t|| Test Loss: 0.37407456969353525\n",
      "Epoch: 4342 \t|| Train Loss: 0.15998669732603837 \t|| Test Loss: 0.374033059695845\n",
      "Epoch: 4343 \t|| Train Loss: 0.15995211623835126 \t|| Test Loss: 0.373922859698538\n",
      "Epoch: 4344 \t|| Train Loss: 0.15991779632896708 \t|| Test Loss: 0.37388134970084774\n",
      "Epoch: 4345 \t|| Train Loss: 0.15988359624169654 \t|| Test Loss: 0.37377114970354064\n",
      "Epoch: 4346 \t|| Train Loss: 0.15984889533189578 \t|| Test Loss: 0.3737296397058505\n",
      "Epoch: 4347 \t|| Train Loss: 0.15981507624504182 \t|| Test Loss: 0.3736194397085434\n",
      "Epoch: 4348 \t|| Train Loss: 0.15978017624682547 \t|| Test Loss: 0.3735092397112364\n",
      "Epoch: 4349 \t|| Train Loss: 0.15974637433638605 \t|| Test Loss: 0.37346772971354614\n",
      "Epoch: 4350 \t|| Train Loss: 0.15971165625017075 \t|| Test Loss: 0.3733575297162392\n",
      "Epoch: 4351 \t|| Train Loss: 0.15967747333931476 \t|| Test Loss: 0.37331601971854894\n",
      "Epoch: 4352 \t|| Train Loss: 0.159643136253516 \t|| Test Loss: 0.37320581972124195\n",
      "Epoch: 4353 \t|| Train Loss: 0.1596085723422434 \t|| Test Loss: 0.37316430972355175\n",
      "Epoch: 4354 \t|| Train Loss: 0.15957461625686128 \t|| Test Loss: 0.3730541097262447\n",
      "Epoch: 4355 \t|| Train Loss: 0.15953971625864496 \t|| Test Loss: 0.37294390972893765\n",
      "Epoch: 4356 \t|| Train Loss: 0.15950605134673368 \t|| Test Loss: 0.3729023997312474\n",
      "Epoch: 4357 \t|| Train Loss: 0.15947119626199027 \t|| Test Loss: 0.37279219973394045\n",
      "Epoch: 4358 \t|| Train Loss: 0.15943715034966238 \t|| Test Loss: 0.3727506897362502\n",
      "Epoch: 4359 \t|| Train Loss: 0.1594026762653355 \t|| Test Loss: 0.3726404897389432\n",
      "Epoch: 4360 \t|| Train Loss: 0.15936824935259106 \t|| Test Loss: 0.37259897974125294\n",
      "Epoch: 4361 \t|| Train Loss: 0.1593341562686808 \t|| Test Loss: 0.372488779743946\n",
      "Epoch: 4362 \t|| Train Loss: 0.15929934835551976 \t|| Test Loss: 0.37244726974625575\n",
      "Epoch: 4363 \t|| Train Loss: 0.15926563627202606 \t|| Test Loss: 0.3723370697489487\n",
      "Epoch: 4364 \t|| Train Loss: 0.15923073627380974 \t|| Test Loss: 0.3722268697516417\n",
      "Epoch: 4365 \t|| Train Loss: 0.15919682736001003 \t|| Test Loss: 0.37218535975395145\n",
      "Epoch: 4366 \t|| Train Loss: 0.15916221627715502 \t|| Test Loss: 0.37207515975664446\n",
      "Epoch: 4367 \t|| Train Loss: 0.15912792636293874 \t|| Test Loss: 0.3720336497589542\n",
      "Epoch: 4368 \t|| Train Loss: 0.15909369628050027 \t|| Test Loss: 0.37192344976164715\n",
      "Epoch: 4369 \t|| Train Loss: 0.1590590253658674 \t|| Test Loss: 0.3718819397639569\n",
      "Epoch: 4370 \t|| Train Loss: 0.15902517628384555 \t|| Test Loss: 0.3717717397666499\n",
      "Epoch: 4371 \t|| Train Loss: 0.15899027628562923 \t|| Test Loss: 0.3716615397693429\n",
      "Epoch: 4372 \t|| Train Loss: 0.15895650437035766 \t|| Test Loss: 0.3716200297716527\n",
      "Epoch: 4373 \t|| Train Loss: 0.1589217562889745 \t|| Test Loss: 0.37150982977434566\n",
      "Epoch: 4374 \t|| Train Loss: 0.15888760337328636 \t|| Test Loss: 0.3714683197766555\n",
      "Epoch: 4375 \t|| Train Loss: 0.15885323629231976 \t|| Test Loss: 0.3713581197793484\n",
      "Epoch: 4376 \t|| Train Loss: 0.15881870237621504 \t|| Test Loss: 0.37131660978165815\n",
      "Epoch: 4377 \t|| Train Loss: 0.15878471629566504 \t|| Test Loss: 0.37120640978435115\n",
      "Epoch: 4378 \t|| Train Loss: 0.15874981629744872 \t|| Test Loss: 0.37109620978704416\n",
      "Epoch: 4379 \t|| Train Loss: 0.15871618138070534 \t|| Test Loss: 0.3710546997893539\n",
      "Epoch: 4380 \t|| Train Loss: 0.158681296300794 \t|| Test Loss: 0.3709444997920469\n",
      "Epoch: 4381 \t|| Train Loss: 0.158647280383634 \t|| Test Loss: 0.37090298979435665\n",
      "Epoch: 4382 \t|| Train Loss: 0.15861277630413925 \t|| Test Loss: 0.37079278979704966\n",
      "Epoch: 4383 \t|| Train Loss: 0.15857837938656272 \t|| Test Loss: 0.37075127979935946\n",
      "Epoch: 4384 \t|| Train Loss: 0.15854425630748453 \t|| Test Loss: 0.3706410798020524\n",
      "Epoch: 4385 \t|| Train Loss: 0.15850947838949142 \t|| Test Loss: 0.3705995698043622\n",
      "Epoch: 4386 \t|| Train Loss: 0.1584757363108298 \t|| Test Loss: 0.37048936980705516\n",
      "Epoch: 4387 \t|| Train Loss: 0.15844083631261346 \t|| Test Loss: 0.3703791698097482\n",
      "Epoch: 4388 \t|| Train Loss: 0.15840695739398167 \t|| Test Loss: 0.3703376598120579\n",
      "Epoch: 4389 \t|| Train Loss: 0.15837231631595874 \t|| Test Loss: 0.37022745981475097\n",
      "Epoch: 4390 \t|| Train Loss: 0.15833805639691034 \t|| Test Loss: 0.3701859498170607\n",
      "Epoch: 4391 \t|| Train Loss: 0.15830379631930405 \t|| Test Loss: 0.3700757498197537\n",
      "Epoch: 4392 \t|| Train Loss: 0.15826915539983905 \t|| Test Loss: 0.37003423982206346\n",
      "Epoch: 4393 \t|| Train Loss: 0.15823527632264928 \t|| Test Loss: 0.3699240398247564\n",
      "Epoch: 4394 \t|| Train Loss: 0.15820037632443298 \t|| Test Loss: 0.3698138398274495\n",
      "Epoch: 4395 \t|| Train Loss: 0.15816663440432938 \t|| Test Loss: 0.3697723298297592\n",
      "Epoch: 4396 \t|| Train Loss: 0.15813185632777824 \t|| Test Loss: 0.3696621298324522\n",
      "Epoch: 4397 \t|| Train Loss: 0.158097733407258 \t|| Test Loss: 0.36962061983476197\n",
      "Epoch: 4398 \t|| Train Loss: 0.15806333633112352 \t|| Test Loss: 0.369510419837455\n",
      "Epoch: 4399 \t|| Train Loss: 0.15802883241018667 \t|| Test Loss: 0.36946890983976477\n",
      "Epoch: 4400 \t|| Train Loss: 0.1579948163344688 \t|| Test Loss: 0.3693587098424577\n",
      "Epoch: 4401 \t|| Train Loss: 0.1579599314131154 \t|| Test Loss: 0.3693171998447675\n",
      "Epoch: 4402 \t|| Train Loss: 0.15792629633781408 \t|| Test Loss: 0.36920699984746047\n",
      "Epoch: 4403 \t|| Train Loss: 0.15789139633959773 \t|| Test Loss: 0.3690967998501534\n",
      "Epoch: 4404 \t|| Train Loss: 0.15785741041760568 \t|| Test Loss: 0.36905528985246316\n",
      "Epoch: 4405 \t|| Train Loss: 0.15782287634294298 \t|| Test Loss: 0.36894508985515617\n",
      "Epoch: 4406 \t|| Train Loss: 0.15778850942053435 \t|| Test Loss: 0.3689035798574659\n",
      "Epoch: 4407 \t|| Train Loss: 0.1577543563462883 \t|| Test Loss: 0.3687933798601589\n",
      "Epoch: 4408 \t|| Train Loss: 0.15771960842346305 \t|| Test Loss: 0.36875186986246866\n",
      "Epoch: 4409 \t|| Train Loss: 0.15768583634963357 \t|| Test Loss: 0.36864166986516167\n",
      "Epoch: 4410 \t|| Train Loss: 0.15765093635141722 \t|| Test Loss: 0.3685314698678547\n",
      "Epoch: 4411 \t|| Train Loss: 0.1576170874279533 \t|| Test Loss: 0.3684899598701644\n",
      "Epoch: 4412 \t|| Train Loss: 0.1575824163547625 \t|| Test Loss: 0.3683797598728574\n",
      "Epoch: 4413 \t|| Train Loss: 0.157548186430882 \t|| Test Loss: 0.36833824987516717\n",
      "Epoch: 4414 \t|| Train Loss: 0.1575138963581078 \t|| Test Loss: 0.3682280498778602\n",
      "Epoch: 4415 \t|| Train Loss: 0.15747928543381068 \t|| Test Loss: 0.36818653988016997\n",
      "Epoch: 4416 \t|| Train Loss: 0.15744537636145306 \t|| Test Loss: 0.3680763398828629\n",
      "Epoch: 4417 \t|| Train Loss: 0.1574104763632367 \t|| Test Loss: 0.367966139885556\n",
      "Epoch: 4418 \t|| Train Loss: 0.15737676443830095 \t|| Test Loss: 0.36792462988786573\n",
      "Epoch: 4419 \t|| Train Loss: 0.15734195636658196 \t|| Test Loss: 0.36781442989055874\n",
      "Epoch: 4420 \t|| Train Loss: 0.15730786344122963 \t|| Test Loss: 0.3677729198928685\n",
      "Epoch: 4421 \t|| Train Loss: 0.15727343636992724 \t|| Test Loss: 0.3676627198955615\n",
      "Epoch: 4422 \t|| Train Loss: 0.15723896244415836 \t|| Test Loss: 0.3676212098978712\n",
      "Epoch: 4423 \t|| Train Loss: 0.15720491637327255 \t|| Test Loss: 0.3675110099005642\n",
      "Epoch: 4424 \t|| Train Loss: 0.15717006144708703 \t|| Test Loss: 0.367469499902874\n",
      "Epoch: 4425 \t|| Train Loss: 0.1571363963766178 \t|| Test Loss: 0.3673592999055669\n",
      "Epoch: 4426 \t|| Train Loss: 0.15710149637840148 \t|| Test Loss: 0.36724909990826\n",
      "Epoch: 4427 \t|| Train Loss: 0.1570675404515773 \t|| Test Loss: 0.36720758991056973\n",
      "Epoch: 4428 \t|| Train Loss: 0.15703297638174676 \t|| Test Loss: 0.3670973899132627\n",
      "Epoch: 4429 \t|| Train Loss: 0.15699863945450598 \t|| Test Loss: 0.3670558799155725\n",
      "Epoch: 4430 \t|| Train Loss: 0.15696445638509204 \t|| Test Loss: 0.36694567991826543\n",
      "Epoch: 4431 \t|| Train Loss: 0.1569297384574347 \t|| Test Loss: 0.36690416992057523\n",
      "Epoch: 4432 \t|| Train Loss: 0.15689593638843732 \t|| Test Loss: 0.3667939699232682\n",
      "Epoch: 4433 \t|| Train Loss: 0.15686103639022098 \t|| Test Loss: 0.36668376992596124\n",
      "Epoch: 4434 \t|| Train Loss: 0.15682721746192496 \t|| Test Loss: 0.36664225992827093\n",
      "Epoch: 4435 \t|| Train Loss: 0.15679251639356623 \t|| Test Loss: 0.36653205993096394\n",
      "Epoch: 4436 \t|| Train Loss: 0.1567583164648536 \t|| Test Loss: 0.3664905499332737\n",
      "Epoch: 4437 \t|| Train Loss: 0.1567239963969115 \t|| Test Loss: 0.3663803499359667\n",
      "Epoch: 4438 \t|| Train Loss: 0.1566894154677823 \t|| Test Loss: 0.36633883993827643\n",
      "Epoch: 4439 \t|| Train Loss: 0.1566554764002568 \t|| Test Loss: 0.36622863994096944\n",
      "Epoch: 4440 \t|| Train Loss: 0.15662057640204047 \t|| Test Loss: 0.36611843994366244\n",
      "Epoch: 4441 \t|| Train Loss: 0.15658689447227264 \t|| Test Loss: 0.3660769299459722\n",
      "Epoch: 4442 \t|| Train Loss: 0.15655205640538572 \t|| Test Loss: 0.36596672994866525\n",
      "Epoch: 4443 \t|| Train Loss: 0.15651799347520132 \t|| Test Loss: 0.36592521995097493\n",
      "Epoch: 4444 \t|| Train Loss: 0.15648353640873097 \t|| Test Loss: 0.36581501995366794\n",
      "Epoch: 4445 \t|| Train Loss: 0.15644909247813002 \t|| Test Loss: 0.3657735099559777\n",
      "Epoch: 4446 \t|| Train Loss: 0.15641501641207628 \t|| Test Loss: 0.36566330995867075\n",
      "Epoch: 4447 \t|| Train Loss: 0.15638019148105867 \t|| Test Loss: 0.36562179996098043\n",
      "Epoch: 4448 \t|| Train Loss: 0.1563464964154216 \t|| Test Loss: 0.36551159996367344\n",
      "Epoch: 4449 \t|| Train Loss: 0.1563115964172052 \t|| Test Loss: 0.36540139996636645\n",
      "Epoch: 4450 \t|| Train Loss: 0.15627767048554894 \t|| Test Loss: 0.36535988996867624\n",
      "Epoch: 4451 \t|| Train Loss: 0.15624307642055052 \t|| Test Loss: 0.3652496899713692\n",
      "Epoch: 4452 \t|| Train Loss: 0.15620876948847764 \t|| Test Loss: 0.36520817997367894\n",
      "Epoch: 4453 \t|| Train Loss: 0.15617455642389577 \t|| Test Loss: 0.36509797997637194\n",
      "Epoch: 4454 \t|| Train Loss: 0.15613986849140632 \t|| Test Loss: 0.3650564699786817\n",
      "Epoch: 4455 \t|| Train Loss: 0.15610603642724105 \t|| Test Loss: 0.3649462699813747\n",
      "Epoch: 4456 \t|| Train Loss: 0.15607113642902473 \t|| Test Loss: 0.3648360699840677\n",
      "Epoch: 4457 \t|| Train Loss: 0.15603734749589662 \t|| Test Loss: 0.3647945599863775\n",
      "Epoch: 4458 \t|| Train Loss: 0.15600261643236996 \t|| Test Loss: 0.36468435998907045\n",
      "Epoch: 4459 \t|| Train Loss: 0.15596844649882527 \t|| Test Loss: 0.3646428499913802\n",
      "Epoch: 4460 \t|| Train Loss: 0.15593409643571526 \t|| Test Loss: 0.3645326499940732\n",
      "Epoch: 4461 \t|| Train Loss: 0.155899545501754 \t|| Test Loss: 0.36449113999638294\n",
      "Epoch: 4462 \t|| Train Loss: 0.15586557643906054 \t|| Test Loss: 0.36438093999907595\n",
      "Epoch: 4463 \t|| Train Loss: 0.1558306764408442 \t|| Test Loss: 0.36427074000176896\n",
      "Epoch: 4464 \t|| Train Loss: 0.15579702450624427 \t|| Test Loss: 0.36422923000407875\n",
      "Epoch: 4465 \t|| Train Loss: 0.15576215644418948 \t|| Test Loss: 0.3641190300067717\n",
      "Epoch: 4466 \t|| Train Loss: 0.15572812350917295 \t|| Test Loss: 0.36407752000908145\n",
      "Epoch: 4467 \t|| Train Loss: 0.15569363644753476 \t|| Test Loss: 0.3639673200117745\n",
      "Epoch: 4468 \t|| Train Loss: 0.15565922251210162 \t|| Test Loss: 0.3639258100140842\n",
      "Epoch: 4469 \t|| Train Loss: 0.15562511645088004 \t|| Test Loss: 0.3638156100167772\n",
      "Epoch: 4470 \t|| Train Loss: 0.15559032151503033 \t|| Test Loss: 0.36377410001908694\n",
      "Epoch: 4471 \t|| Train Loss: 0.15555659645422532 \t|| Test Loss: 0.36366390002177995\n",
      "Epoch: 4472 \t|| Train Loss: 0.15552169645600897 \t|| Test Loss: 0.363553700024473\n",
      "Epoch: 4473 \t|| Train Loss: 0.1554878005195206 \t|| Test Loss: 0.3635121900267828\n",
      "Epoch: 4474 \t|| Train Loss: 0.15545317645935425 \t|| Test Loss: 0.3634019900294757\n",
      "Epoch: 4475 \t|| Train Loss: 0.15541889952244928 \t|| Test Loss: 0.3633604800317855\n",
      "Epoch: 4476 \t|| Train Loss: 0.1553846564626995 \t|| Test Loss: 0.36325028003447846\n",
      "Epoch: 4477 \t|| Train Loss: 0.15534999852537798 \t|| Test Loss: 0.3632087700367882\n",
      "Epoch: 4478 \t|| Train Loss: 0.15531613646604478 \t|| Test Loss: 0.3630985700394812\n",
      "Epoch: 4479 \t|| Train Loss: 0.15528123646782846 \t|| Test Loss: 0.3629883700421742\n",
      "Epoch: 4480 \t|| Train Loss: 0.15524747752986828 \t|| Test Loss: 0.36294686004448395\n",
      "Epoch: 4481 \t|| Train Loss: 0.15521271647117377 \t|| Test Loss: 0.362836660047177\n",
      "Epoch: 4482 \t|| Train Loss: 0.15517857653279696 \t|| Test Loss: 0.3627951500494867\n",
      "Epoch: 4483 \t|| Train Loss: 0.155144196474519 \t|| Test Loss: 0.3626849500521797\n",
      "Epoch: 4484 \t|| Train Loss: 0.1551096755357256 \t|| Test Loss: 0.36264344005448945\n",
      "Epoch: 4485 \t|| Train Loss: 0.1550756764778643 \t|| Test Loss: 0.36253324005718246\n",
      "Epoch: 4486 \t|| Train Loss: 0.15504077647964795 \t|| Test Loss: 0.36242304005987547\n",
      "Epoch: 4487 \t|| Train Loss: 0.1550071545402159 \t|| Test Loss: 0.3623815300621852\n",
      "Epoch: 4488 \t|| Train Loss: 0.15497225648299323 \t|| Test Loss: 0.3622713300648782\n",
      "Epoch: 4489 \t|| Train Loss: 0.15493825354314458 \t|| Test Loss: 0.36222982006718796\n",
      "Epoch: 4490 \t|| Train Loss: 0.1549037364863385 \t|| Test Loss: 0.36211962006988097\n",
      "Epoch: 4491 \t|| Train Loss: 0.15486935254607326 \t|| Test Loss: 0.3620781100721907\n",
      "Epoch: 4492 \t|| Train Loss: 0.15483521648968376 \t|| Test Loss: 0.3619679100748837\n",
      "Epoch: 4493 \t|| Train Loss: 0.15480045154900196 \t|| Test Loss: 0.36192640007719346\n",
      "Epoch: 4494 \t|| Train Loss: 0.15476669649302904 \t|| Test Loss: 0.3618162000798865\n",
      "Epoch: 4495 \t|| Train Loss: 0.15473179649481272 \t|| Test Loss: 0.3617060000825795\n",
      "Epoch: 4496 \t|| Train Loss: 0.15469793055349226 \t|| Test Loss: 0.3616644900848892\n",
      "Epoch: 4497 \t|| Train Loss: 0.15466327649815798 \t|| Test Loss: 0.3615542900875822\n",
      "Epoch: 4498 \t|| Train Loss: 0.1546290295564209 \t|| Test Loss: 0.36151278008989196\n",
      "Epoch: 4499 \t|| Train Loss: 0.15459475650150326 \t|| Test Loss: 0.36140258009258497\n",
      "Epoch: 4500 \t|| Train Loss: 0.15456012855934959 \t|| Test Loss: 0.3613610700948947\n",
      "Epoch: 4501 \t|| Train Loss: 0.15452623650484854 \t|| Test Loss: 0.3612508700975877\n",
      "Epoch: 4502 \t|| Train Loss: 0.15449133650663222 \t|| Test Loss: 0.3611406701002807\n",
      "Epoch: 4503 \t|| Train Loss: 0.15445760756383992 \t|| Test Loss: 0.36109916010259047\n",
      "Epoch: 4504 \t|| Train Loss: 0.15442281650997747 \t|| Test Loss: 0.3609889601052835\n",
      "Epoch: 4505 \t|| Train Loss: 0.15438870656676856 \t|| Test Loss: 0.3609474501075932\n",
      "Epoch: 4506 \t|| Train Loss: 0.15435429651332275 \t|| Test Loss: 0.3608372501102862\n",
      "Epoch: 4507 \t|| Train Loss: 0.15431980556969727 \t|| Test Loss: 0.36079574011259596\n",
      "Epoch: 4508 \t|| Train Loss: 0.15428577651666803 \t|| Test Loss: 0.36068554011528897\n",
      "Epoch: 4509 \t|| Train Loss: 0.15425090457262597 \t|| Test Loss: 0.3606440301175987\n",
      "Epoch: 4510 \t|| Train Loss: 0.1542172565200133 \t|| Test Loss: 0.3605338301202917\n",
      "Epoch: 4511 \t|| Train Loss: 0.15418235652179696 \t|| Test Loss: 0.36042363012298473\n",
      "Epoch: 4512 \t|| Train Loss: 0.15414838357711622 \t|| Test Loss: 0.36038212012529447\n",
      "Epoch: 4513 \t|| Train Loss: 0.15411383652514227 \t|| Test Loss: 0.3602719201279875\n",
      "Epoch: 4514 \t|| Train Loss: 0.15407948258004495 \t|| Test Loss: 0.3602304101302972\n",
      "Epoch: 4515 \t|| Train Loss: 0.1540453165284875 \t|| Test Loss: 0.3601202101329902\n",
      "Epoch: 4516 \t|| Train Loss: 0.1540105815829736 \t|| Test Loss: 0.36007870013529997\n",
      "Epoch: 4517 \t|| Train Loss: 0.15397679653183277 \t|| Test Loss: 0.359968500137993\n",
      "Epoch: 4518 \t|| Train Loss: 0.15394189653361648 \t|| Test Loss: 0.35985830014068604\n",
      "Epoch: 4519 \t|| Train Loss: 0.15390806058746387 \t|| Test Loss: 0.3598167901429957\n",
      "Epoch: 4520 \t|| Train Loss: 0.15387337653696173 \t|| Test Loss: 0.35970659014568873\n",
      "Epoch: 4521 \t|| Train Loss: 0.15383915959039257 \t|| Test Loss: 0.3596650801479985\n",
      "Epoch: 4522 \t|| Train Loss: 0.153804856540307 \t|| Test Loss: 0.3595548801506915\n",
      "Epoch: 4523 \t|| Train Loss: 0.15377025859332125 \t|| Test Loss: 0.3595133701530012\n",
      "Epoch: 4524 \t|| Train Loss: 0.1537363365436523 \t|| Test Loss: 0.3594031701556943\n",
      "Epoch: 4525 \t|| Train Loss: 0.15370143654543594 \t|| Test Loss: 0.3592929701583873\n",
      "Epoch: 4526 \t|| Train Loss: 0.15366773759781155 \t|| Test Loss: 0.359251460160697\n",
      "Epoch: 4527 \t|| Train Loss: 0.15363291654878125 \t|| Test Loss: 0.35914126016339\n",
      "Epoch: 4528 \t|| Train Loss: 0.15359883660074022 \t|| Test Loss: 0.3590997501656997\n",
      "Epoch: 4529 \t|| Train Loss: 0.15356439655212648 \t|| Test Loss: 0.35898955016839273\n",
      "Epoch: 4530 \t|| Train Loss: 0.15352993560366893 \t|| Test Loss: 0.3589480401707025\n",
      "Epoch: 4531 \t|| Train Loss: 0.15349587655547176 \t|| Test Loss: 0.3588378401733955\n",
      "Epoch: 4532 \t|| Train Loss: 0.1534610346065976 \t|| Test Loss: 0.3587963301757052\n",
      "Epoch: 4533 \t|| Train Loss: 0.15342735655881706 \t|| Test Loss: 0.35868613017839823\n",
      "Epoch: 4534 \t|| Train Loss: 0.15339245656060072 \t|| Test Loss: 0.35857593018109124\n",
      "Epoch: 4535 \t|| Train Loss: 0.15335851361108788 \t|| Test Loss: 0.358534420183401\n",
      "Epoch: 4536 \t|| Train Loss: 0.153323936563946 \t|| Test Loss: 0.358424220186094\n",
      "Epoch: 4537 \t|| Train Loss: 0.15328961261401658 \t|| Test Loss: 0.35838271018840373\n",
      "Epoch: 4538 \t|| Train Loss: 0.1532554165672913 \t|| Test Loss: 0.35827251019109674\n",
      "Epoch: 4539 \t|| Train Loss: 0.15322071161694523 \t|| Test Loss: 0.3582310001934065\n",
      "Epoch: 4540 \t|| Train Loss: 0.15318689657063655 \t|| Test Loss: 0.3581208001960995\n",
      "Epoch: 4541 \t|| Train Loss: 0.15315199657242018 \t|| Test Loss: 0.3580106001987925\n",
      "Epoch: 4542 \t|| Train Loss: 0.15311819062143553 \t|| Test Loss: 0.35796909020110224\n",
      "Epoch: 4543 \t|| Train Loss: 0.15308347657576546 \t|| Test Loss: 0.35785889020379524\n",
      "Epoch: 4544 \t|| Train Loss: 0.1530492896243642 \t|| Test Loss: 0.357817380206105\n",
      "Epoch: 4545 \t|| Train Loss: 0.15301495657911074 \t|| Test Loss: 0.357707180208798\n",
      "Epoch: 4546 \t|| Train Loss: 0.15298038862729288 \t|| Test Loss: 0.35766567021110773\n",
      "Epoch: 4547 \t|| Train Loss: 0.15294643658245602 \t|| Test Loss: 0.35755547021380074\n",
      "Epoch: 4548 \t|| Train Loss: 0.15291153658423973 \t|| Test Loss: 0.35744527021649375\n",
      "Epoch: 4549 \t|| Train Loss: 0.15287786763178318 \t|| Test Loss: 0.35740376021880355\n",
      "Epoch: 4550 \t|| Train Loss: 0.15284301658758498 \t|| Test Loss: 0.3572935602214965\n",
      "Epoch: 4551 \t|| Train Loss: 0.15280896663471186 \t|| Test Loss: 0.3572520502238063\n",
      "Epoch: 4552 \t|| Train Loss: 0.15277449659093026 \t|| Test Loss: 0.35714185022649925\n",
      "Epoch: 4553 \t|| Train Loss: 0.1527400656376406 \t|| Test Loss: 0.357100340228809\n",
      "Epoch: 4554 \t|| Train Loss: 0.1527059765942755 \t|| Test Loss: 0.356990140231502\n",
      "Epoch: 4555 \t|| Train Loss: 0.15267116464056923 \t|| Test Loss: 0.3569486302338118\n",
      "Epoch: 4556 \t|| Train Loss: 0.1526374565976208 \t|| Test Loss: 0.35683843023650474\n",
      "Epoch: 4557 \t|| Train Loss: 0.15260255659940447 \t|| Test Loss: 0.35672823023919775\n",
      "Epoch: 4558 \t|| Train Loss: 0.1525686436450595 \t|| Test Loss: 0.3566867202415075\n",
      "Epoch: 4559 \t|| Train Loss: 0.15253403660274972 \t|| Test Loss: 0.35657652024420056\n",
      "Epoch: 4560 \t|| Train Loss: 0.1524997426479882 \t|| Test Loss: 0.35653501024651024\n",
      "Epoch: 4561 \t|| Train Loss: 0.152465516606095 \t|| Test Loss: 0.35642481024920325\n",
      "Epoch: 4562 \t|| Train Loss: 0.1524308416509169 \t|| Test Loss: 0.356383300251513\n",
      "Epoch: 4563 \t|| Train Loss: 0.15239699660944028 \t|| Test Loss: 0.356273100254206\n",
      "Epoch: 4564 \t|| Train Loss: 0.15236209661122396 \t|| Test Loss: 0.356162900256899\n",
      "Epoch: 4565 \t|| Train Loss: 0.15232832065540716 \t|| Test Loss: 0.35612139025920875\n",
      "Epoch: 4566 \t|| Train Loss: 0.15229357661456927 \t|| Test Loss: 0.35601119026190176\n",
      "Epoch: 4567 \t|| Train Loss: 0.15225941965833586 \t|| Test Loss: 0.3559696802642115\n",
      "Epoch: 4568 \t|| Train Loss: 0.1522250566179145 \t|| Test Loss: 0.3558594802669045\n",
      "Epoch: 4569 \t|| Train Loss: 0.15219051866126454 \t|| Test Loss: 0.35581797026921425\n",
      "Epoch: 4570 \t|| Train Loss: 0.15215653662125975 \t|| Test Loss: 0.35570777027190725\n",
      "Epoch: 4571 \t|| Train Loss: 0.15212163662304343 \t|| Test Loss: 0.35559757027460026\n",
      "Epoch: 4572 \t|| Train Loss: 0.15208799766575481 \t|| Test Loss: 0.35555606027691\n",
      "Epoch: 4573 \t|| Train Loss: 0.1520531166263887 \t|| Test Loss: 0.355445860279603\n",
      "Epoch: 4574 \t|| Train Loss: 0.1520190966686835 \t|| Test Loss: 0.35540435028191275\n",
      "Epoch: 4575 \t|| Train Loss: 0.15198459662973401 \t|| Test Loss: 0.35529415028460576\n",
      "Epoch: 4576 \t|| Train Loss: 0.1519501956716122 \t|| Test Loss: 0.3552526402869155\n",
      "Epoch: 4577 \t|| Train Loss: 0.1519160766330793 \t|| Test Loss: 0.3551424402896085\n",
      "Epoch: 4578 \t|| Train Loss: 0.15188129467454087 \t|| Test Loss: 0.35510093029191825\n",
      "Epoch: 4579 \t|| Train Loss: 0.15184755663642452 \t|| Test Loss: 0.35499073029461126\n",
      "Epoch: 4580 \t|| Train Loss: 0.1518126566382082 \t|| Test Loss: 0.35488053029730426\n",
      "Epoch: 4581 \t|| Train Loss: 0.1517787736790312 \t|| Test Loss: 0.35483902029961406\n",
      "Epoch: 4582 \t|| Train Loss: 0.15174413664155348 \t|| Test Loss: 0.354728820302307\n",
      "Epoch: 4583 \t|| Train Loss: 0.15170987268195985 \t|| Test Loss: 0.35468731030461675\n",
      "Epoch: 4584 \t|| Train Loss: 0.15167561664489876 \t|| Test Loss: 0.35457711030730976\n",
      "Epoch: 4585 \t|| Train Loss: 0.15164097168488852 \t|| Test Loss: 0.3545356003096195\n",
      "Epoch: 4586 \t|| Train Loss: 0.15160709664824404 \t|| Test Loss: 0.3544254003123125\n",
      "Epoch: 4587 \t|| Train Loss: 0.1515721966500277 \t|| Test Loss: 0.3543152003150055\n",
      "Epoch: 4588 \t|| Train Loss: 0.15153845068937882 \t|| Test Loss: 0.35427369031731526\n",
      "Epoch: 4589 \t|| Train Loss: 0.151503676653373 \t|| Test Loss: 0.35416349032000827\n",
      "Epoch: 4590 \t|| Train Loss: 0.1514695496923075 \t|| Test Loss: 0.35412198032231795\n",
      "Epoch: 4591 \t|| Train Loss: 0.15143515665671825 \t|| Test Loss: 0.354011780325011\n",
      "Epoch: 4592 \t|| Train Loss: 0.15140064869523617 \t|| Test Loss: 0.35397027032732076\n",
      "Epoch: 4593 \t|| Train Loss: 0.15136663666006353 \t|| Test Loss: 0.35386007033001377\n",
      "Epoch: 4594 \t|| Train Loss: 0.15133174769816488 \t|| Test Loss: 0.3538185603323235\n",
      "Epoch: 4595 \t|| Train Loss: 0.1512981166634088 \t|| Test Loss: 0.3537083603350165\n",
      "Epoch: 4596 \t|| Train Loss: 0.15126321666519246 \t|| Test Loss: 0.3535981603377095\n",
      "Epoch: 4597 \t|| Train Loss: 0.1512292267026552 \t|| Test Loss: 0.35355665034001926\n",
      "Epoch: 4598 \t|| Train Loss: 0.15119469666853774 \t|| Test Loss: 0.35344645034271227\n",
      "Epoch: 4599 \t|| Train Loss: 0.15116032570558385 \t|| Test Loss: 0.353404940345022\n",
      "Epoch: 4600 \t|| Train Loss: 0.151126176671883 \t|| Test Loss: 0.353294740347715\n",
      "Epoch: 4601 \t|| Train Loss: 0.1510914247085125 \t|| Test Loss: 0.35325323035002476\n",
      "Epoch: 4602 \t|| Train Loss: 0.15105765667522827 \t|| Test Loss: 0.35314303035271777\n",
      "Epoch: 4603 \t|| Train Loss: 0.15102275667701198 \t|| Test Loss: 0.3530328303554108\n",
      "Epoch: 4604 \t|| Train Loss: 0.1509889037130028 \t|| Test Loss: 0.3529913203577205\n",
      "Epoch: 4605 \t|| Train Loss: 0.15095423668035723 \t|| Test Loss: 0.3528811203604135\n",
      "Epoch: 4606 \t|| Train Loss: 0.1509200027159315 \t|| Test Loss: 0.35283961036272327\n",
      "Epoch: 4607 \t|| Train Loss: 0.1508857166837025 \t|| Test Loss: 0.3527294103654163\n",
      "Epoch: 4608 \t|| Train Loss: 0.15085110171886018 \t|| Test Loss: 0.352687900367726\n",
      "Epoch: 4609 \t|| Train Loss: 0.1508171966870478 \t|| Test Loss: 0.352577700370419\n",
      "Epoch: 4610 \t|| Train Loss: 0.15078229668883142 \t|| Test Loss: 0.35246750037311203\n",
      "Epoch: 4611 \t|| Train Loss: 0.15074858072335046 \t|| Test Loss: 0.35242599037542177\n",
      "Epoch: 4612 \t|| Train Loss: 0.15071377669217673 \t|| Test Loss: 0.3523157903781148\n",
      "Epoch: 4613 \t|| Train Loss: 0.15067967972627913 \t|| Test Loss: 0.3522742803804245\n",
      "Epoch: 4614 \t|| Train Loss: 0.150645256695522 \t|| Test Loss: 0.35216408038311753\n",
      "Epoch: 4615 \t|| Train Loss: 0.15061077872920783 \t|| Test Loss: 0.35212257038542727\n",
      "Epoch: 4616 \t|| Train Loss: 0.15057673669886726 \t|| Test Loss: 0.3520123703881203\n",
      "Epoch: 4617 \t|| Train Loss: 0.1505418777321365 \t|| Test Loss: 0.3519708603904301\n",
      "Epoch: 4618 \t|| Train Loss: 0.15050821670221254 \t|| Test Loss: 0.351860660393123\n",
      "Epoch: 4619 \t|| Train Loss: 0.15047331670399622 \t|| Test Loss: 0.35175046039581603\n",
      "Epoch: 4620 \t|| Train Loss: 0.1504393567366268 \t|| Test Loss: 0.3517089503981258\n",
      "Epoch: 4621 \t|| Train Loss: 0.1504047967073415 \t|| Test Loss: 0.3515987504008188\n",
      "Epoch: 4622 \t|| Train Loss: 0.1503704557395555 \t|| Test Loss: 0.3515572404031285\n",
      "Epoch: 4623 \t|| Train Loss: 0.15033627671068675 \t|| Test Loss: 0.35144704040582153\n",
      "Epoch: 4624 \t|| Train Loss: 0.1503015547424842 \t|| Test Loss: 0.3514055304081313\n",
      "Epoch: 4625 \t|| Train Loss: 0.150267756714032 \t|| Test Loss: 0.3512953304108243\n",
      "Epoch: 4626 \t|| Train Loss: 0.1502328567158157 \t|| Test Loss: 0.3511851304135173\n",
      "Epoch: 4627 \t|| Train Loss: 0.15019903374697446 \t|| Test Loss: 0.35114362041582703\n",
      "Epoch: 4628 \t|| Train Loss: 0.15016433671916096 \t|| Test Loss: 0.35103342041852004\n",
      "Epoch: 4629 \t|| Train Loss: 0.1501301327499031 \t|| Test Loss: 0.3509919104208298\n",
      "Epoch: 4630 \t|| Train Loss: 0.15009581672250624 \t|| Test Loss: 0.3508817104235228\n",
      "Epoch: 4631 \t|| Train Loss: 0.15006123175283181 \t|| Test Loss: 0.3508402004258325\n",
      "Epoch: 4632 \t|| Train Loss: 0.15002729672585152 \t|| Test Loss: 0.35073000042852553\n",
      "Epoch: 4633 \t|| Train Loss: 0.1499923967276352 \t|| Test Loss: 0.35061980043121854\n",
      "Epoch: 4634 \t|| Train Loss: 0.1499587107573221 \t|| Test Loss: 0.3505782904335283\n",
      "Epoch: 4635 \t|| Train Loss: 0.14992387673098048 \t|| Test Loss: 0.3504680904362213\n",
      "Epoch: 4636 \t|| Train Loss: 0.14988980976025082 \t|| Test Loss: 0.35042658043853103\n",
      "Epoch: 4637 \t|| Train Loss: 0.1498553567343257 \t|| Test Loss: 0.35031638044122404\n",
      "Epoch: 4638 \t|| Train Loss: 0.1498209087631795 \t|| Test Loss: 0.3502748704435338\n",
      "Epoch: 4639 \t|| Train Loss: 0.14978683673767101 \t|| Test Loss: 0.3501646704462268\n",
      "Epoch: 4640 \t|| Train Loss: 0.14975200776610814 \t|| Test Loss: 0.35012316044853653\n",
      "Epoch: 4641 \t|| Train Loss: 0.14971831674101627 \t|| Test Loss: 0.35001296045122954\n",
      "Epoch: 4642 \t|| Train Loss: 0.14968341674279997 \t|| Test Loss: 0.34990276045392255\n",
      "Epoch: 4643 \t|| Train Loss: 0.14964948677059847 \t|| Test Loss: 0.3498612504562323\n",
      "Epoch: 4644 \t|| Train Loss: 0.14961489674614523 \t|| Test Loss: 0.3497510504589253\n",
      "Epoch: 4645 \t|| Train Loss: 0.14958058577352712 \t|| Test Loss: 0.34970954046123504\n",
      "Epoch: 4646 \t|| Train Loss: 0.1495463767494905 \t|| Test Loss: 0.34959934046392804\n",
      "Epoch: 4647 \t|| Train Loss: 0.1495116847764558 \t|| Test Loss: 0.3495578304662378\n",
      "Epoch: 4648 \t|| Train Loss: 0.14947785675283579 \t|| Test Loss: 0.3494476304689308\n",
      "Epoch: 4649 \t|| Train Loss: 0.14944295675461944 \t|| Test Loss: 0.3493374304716238\n",
      "Epoch: 4650 \t|| Train Loss: 0.14940916378094612 \t|| Test Loss: 0.34929592047393354\n",
      "Epoch: 4651 \t|| Train Loss: 0.14937443675796475 \t|| Test Loss: 0.34918572047662655\n",
      "Epoch: 4652 \t|| Train Loss: 0.14934026278387477 \t|| Test Loss: 0.3491442104789363\n",
      "Epoch: 4653 \t|| Train Loss: 0.14930591676130997 \t|| Test Loss: 0.3490340104816293\n",
      "Epoch: 4654 \t|| Train Loss: 0.14927136178680345 \t|| Test Loss: 0.34899250048393904\n",
      "Epoch: 4655 \t|| Train Loss: 0.14923739676465525 \t|| Test Loss: 0.34888230048663205\n",
      "Epoch: 4656 \t|| Train Loss: 0.14920249676643893 \t|| Test Loss: 0.348772100489325\n",
      "Epoch: 4657 \t|| Train Loss: 0.14916884079129375 \t|| Test Loss: 0.3487305904916348\n",
      "Epoch: 4658 \t|| Train Loss: 0.14913397676978418 \t|| Test Loss: 0.3486203904943278\n",
      "Epoch: 4659 \t|| Train Loss: 0.14909993979422245 \t|| Test Loss: 0.34857888049663754\n",
      "Epoch: 4660 \t|| Train Loss: 0.1490654567731295 \t|| Test Loss: 0.34846868049933055\n",
      "Epoch: 4661 \t|| Train Loss: 0.14903103879715113 \t|| Test Loss: 0.3484271705016403\n",
      "Epoch: 4662 \t|| Train Loss: 0.1489969367764748 \t|| Test Loss: 0.34831697050433325\n",
      "Epoch: 4663 \t|| Train Loss: 0.14896213780007983 \t|| Test Loss: 0.34827546050664304\n",
      "Epoch: 4664 \t|| Train Loss: 0.14892841677982005 \t|| Test Loss: 0.348165260509336\n",
      "Epoch: 4665 \t|| Train Loss: 0.14889351678160373 \t|| Test Loss: 0.34805506051202906\n",
      "Epoch: 4666 \t|| Train Loss: 0.1488596168045701 \t|| Test Loss: 0.3480135505143388\n",
      "Epoch: 4667 \t|| Train Loss: 0.14882499678494898 \t|| Test Loss: 0.3479033505170318\n",
      "Epoch: 4668 \t|| Train Loss: 0.14879071580749875 \t|| Test Loss: 0.34786184051934155\n",
      "Epoch: 4669 \t|| Train Loss: 0.14875647678829426 \t|| Test Loss: 0.34775164052203456\n",
      "Epoch: 4670 \t|| Train Loss: 0.14872181481042748 \t|| Test Loss: 0.3477101305243443\n",
      "Epoch: 4671 \t|| Train Loss: 0.14868795679163954 \t|| Test Loss: 0.3475999305270373\n",
      "Epoch: 4672 \t|| Train Loss: 0.14865305679342322 \t|| Test Loss: 0.3474897305297303\n",
      "Epoch: 4673 \t|| Train Loss: 0.14861929381491773 \t|| Test Loss: 0.34744822053204005\n",
      "Epoch: 4674 \t|| Train Loss: 0.14858453679676847 \t|| Test Loss: 0.34733802053473306\n",
      "Epoch: 4675 \t|| Train Loss: 0.14855039281784646 \t|| Test Loss: 0.3472965105370428\n",
      "Epoch: 4676 \t|| Train Loss: 0.14851601680011375 \t|| Test Loss: 0.3471863105397358\n",
      "Epoch: 4677 \t|| Train Loss: 0.14848149182077514 \t|| Test Loss: 0.34714480054204555\n",
      "Epoch: 4678 \t|| Train Loss: 0.148447496803459 \t|| Test Loss: 0.34703460054473856\n",
      "Epoch: 4679 \t|| Train Loss: 0.14841259680524269 \t|| Test Loss: 0.34692440054743157\n",
      "Epoch: 4680 \t|| Train Loss: 0.1483789708252654 \t|| Test Loss: 0.3468828905497413\n",
      "Epoch: 4681 \t|| Train Loss: 0.14834407680858794 \t|| Test Loss: 0.3467726905524343\n",
      "Epoch: 4682 \t|| Train Loss: 0.14831006982819409 \t|| Test Loss: 0.34673118055474406\n",
      "Epoch: 4683 \t|| Train Loss: 0.14827555681193322 \t|| Test Loss: 0.34662098055743706\n",
      "Epoch: 4684 \t|| Train Loss: 0.14824116883112276 \t|| Test Loss: 0.3465794705597468\n",
      "Epoch: 4685 \t|| Train Loss: 0.1482070368152785 \t|| Test Loss: 0.3464692705624398\n",
      "Epoch: 4686 \t|| Train Loss: 0.14817226783405144 \t|| Test Loss: 0.34642776056474955\n",
      "Epoch: 4687 \t|| Train Loss: 0.14813851681862378 \t|| Test Loss: 0.34631756056744256\n",
      "Epoch: 4688 \t|| Train Loss: 0.14810361682040746 \t|| Test Loss: 0.34620736057013557\n",
      "Epoch: 4689 \t|| Train Loss: 0.14806974683854174 \t|| Test Loss: 0.34616585057244526\n",
      "Epoch: 4690 \t|| Train Loss: 0.1480350968237527 \t|| Test Loss: 0.3460556505751383\n",
      "Epoch: 4691 \t|| Train Loss: 0.1480008458414704 \t|| Test Loss: 0.34601414057744806\n",
      "Epoch: 4692 \t|| Train Loss: 0.14796657682709796 \t|| Test Loss: 0.34590394058014107\n",
      "Epoch: 4693 \t|| Train Loss: 0.1479319448443991 \t|| Test Loss: 0.3458624305824508\n",
      "Epoch: 4694 \t|| Train Loss: 0.14789805683044327 \t|| Test Loss: 0.3457522305851438\n",
      "Epoch: 4695 \t|| Train Loss: 0.14786315683222695 \t|| Test Loss: 0.3456420305878368\n",
      "Epoch: 4696 \t|| Train Loss: 0.1478294238488894 \t|| Test Loss: 0.34560052059014656\n",
      "Epoch: 4697 \t|| Train Loss: 0.1477946368355722 \t|| Test Loss: 0.3454903205928396\n",
      "Epoch: 4698 \t|| Train Loss: 0.1477605228518181 \t|| Test Loss: 0.3454488105951493\n",
      "Epoch: 4699 \t|| Train Loss: 0.1477261168389175 \t|| Test Loss: 0.3453386105978423\n",
      "Epoch: 4700 \t|| Train Loss: 0.14769162185474677 \t|| Test Loss: 0.34529710060015206\n",
      "Epoch: 4701 \t|| Train Loss: 0.14765759684226276 \t|| Test Loss: 0.345186900602845\n",
      "Epoch: 4702 \t|| Train Loss: 0.14762272085767544 \t|| Test Loss: 0.34514539060515476\n",
      "Epoch: 4703 \t|| Train Loss: 0.147589076845608 \t|| Test Loss: 0.3450351906078478\n",
      "Epoch: 4704 \t|| Train Loss: 0.1475541768473917 \t|| Test Loss: 0.3449249906105408\n",
      "Epoch: 4705 \t|| Train Loss: 0.14752019986216572 \t|| Test Loss: 0.34488348061285057\n",
      "Epoch: 4706 \t|| Train Loss: 0.147485656850737 \t|| Test Loss: 0.3447732806155436\n",
      "Epoch: 4707 \t|| Train Loss: 0.14745129886509442 \t|| Test Loss: 0.3447317706178533\n",
      "Epoch: 4708 \t|| Train Loss: 0.14741713685408225 \t|| Test Loss: 0.3446215706205463\n",
      "Epoch: 4709 \t|| Train Loss: 0.1473823978680231 \t|| Test Loss: 0.34458006062285607\n",
      "Epoch: 4710 \t|| Train Loss: 0.14734861685742753 \t|| Test Loss: 0.344469860625549\n",
      "Epoch: 4711 \t|| Train Loss: 0.14731371685921119 \t|| Test Loss: 0.3443596606282421\n",
      "Epoch: 4712 \t|| Train Loss: 0.1472798768725134 \t|| Test Loss: 0.3443181506305518\n",
      "Epoch: 4713 \t|| Train Loss: 0.14724519686255647 \t|| Test Loss: 0.34420795063324483\n",
      "Epoch: 4714 \t|| Train Loss: 0.14721097587544207 \t|| Test Loss: 0.34416644063555457\n",
      "Epoch: 4715 \t|| Train Loss: 0.14717667686590175 \t|| Test Loss: 0.3440562406382476\n",
      "Epoch: 4716 \t|| Train Loss: 0.14714207487837078 \t|| Test Loss: 0.3440147306405573\n",
      "Epoch: 4717 \t|| Train Loss: 0.14710815686924703 \t|| Test Loss: 0.3439045306432503\n",
      "Epoch: 4718 \t|| Train Loss: 0.14707325687103068 \t|| Test Loss: 0.3437943306459433\n",
      "Epoch: 4719 \t|| Train Loss: 0.14703955388286102 \t|| Test Loss: 0.343752820648253\n",
      "Epoch: 4720 \t|| Train Loss: 0.14700473687437596 \t|| Test Loss: 0.34364262065094603\n",
      "Epoch: 4721 \t|| Train Loss: 0.14697065288578973 \t|| Test Loss: 0.3436011106532558\n",
      "Epoch: 4722 \t|| Train Loss: 0.14693621687772127 \t|| Test Loss: 0.34349091065594883\n",
      "Epoch: 4723 \t|| Train Loss: 0.1469017518887184 \t|| Test Loss: 0.3434494006582586\n",
      "Epoch: 4724 \t|| Train Loss: 0.14686769688106652 \t|| Test Loss: 0.3433392006609516\n",
      "Epoch: 4725 \t|| Train Loss: 0.14683285089164708 \t|| Test Loss: 0.3432976906632613\n",
      "Epoch: 4726 \t|| Train Loss: 0.14679917688441177 \t|| Test Loss: 0.34318749066595433\n",
      "Epoch: 4727 \t|| Train Loss: 0.14676427688619548 \t|| Test Loss: 0.34307729066864734\n",
      "Epoch: 4728 \t|| Train Loss: 0.14673032989613738 \t|| Test Loss: 0.3430357806709571\n",
      "Epoch: 4729 \t|| Train Loss: 0.1466957568895407 \t|| Test Loss: 0.3429255806736501\n",
      "Epoch: 4730 \t|| Train Loss: 0.14666142889906605 \t|| Test Loss: 0.34288407067595983\n",
      "Epoch: 4731 \t|| Train Loss: 0.146627236892886 \t|| Test Loss: 0.34277387067865284\n",
      "Epoch: 4732 \t|| Train Loss: 0.14659252790199476 \t|| Test Loss: 0.3427323606809626\n",
      "Epoch: 4733 \t|| Train Loss: 0.1465587168962313 \t|| Test Loss: 0.3426221606836556\n",
      "Epoch: 4734 \t|| Train Loss: 0.14652381689801494 \t|| Test Loss: 0.3425119606863486\n",
      "Epoch: 4735 \t|| Train Loss: 0.14649000690648503 \t|| Test Loss: 0.34247045068865833\n",
      "Epoch: 4736 \t|| Train Loss: 0.14645529690136022 \t|| Test Loss: 0.34236025069135134\n",
      "Epoch: 4737 \t|| Train Loss: 0.14642110590941373 \t|| Test Loss: 0.3423187406936611\n",
      "Epoch: 4738 \t|| Train Loss: 0.1463867769047055 \t|| Test Loss: 0.3422085406963541\n",
      "Epoch: 4739 \t|| Train Loss: 0.1463522049123424 \t|| Test Loss: 0.3421670306986639\n",
      "Epoch: 4740 \t|| Train Loss: 0.14631825690805073 \t|| Test Loss: 0.34205683070135684\n",
      "Epoch: 4741 \t|| Train Loss: 0.14628335690983446 \t|| Test Loss: 0.34194663070404985\n",
      "Epoch: 4742 \t|| Train Loss: 0.14624968391683268 \t|| Test Loss: 0.3419051207063596\n",
      "Epoch: 4743 \t|| Train Loss: 0.1462148369131797 \t|| Test Loss: 0.3417949207090526\n",
      "Epoch: 4744 \t|| Train Loss: 0.14618078291976136 \t|| Test Loss: 0.34175341071136234\n",
      "Epoch: 4745 \t|| Train Loss: 0.146146316916525 \t|| Test Loss: 0.34164321071405535\n",
      "Epoch: 4746 \t|| Train Loss: 0.14611188192269006 \t|| Test Loss: 0.3416017007163651\n",
      "Epoch: 4747 \t|| Train Loss: 0.14607779691987027 \t|| Test Loss: 0.34149150071905815\n",
      "Epoch: 4748 \t|| Train Loss: 0.14604298092561876 \t|| Test Loss: 0.34144999072136784\n",
      "Epoch: 4749 \t|| Train Loss: 0.14600927692321555 \t|| Test Loss: 0.3413397907240609\n",
      "Epoch: 4750 \t|| Train Loss: 0.1459743769249992 \t|| Test Loss: 0.34122959072675385\n",
      "Epoch: 4751 \t|| Train Loss: 0.145940459930109 \t|| Test Loss: 0.34118808072906365\n",
      "Epoch: 4752 \t|| Train Loss: 0.1459058569283445 \t|| Test Loss: 0.3410778807317566\n",
      "Epoch: 4753 \t|| Train Loss: 0.14587155893303771 \t|| Test Loss: 0.34103637073406634\n",
      "Epoch: 4754 \t|| Train Loss: 0.14583733693168974 \t|| Test Loss: 0.34092617073675935\n",
      "Epoch: 4755 \t|| Train Loss: 0.14580265793596642 \t|| Test Loss: 0.3408846607390691\n",
      "Epoch: 4756 \t|| Train Loss: 0.14576881693503502 \t|| Test Loss: 0.34077446074176215\n",
      "Epoch: 4757 \t|| Train Loss: 0.14573391693681872 \t|| Test Loss: 0.34066426074445516\n",
      "Epoch: 4758 \t|| Train Loss: 0.14570013694045666 \t|| Test Loss: 0.34062275074676496\n",
      "Epoch: 4759 \t|| Train Loss: 0.14566539694016398 \t|| Test Loss: 0.34051255074945785\n",
      "Epoch: 4760 \t|| Train Loss: 0.14563123594338537 \t|| Test Loss: 0.3404710407517676\n",
      "Epoch: 4761 \t|| Train Loss: 0.14559687694350926 \t|| Test Loss: 0.3403608407544606\n",
      "Epoch: 4762 \t|| Train Loss: 0.14556233494631404 \t|| Test Loss: 0.34031933075677034\n",
      "Epoch: 4763 \t|| Train Loss: 0.14552835694685454 \t|| Test Loss: 0.34020913075946335\n",
      "Epoch: 4764 \t|| Train Loss: 0.1454934569486382 \t|| Test Loss: 0.3400989307621564\n",
      "Epoch: 4765 \t|| Train Loss: 0.14545981395080437 \t|| Test Loss: 0.3400574207644661\n",
      "Epoch: 4766 \t|| Train Loss: 0.1454249369519835 \t|| Test Loss: 0.3399472207671592\n",
      "Epoch: 4767 \t|| Train Loss: 0.14539091295373302 \t|| Test Loss: 0.3399057107694689\n",
      "Epoch: 4768 \t|| Train Loss: 0.14535641695532875 \t|| Test Loss: 0.3397955107721619\n",
      "Epoch: 4769 \t|| Train Loss: 0.1453220119566617 \t|| Test Loss: 0.3397540007744717\n",
      "Epoch: 4770 \t|| Train Loss: 0.145287896958674 \t|| Test Loss: 0.3396438007771646\n",
      "Epoch: 4771 \t|| Train Loss: 0.14525311095959034 \t|| Test Loss: 0.3396022907794744\n",
      "Epoch: 4772 \t|| Train Loss: 0.1452193769620193 \t|| Test Loss: 0.3394920907821674\n",
      "Epoch: 4773 \t|| Train Loss: 0.14518447696380296 \t|| Test Loss: 0.3393818907848604\n",
      "Epoch: 4774 \t|| Train Loss: 0.14515058996408065 \t|| Test Loss: 0.33934038078717016\n",
      "Epoch: 4775 \t|| Train Loss: 0.14511595696714824 \t|| Test Loss: 0.3392301807898632\n",
      "Epoch: 4776 \t|| Train Loss: 0.14508168896700938 \t|| Test Loss: 0.3391886707921729\n",
      "Epoch: 4777 \t|| Train Loss: 0.14504743697049352 \t|| Test Loss: 0.33907847079486597\n",
      "Epoch: 4778 \t|| Train Loss: 0.14501278796993805 \t|| Test Loss: 0.33903696079717566\n",
      "Epoch: 4779 \t|| Train Loss: 0.14497891697383877 \t|| Test Loss: 0.33892676079986866\n",
      "Epoch: 4780 \t|| Train Loss: 0.14494401697562245 \t|| Test Loss: 0.3388165608025617\n",
      "Epoch: 4781 \t|| Train Loss: 0.14491026697442833 \t|| Test Loss: 0.3387750508048714\n",
      "Epoch: 4782 \t|| Train Loss: 0.1448754969789677 \t|| Test Loss: 0.3386648508075645\n",
      "Epoch: 4783 \t|| Train Loss: 0.144841365977357 \t|| Test Loss: 0.3386233408098742\n",
      "Epoch: 4784 \t|| Train Loss: 0.144806976982313 \t|| Test Loss: 0.33851314081256717\n",
      "Epoch: 4785 \t|| Train Loss: 0.14477246498028568 \t|| Test Loss: 0.33847163081487697\n",
      "Epoch: 4786 \t|| Train Loss: 0.14473845698565826 \t|| Test Loss: 0.3383614308175699\n",
      "Epoch: 4787 \t|| Train Loss: 0.14470356398321438 \t|| Test Loss: 0.33831992081987966\n",
      "Epoch: 4788 \t|| Train Loss: 0.14466993698900357 \t|| Test Loss: 0.33820972082257267\n",
      "Epoch: 4789 \t|| Train Loss: 0.14463503699078722 \t|| Test Loss: 0.33809952082526573\n",
      "Epoch: 4790 \t|| Train Loss: 0.14460104298770468 \t|| Test Loss: 0.3380580108275754\n",
      "Epoch: 4791 \t|| Train Loss: 0.14456651699413245 \t|| Test Loss: 0.3379478108302685\n",
      "Epoch: 4792 \t|| Train Loss: 0.14453214199063336 \t|| Test Loss: 0.3379063008325783\n",
      "Epoch: 4793 \t|| Train Loss: 0.14449799699747778 \t|| Test Loss: 0.33779610083527123\n",
      "Epoch: 4794 \t|| Train Loss: 0.14446324099356206 \t|| Test Loss: 0.337754590837581\n",
      "Epoch: 4795 \t|| Train Loss: 0.14442947700082306 \t|| Test Loss: 0.337644390840274\n",
      "Epoch: 4796 \t|| Train Loss: 0.14439457700260672 \t|| Test Loss: 0.337534190842967\n",
      "Epoch: 4797 \t|| Train Loss: 0.14436071999805233 \t|| Test Loss: 0.3374926808452767\n",
      "Epoch: 4798 \t|| Train Loss: 0.14432605700595197 \t|| Test Loss: 0.33738248084796973\n",
      "Epoch: 4799 \t|| Train Loss: 0.144291819000981 \t|| Test Loss: 0.3373409708502795\n",
      "Epoch: 4800 \t|| Train Loss: 0.14425753700929725 \t|| Test Loss: 0.3372307708529725\n",
      "Epoch: 4801 \t|| Train Loss: 0.14422291800390968 \t|| Test Loss: 0.3371892608552822\n",
      "Epoch: 4802 \t|| Train Loss: 0.14418901701264253 \t|| Test Loss: 0.33707906085797523\n",
      "Epoch: 4803 \t|| Train Loss: 0.1441541170144262 \t|| Test Loss: 0.33696886086066824\n",
      "Epoch: 4804 \t|| Train Loss: 0.14412039700839996 \t|| Test Loss: 0.336927350862978\n",
      "Epoch: 4805 \t|| Train Loss: 0.14408559701777152 \t|| Test Loss: 0.336817150865671\n",
      "Epoch: 4806 \t|| Train Loss: 0.1440514960113287 \t|| Test Loss: 0.33677564086798073\n",
      "Epoch: 4807 \t|| Train Loss: 0.14401707702111677 \t|| Test Loss: 0.33666544087067385\n",
      "Epoch: 4808 \t|| Train Loss: 0.14398259501425736 \t|| Test Loss: 0.3366239308729835\n",
      "Epoch: 4809 \t|| Train Loss: 0.14394855702446202 \t|| Test Loss: 0.33651373087567654\n",
      "Epoch: 4810 \t|| Train Loss: 0.14391369401718607 \t|| Test Loss: 0.33647222087798623\n",
      "Epoch: 4811 \t|| Train Loss: 0.1438800370278073 \t|| Test Loss: 0.33636202088067924\n",
      "Epoch: 4812 \t|| Train Loss: 0.14384513702959095 \t|| Test Loss: 0.33625182088337224\n",
      "Epoch: 4813 \t|| Train Loss: 0.14381117302167631 \t|| Test Loss: 0.33621031088568204\n",
      "Epoch: 4814 \t|| Train Loss: 0.14377661703293626 \t|| Test Loss: 0.336100110888375\n",
      "Epoch: 4815 \t|| Train Loss: 0.14374227202460502 \t|| Test Loss: 0.33605860089068473\n",
      "Epoch: 4816 \t|| Train Loss: 0.14370809703628154 \t|| Test Loss: 0.33594840089337774\n",
      "Epoch: 4817 \t|| Train Loss: 0.1436733710275337 \t|| Test Loss: 0.3359068908956875\n",
      "Epoch: 4818 \t|| Train Loss: 0.1436395770396268 \t|| Test Loss: 0.3357966908983805\n",
      "Epoch: 4819 \t|| Train Loss: 0.1436046770414105 \t|| Test Loss: 0.3356864909010735\n",
      "Epoch: 4820 \t|| Train Loss: 0.143570850032024 \t|| Test Loss: 0.3356449809033833\n",
      "Epoch: 4821 \t|| Train Loss: 0.14353615704475572 \t|| Test Loss: 0.3355347809060763\n",
      "Epoch: 4822 \t|| Train Loss: 0.14350194903495267 \t|| Test Loss: 0.335493270908386\n",
      "Epoch: 4823 \t|| Train Loss: 0.14346763704810103 \t|| Test Loss: 0.335383070911079\n",
      "Epoch: 4824 \t|| Train Loss: 0.14343304803788132 \t|| Test Loss: 0.3353415609133888\n",
      "Epoch: 4825 \t|| Train Loss: 0.1433991170514463 \t|| Test Loss: 0.33523136091608174\n",
      "Epoch: 4826 \t|| Train Loss: 0.14336421705322996 \t|| Test Loss: 0.3351211609187748\n",
      "Epoch: 4827 \t|| Train Loss: 0.14333052704237165 \t|| Test Loss: 0.33507965092108455\n",
      "Epoch: 4828 \t|| Train Loss: 0.14329569705657524 \t|| Test Loss: 0.33496945092377756\n",
      "Epoch: 4829 \t|| Train Loss: 0.14326162604530032 \t|| Test Loss: 0.3349279409260873\n",
      "Epoch: 4830 \t|| Train Loss: 0.1432271770599205 \t|| Test Loss: 0.3348177409287803\n",
      "Epoch: 4831 \t|| Train Loss: 0.143192725048229 \t|| Test Loss: 0.33477623093109005\n",
      "Epoch: 4832 \t|| Train Loss: 0.14315865706326578 \t|| Test Loss: 0.33466603093378305\n",
      "Epoch: 4833 \t|| Train Loss: 0.1431238240511577 \t|| Test Loss: 0.33462452093609274\n",
      "Epoch: 4834 \t|| Train Loss: 0.14309013706661106 \t|| Test Loss: 0.3345143209387858\n",
      "Epoch: 4835 \t|| Train Loss: 0.1430552370683947 \t|| Test Loss: 0.3344041209414788\n",
      "Epoch: 4836 \t|| Train Loss: 0.14302130305564797 \t|| Test Loss: 0.3343626109437885\n",
      "Epoch: 4837 \t|| Train Loss: 0.14298671707174002 \t|| Test Loss: 0.3342524109464815\n",
      "Epoch: 4838 \t|| Train Loss: 0.14295240205857668 \t|| Test Loss: 0.33421090094879136\n",
      "Epoch: 4839 \t|| Train Loss: 0.14291819707508527 \t|| Test Loss: 0.33410070095148436\n",
      "Epoch: 4840 \t|| Train Loss: 0.14288350106150532 \t|| Test Loss: 0.3340591909537941\n",
      "Epoch: 4841 \t|| Train Loss: 0.14284967707843055 \t|| Test Loss: 0.3339489909564871\n",
      "Epoch: 4842 \t|| Train Loss: 0.14281477708021423 \t|| Test Loss: 0.3338387909591801\n",
      "Epoch: 4843 \t|| Train Loss: 0.14278098006599563 \t|| Test Loss: 0.33379728096148975\n",
      "Epoch: 4844 \t|| Train Loss: 0.1427462570835595 \t|| Test Loss: 0.3336870809641828\n",
      "Epoch: 4845 \t|| Train Loss: 0.1427120790689243 \t|| Test Loss: 0.3336455709664926\n",
      "Epoch: 4846 \t|| Train Loss: 0.14267773708690476 \t|| Test Loss: 0.3335353709691856\n",
      "Epoch: 4847 \t|| Train Loss: 0.142643178071853 \t|| Test Loss: 0.33349386097149525\n",
      "Epoch: 4848 \t|| Train Loss: 0.14260921709025004 \t|| Test Loss: 0.3333836609741883\n",
      "Epoch: 4849 \t|| Train Loss: 0.14257431709203372 \t|| Test Loss: 0.3332734609768813\n",
      "Epoch: 4850 \t|| Train Loss: 0.14254065707634328 \t|| Test Loss: 0.33323195097919106\n",
      "Epoch: 4851 \t|| Train Loss: 0.14250579709537897 \t|| Test Loss: 0.33312175098188407\n",
      "Epoch: 4852 \t|| Train Loss: 0.14247175607927196 \t|| Test Loss: 0.3330802409841938\n",
      "Epoch: 4853 \t|| Train Loss: 0.14243727709872428 \t|| Test Loss: 0.3329700409868869\n",
      "Epoch: 4854 \t|| Train Loss: 0.14240285508220066 \t|| Test Loss: 0.33292853098919656\n",
      "Epoch: 4855 \t|| Train Loss: 0.14236875710206953 \t|| Test Loss: 0.33281833099188957\n",
      "Epoch: 4856 \t|| Train Loss: 0.14233395408512933 \t|| Test Loss: 0.3327768209941993\n",
      "Epoch: 4857 \t|| Train Loss: 0.14230023710541484 \t|| Test Loss: 0.3326666209968923\n",
      "Epoch: 4858 \t|| Train Loss: 0.14226533710719846 \t|| Test Loss: 0.3325564209995853\n",
      "Epoch: 4859 \t|| Train Loss: 0.14223143308961964 \t|| Test Loss: 0.33251491100189506\n",
      "Epoch: 4860 \t|| Train Loss: 0.14219681711054374 \t|| Test Loss: 0.33240471100458807\n",
      "Epoch: 4861 \t|| Train Loss: 0.14216253209254828 \t|| Test Loss: 0.3323632010068978\n",
      "Epoch: 4862 \t|| Train Loss: 0.14212829711388902 \t|| Test Loss: 0.3322530010095908\n",
      "Epoch: 4863 \t|| Train Loss: 0.142093631095477 \t|| Test Loss: 0.33221149101190056\n",
      "Epoch: 4864 \t|| Train Loss: 0.1420597771172343 \t|| Test Loss: 0.33210129101459357\n",
      "Epoch: 4865 \t|| Train Loss: 0.14202487711901798 \t|| Test Loss: 0.3319910910172866\n",
      "Epoch: 4866 \t|| Train Loss: 0.1419911100999673 \t|| Test Loss: 0.3319495810195963\n",
      "Epoch: 4867 \t|| Train Loss: 0.14195635712236326 \t|| Test Loss: 0.3318393810222893\n",
      "Epoch: 4868 \t|| Train Loss: 0.14192220910289594 \t|| Test Loss: 0.33179787102459907\n",
      "Epoch: 4869 \t|| Train Loss: 0.14188783712570854 \t|| Test Loss: 0.3316876710272921\n",
      "Epoch: 4870 \t|| Train Loss: 0.14185330810582464 \t|| Test Loss: 0.33164616102960187\n",
      "Epoch: 4871 \t|| Train Loss: 0.1418193171290538 \t|| Test Loss: 0.3315359610322949\n",
      "Epoch: 4872 \t|| Train Loss: 0.14178441713083748 \t|| Test Loss: 0.3314257610349878\n",
      "Epoch: 4873 \t|| Train Loss: 0.1417507871103149 \t|| Test Loss: 0.33138425103729763\n",
      "Epoch: 4874 \t|| Train Loss: 0.14171589713418276 \t|| Test Loss: 0.33127405103999064\n",
      "Epoch: 4875 \t|| Train Loss: 0.14168188611324362 \t|| Test Loss: 0.3312325410423004\n",
      "Epoch: 4876 \t|| Train Loss: 0.141647377137528 \t|| Test Loss: 0.3311223410449933\n",
      "Epoch: 4877 \t|| Train Loss: 0.14161298511617232 \t|| Test Loss: 0.3310808310473031\n",
      "Epoch: 4878 \t|| Train Loss: 0.1415788571408733 \t|| Test Loss: 0.33097063104999613\n",
      "Epoch: 4879 \t|| Train Loss: 0.141544084119101 \t|| Test Loss: 0.3309291210523059\n",
      "Epoch: 4880 \t|| Train Loss: 0.14151033714421857 \t|| Test Loss: 0.3308189210549989\n",
      "Epoch: 4881 \t|| Train Loss: 0.14147543714600225 \t|| Test Loss: 0.3307087210576919\n",
      "Epoch: 4882 \t|| Train Loss: 0.1414415631235913 \t|| Test Loss: 0.33066721106000163\n",
      "Epoch: 4883 \t|| Train Loss: 0.1414069171493475 \t|| Test Loss: 0.33055701106269464\n",
      "Epoch: 4884 \t|| Train Loss: 0.14137266212651997 \t|| Test Loss: 0.3305155010650043\n",
      "Epoch: 4885 \t|| Train Loss: 0.14133839715269275 \t|| Test Loss: 0.3304053010676974\n",
      "Epoch: 4886 \t|| Train Loss: 0.14130376112944865 \t|| Test Loss: 0.3303637910700071\n",
      "Epoch: 4887 \t|| Train Loss: 0.14126987715603806 \t|| Test Loss: 0.3302535910727001\n",
      "Epoch: 4888 \t|| Train Loss: 0.1412349771578217 \t|| Test Loss: 0.33014339107539314\n",
      "Epoch: 4889 \t|| Train Loss: 0.14120124013393892 \t|| Test Loss: 0.33010188107770283\n",
      "Epoch: 4890 \t|| Train Loss: 0.141166457161167 \t|| Test Loss: 0.3299916810803959\n",
      "Epoch: 4891 \t|| Train Loss: 0.1411323391368676 \t|| Test Loss: 0.32995017108270563\n",
      "Epoch: 4892 \t|| Train Loss: 0.14109793716451224 \t|| Test Loss: 0.32983997108539864\n",
      "Epoch: 4893 \t|| Train Loss: 0.1410634381397963 \t|| Test Loss: 0.32979846108770833\n",
      "Epoch: 4894 \t|| Train Loss: 0.14102941716785755 \t|| Test Loss: 0.3296882610904014\n",
      "Epoch: 4895 \t|| Train Loss: 0.14099453714272497 \t|| Test Loss: 0.32964675109271113\n",
      "Epoch: 4896 \t|| Train Loss: 0.1409608971712028 \t|| Test Loss: 0.32953655109540414\n",
      "Epoch: 4897 \t|| Train Loss: 0.1409259971729865 \t|| Test Loss: 0.3294263510980971\n",
      "Epoch: 4898 \t|| Train Loss: 0.14089201614721528 \t|| Test Loss: 0.3293848411004069\n",
      "Epoch: 4899 \t|| Train Loss: 0.14085747717633174 \t|| Test Loss: 0.32927464110309984\n",
      "Epoch: 4900 \t|| Train Loss: 0.14082311515014395 \t|| Test Loss: 0.3292331311054096\n",
      "Epoch: 4901 \t|| Train Loss: 0.14078895717967704 \t|| Test Loss: 0.32912293110810265\n",
      "Epoch: 4902 \t|| Train Loss: 0.1407542141530726 \t|| Test Loss: 0.3290814211104124\n",
      "Epoch: 4903 \t|| Train Loss: 0.1407204371830223 \t|| Test Loss: 0.3289712211131054\n",
      "Epoch: 4904 \t|| Train Loss: 0.14068553718480598 \t|| Test Loss: 0.3288610211157984\n",
      "Epoch: 4905 \t|| Train Loss: 0.14065169315756293 \t|| Test Loss: 0.32881951111810814\n",
      "Epoch: 4906 \t|| Train Loss: 0.14061701718815128 \t|| Test Loss: 0.32870931112080115\n",
      "Epoch: 4907 \t|| Train Loss: 0.1405827921604916 \t|| Test Loss: 0.3286678011231109\n",
      "Epoch: 4908 \t|| Train Loss: 0.14054849719149654 \t|| Test Loss: 0.32855760112580384\n",
      "Epoch: 4909 \t|| Train Loss: 0.14051389116342028 \t|| Test Loss: 0.32851609112811364\n",
      "Epoch: 4910 \t|| Train Loss: 0.1404799771948418 \t|| Test Loss: 0.32840589113080665\n",
      "Epoch: 4911 \t|| Train Loss: 0.14044507719662547 \t|| Test Loss: 0.32829569113349966\n",
      "Epoch: 4912 \t|| Train Loss: 0.14041137016791055 \t|| Test Loss: 0.3282541811358094\n",
      "Epoch: 4913 \t|| Train Loss: 0.14037655719997075 \t|| Test Loss: 0.3281439811385024\n",
      "Epoch: 4914 \t|| Train Loss: 0.14034246917083926 \t|| Test Loss: 0.32810247114081215\n",
      "Epoch: 4915 \t|| Train Loss: 0.14030803720331603 \t|| Test Loss: 0.32799227114350515\n",
      "Epoch: 4916 \t|| Train Loss: 0.14027356817376793 \t|| Test Loss: 0.3279507611458149\n",
      "Epoch: 4917 \t|| Train Loss: 0.1402395172066613 \t|| Test Loss: 0.3278405611485079\n",
      "Epoch: 4918 \t|| Train Loss: 0.14020466717669663 \t|| Test Loss: 0.3277990511508176\n",
      "Epoch: 4919 \t|| Train Loss: 0.14017099721000656 \t|| Test Loss: 0.32768885115351065\n",
      "Epoch: 4920 \t|| Train Loss: 0.14013609721179027 \t|| Test Loss: 0.32757865115620366\n",
      "Epoch: 4921 \t|| Train Loss: 0.14010214618118694 \t|| Test Loss: 0.3275371411585134\n",
      "Epoch: 4922 \t|| Train Loss: 0.14006757721513555 \t|| Test Loss: 0.3274269411612064\n",
      "Epoch: 4923 \t|| Train Loss: 0.14003324518411558 \t|| Test Loss: 0.32738543116351615\n",
      "Epoch: 4924 \t|| Train Loss: 0.1399990572184808 \t|| Test Loss: 0.32727523116620916\n",
      "Epoch: 4925 \t|| Train Loss: 0.1399643441870443 \t|| Test Loss: 0.3272337211685189\n",
      "Epoch: 4926 \t|| Train Loss: 0.13993053722182608 \t|| Test Loss: 0.3271235211712119\n",
      "Epoch: 4927 \t|| Train Loss: 0.1398956372236097 \t|| Test Loss: 0.3270133211739049\n",
      "Epoch: 4928 \t|| Train Loss: 0.1398618231915346 \t|| Test Loss: 0.32697181117621466\n",
      "Epoch: 4929 \t|| Train Loss: 0.139827117226955 \t|| Test Loss: 0.32686161117890766\n",
      "Epoch: 4930 \t|| Train Loss: 0.13979292219446324 \t|| Test Loss: 0.3268201011812174\n",
      "Epoch: 4931 \t|| Train Loss: 0.1397585972303003 \t|| Test Loss: 0.3267099011839104\n",
      "Epoch: 4932 \t|| Train Loss: 0.1397240211973919 \t|| Test Loss: 0.32666839118622015\n",
      "Epoch: 4933 \t|| Train Loss: 0.13969007723364554 \t|| Test Loss: 0.32655819118891316\n",
      "Epoch: 4934 \t|| Train Loss: 0.13965517723542922 \t|| Test Loss: 0.32644799119160617\n",
      "Epoch: 4935 \t|| Train Loss: 0.13962150020188221 \t|| Test Loss: 0.3264064811939159\n",
      "Epoch: 4936 \t|| Train Loss: 0.1395866572387745 \t|| Test Loss: 0.3262962811966089\n",
      "Epoch: 4937 \t|| Train Loss: 0.13955259920481092 \t|| Test Loss: 0.32625477119891866\n",
      "Epoch: 4938 \t|| Train Loss: 0.13951813724211976 \t|| Test Loss: 0.32614457120161167\n",
      "Epoch: 4939 \t|| Train Loss: 0.1394836982077396 \t|| Test Loss: 0.3261030612039214\n",
      "Epoch: 4940 \t|| Train Loss: 0.13944961724546504 \t|| Test Loss: 0.3259928612066144\n",
      "Epoch: 4941 \t|| Train Loss: 0.13941479721066827 \t|| Test Loss: 0.32595135120892416\n",
      "Epoch: 4942 \t|| Train Loss: 0.13938109724881032 \t|| Test Loss: 0.32584115121161716\n",
      "Epoch: 4943 \t|| Train Loss: 0.139346197250594 \t|| Test Loss: 0.32573095121431017\n",
      "Epoch: 4944 \t|| Train Loss: 0.13931227621515857 \t|| Test Loss: 0.3256894412166199\n",
      "Epoch: 4945 \t|| Train Loss: 0.13927767725393925 \t|| Test Loss: 0.3255792412193129\n",
      "Epoch: 4946 \t|| Train Loss: 0.13924337521808722 \t|| Test Loss: 0.32553773122162266\n",
      "Epoch: 4947 \t|| Train Loss: 0.13920915725728455 \t|| Test Loss: 0.32542753122431567\n",
      "Epoch: 4948 \t|| Train Loss: 0.13917447422101592 \t|| Test Loss: 0.3253860212266254\n",
      "Epoch: 4949 \t|| Train Loss: 0.1391406372606298 \t|| Test Loss: 0.3252758212293184\n",
      "Epoch: 4950 \t|| Train Loss: 0.13910573726241346 \t|| Test Loss: 0.3251656212320114\n",
      "Epoch: 4951 \t|| Train Loss: 0.1390719532255062 \t|| Test Loss: 0.32512411123432117\n",
      "Epoch: 4952 \t|| Train Loss: 0.13903721726575877 \t|| Test Loss: 0.3250139112370142\n",
      "Epoch: 4953 \t|| Train Loss: 0.1390030522284349 \t|| Test Loss: 0.3249724012393239\n",
      "Epoch: 4954 \t|| Train Loss: 0.13896869726910405 \t|| Test Loss: 0.3248622012420169\n",
      "Epoch: 4955 \t|| Train Loss: 0.1389341512313636 \t|| Test Loss: 0.3248206912443267\n",
      "Epoch: 4956 \t|| Train Loss: 0.1389001772724493 \t|| Test Loss: 0.3247104912470197\n",
      "Epoch: 4957 \t|| Train Loss: 0.13886527727423298 \t|| Test Loss: 0.3246002912497127\n",
      "Epoch: 4958 \t|| Train Loss: 0.13883163023585388 \t|| Test Loss: 0.3245587812520224\n",
      "Epoch: 4959 \t|| Train Loss: 0.13879675727757826 \t|| Test Loss: 0.32444858125471543\n",
      "Epoch: 4960 \t|| Train Loss: 0.13876272923878258 \t|| Test Loss: 0.32440707125702517\n",
      "Epoch: 4961 \t|| Train Loss: 0.1387282372809235 \t|| Test Loss: 0.3242968712597182\n",
      "Epoch: 4962 \t|| Train Loss: 0.13869382824171123 \t|| Test Loss: 0.3242553612620279\n",
      "Epoch: 4963 \t|| Train Loss: 0.1386597172842688 \t|| Test Loss: 0.3241451612647209\n",
      "Epoch: 4964 \t|| Train Loss: 0.1386249272446399 \t|| Test Loss: 0.32410365126703067\n",
      "Epoch: 4965 \t|| Train Loss: 0.13859119728761407 \t|| Test Loss: 0.3239934512697237\n",
      "Epoch: 4966 \t|| Train Loss: 0.13855629728939772 \t|| Test Loss: 0.3238832512724167\n",
      "Epoch: 4967 \t|| Train Loss: 0.1385224062491302 \t|| Test Loss: 0.3238417412747264\n",
      "Epoch: 4968 \t|| Train Loss: 0.13848777729274303 \t|| Test Loss: 0.32373154127741943\n",
      "Epoch: 4969 \t|| Train Loss: 0.13845350525205888 \t|| Test Loss: 0.3236900312797292\n",
      "Epoch: 4970 \t|| Train Loss: 0.13841925729608828 \t|| Test Loss: 0.3235798312824222\n",
      "Epoch: 4971 \t|| Train Loss: 0.13838460425498758 \t|| Test Loss: 0.3235383212847319\n",
      "Epoch: 4972 \t|| Train Loss: 0.13835073729943356 \t|| Test Loss: 0.32342812128742493\n",
      "Epoch: 4973 \t|| Train Loss: 0.13831583730121724 \t|| Test Loss: 0.32331792129011794\n",
      "Epoch: 4974 \t|| Train Loss: 0.13828208325947788 \t|| Test Loss: 0.3232764112924277\n",
      "Epoch: 4975 \t|| Train Loss: 0.1382473173045625 \t|| Test Loss: 0.3231662112951207\n",
      "Epoch: 4976 \t|| Train Loss: 0.13821318226240656 \t|| Test Loss: 0.32312470129743043\n",
      "Epoch: 4977 \t|| Train Loss: 0.13817879730790777 \t|| Test Loss: 0.32301450130012344\n",
      "Epoch: 4978 \t|| Train Loss: 0.13814428126533523 \t|| Test Loss: 0.3229729913024332\n",
      "Epoch: 4979 \t|| Train Loss: 0.13811027731125308 \t|| Test Loss: 0.32286279130512624\n",
      "Epoch: 4980 \t|| Train Loss: 0.13807538026826394 \t|| Test Loss: 0.3228212813074359\n",
      "Epoch: 4981 \t|| Train Loss: 0.13804175731459833 \t|| Test Loss: 0.32271108131012893\n",
      "Epoch: 4982 \t|| Train Loss: 0.138006857316382 \t|| Test Loss: 0.32260088131282194\n",
      "Epoch: 4983 \t|| Train Loss: 0.1379728592727542 \t|| Test Loss: 0.3225593713151317\n",
      "Epoch: 4984 \t|| Train Loss: 0.13793833731972724 \t|| Test Loss: 0.32244917131782475\n",
      "Epoch: 4985 \t|| Train Loss: 0.13790395827568286 \t|| Test Loss: 0.32240766132013443\n",
      "Epoch: 4986 \t|| Train Loss: 0.13786981732307255 \t|| Test Loss: 0.3222974613228275\n",
      "Epoch: 4987 \t|| Train Loss: 0.13783505727861156 \t|| Test Loss: 0.3222559513251372\n",
      "Epoch: 4988 \t|| Train Loss: 0.13780129732641783 \t|| Test Loss: 0.3221457513278302\n",
      "Epoch: 4989 \t|| Train Loss: 0.13776639732820148 \t|| Test Loss: 0.3220355513305232\n",
      "Epoch: 4990 \t|| Train Loss: 0.13773253628310184 \t|| Test Loss: 0.32199404133283294\n",
      "Epoch: 4991 \t|| Train Loss: 0.13769787733154676 \t|| Test Loss: 0.32188384133552594\n",
      "Epoch: 4992 \t|| Train Loss: 0.13766363528603054 \t|| Test Loss: 0.3218423313378357\n",
      "Epoch: 4993 \t|| Train Loss: 0.13762935733489204 \t|| Test Loss: 0.3217321313405287\n",
      "Epoch: 4994 \t|| Train Loss: 0.13759473428895924 \t|| Test Loss: 0.32169062134283843\n",
      "Epoch: 4995 \t|| Train Loss: 0.13756083733823732 \t|| Test Loss: 0.32158042134553144\n",
      "Epoch: 4996 \t|| Train Loss: 0.13752593734002097 \t|| Test Loss: 0.32147022134822445\n",
      "Epoch: 4997 \t|| Train Loss: 0.13749221329344952 \t|| Test Loss: 0.3214287113505342\n",
      "Epoch: 4998 \t|| Train Loss: 0.13745741734336628 \t|| Test Loss: 0.3213185113532272\n",
      "Epoch: 4999 \t|| Train Loss: 0.13742331229637822 \t|| Test Loss: 0.32127700135553694\n",
      "Epoch: 5000 \t|| Train Loss: 0.13738889734671153 \t|| Test Loss: 0.32116680135822995\n",
      "Epoch: 5001 \t|| Train Loss: 0.13735441129930687 \t|| Test Loss: 0.3211252913605397\n",
      "Epoch: 5002 \t|| Train Loss: 0.1373203773500568 \t|| Test Loss: 0.32101509136323275\n",
      "Epoch: 5003 \t|| Train Loss: 0.13728551030223554 \t|| Test Loss: 0.3209735813655425\n",
      "Epoch: 5004 \t|| Train Loss: 0.13725185735340206 \t|| Test Loss: 0.32086338136823545\n",
      "Epoch: 5005 \t|| Train Loss: 0.13721695735518574 \t|| Test Loss: 0.32075318137092845\n",
      "Epoch: 5006 \t|| Train Loss: 0.13718298930672584 \t|| Test Loss: 0.3207116713732382\n",
      "Epoch: 5007 \t|| Train Loss: 0.13714843735853102 \t|| Test Loss: 0.3206014713759312\n",
      "Epoch: 5008 \t|| Train Loss: 0.13711408830965452 \t|| Test Loss: 0.32055996137824094\n",
      "Epoch: 5009 \t|| Train Loss: 0.1370799173618763 \t|| Test Loss: 0.320449761380934\n",
      "Epoch: 5010 \t|| Train Loss: 0.1370451873125832 \t|| Test Loss: 0.32040825138324375\n",
      "Epoch: 5011 \t|| Train Loss: 0.13701139736522155 \t|| Test Loss: 0.32029805138593676\n",
      "Epoch: 5012 \t|| Train Loss: 0.13697649736700526 \t|| Test Loss: 0.32018785138862976\n",
      "Epoch: 5013 \t|| Train Loss: 0.1369426663170735 \t|| Test Loss: 0.32014634139093956\n",
      "Epoch: 5014 \t|| Train Loss: 0.13690797737035051 \t|| Test Loss: 0.3200361413936325\n",
      "Epoch: 5015 \t|| Train Loss: 0.1368737653200022 \t|| Test Loss: 0.31999463139594225\n",
      "Epoch: 5016 \t|| Train Loss: 0.1368394573736958 \t|| Test Loss: 0.31988443139863526\n",
      "Epoch: 5017 \t|| Train Loss: 0.13680486432293087 \t|| Test Loss: 0.319842921400945\n",
      "Epoch: 5018 \t|| Train Loss: 0.13677093737704107 \t|| Test Loss: 0.319732721403638\n",
      "Epoch: 5019 \t|| Train Loss: 0.13673603737882475 \t|| Test Loss: 0.3196225214063311\n",
      "Epoch: 5020 \t|| Train Loss: 0.13670234332742118 \t|| Test Loss: 0.31958101140864076\n",
      "Epoch: 5021 \t|| Train Loss: 0.13666751738217 \t|| Test Loss: 0.31947081141133377\n",
      "Epoch: 5022 \t|| Train Loss: 0.13663344233034985 \t|| Test Loss: 0.3194293014136435\n",
      "Epoch: 5023 \t|| Train Loss: 0.1365989973855153 \t|| Test Loss: 0.3193191014163365\n",
      "Epoch: 5024 \t|| Train Loss: 0.1365645413332785 \t|| Test Loss: 0.31927759141864626\n",
      "Epoch: 5025 \t|| Train Loss: 0.13653047738886057 \t|| Test Loss: 0.31916739142133926\n",
      "Epoch: 5026 \t|| Train Loss: 0.1364956403362072 \t|| Test Loss: 0.319125881423649\n",
      "Epoch: 5027 \t|| Train Loss: 0.13646195739220582 \t|| Test Loss: 0.319015681426342\n",
      "Epoch: 5028 \t|| Train Loss: 0.13642705739398947 \t|| Test Loss: 0.318905481429035\n",
      "Epoch: 5029 \t|| Train Loss: 0.13639311934069748 \t|| Test Loss: 0.31886397143134476\n",
      "Epoch: 5030 \t|| Train Loss: 0.13635853739733478 \t|| Test Loss: 0.31875377143403777\n",
      "Epoch: 5031 \t|| Train Loss: 0.13632421834362618 \t|| Test Loss: 0.3187122614363476\n",
      "Epoch: 5032 \t|| Train Loss: 0.13629001740068003 \t|| Test Loss: 0.3186020614390405\n",
      "Epoch: 5033 \t|| Train Loss: 0.13625531734655488 \t|| Test Loss: 0.31856055144135026\n",
      "Epoch: 5034 \t|| Train Loss: 0.13622149740402534 \t|| Test Loss: 0.31845035144404327\n",
      "Epoch: 5035 \t|| Train Loss: 0.13618659740580902 \t|| Test Loss: 0.31834015144673633\n",
      "Epoch: 5036 \t|| Train Loss: 0.13615279635104516 \t|| Test Loss: 0.318298641449046\n",
      "Epoch: 5037 \t|| Train Loss: 0.1361180774091543 \t|| Test Loss: 0.3181884414517391\n",
      "Epoch: 5038 \t|| Train Loss: 0.13608389535397386 \t|| Test Loss: 0.31814693145404876\n",
      "Epoch: 5039 \t|| Train Loss: 0.13604955741249955 \t|| Test Loss: 0.31803673145674183\n",
      "Epoch: 5040 \t|| Train Loss: 0.1360149943569025 \t|| Test Loss: 0.31799522145905157\n",
      "Epoch: 5041 \t|| Train Loss: 0.13598103741584483 \t|| Test Loss: 0.3178850214617445\n",
      "Epoch: 5042 \t|| Train Loss: 0.13594613741762848 \t|| Test Loss: 0.31777482146443753\n",
      "Epoch: 5043 \t|| Train Loss: 0.13591247336139278 \t|| Test Loss: 0.3177333114667473\n",
      "Epoch: 5044 \t|| Train Loss: 0.13587761742097376 \t|| Test Loss: 0.31762311146944033\n",
      "Epoch: 5045 \t|| Train Loss: 0.13584357236432149 \t|| Test Loss: 0.3175816014717501\n",
      "Epoch: 5046 \t|| Train Loss: 0.13580909742431904 \t|| Test Loss: 0.3174714014744431\n",
      "Epoch: 5047 \t|| Train Loss: 0.13577467136725022 \t|| Test Loss: 0.3174298914767528\n",
      "Epoch: 5048 \t|| Train Loss: 0.1357405774276643 \t|| Test Loss: 0.31731969147944583\n",
      "Epoch: 5049 \t|| Train Loss: 0.13570577037017886 \t|| Test Loss: 0.3172781814817556\n",
      "Epoch: 5050 \t|| Train Loss: 0.1356720574310096 \t|| Test Loss: 0.3171679814844486\n",
      "Epoch: 5051 \t|| Train Loss: 0.13563715743279325 \t|| Test Loss: 0.3170577814871416\n",
      "Epoch: 5052 \t|| Train Loss: 0.13560324937466914 \t|| Test Loss: 0.31701627148945133\n",
      "Epoch: 5053 \t|| Train Loss: 0.13556863743613853 \t|| Test Loss: 0.31690607149214434\n",
      "Epoch: 5054 \t|| Train Loss: 0.13553434837759784 \t|| Test Loss: 0.31686456149445413\n",
      "Epoch: 5055 \t|| Train Loss: 0.13550011743948379 \t|| Test Loss: 0.31675436149714714\n",
      "Epoch: 5056 \t|| Train Loss: 0.13546544738052652 \t|| Test Loss: 0.3167128514994569\n",
      "Epoch: 5057 \t|| Train Loss: 0.13543159744282912 \t|| Test Loss: 0.3166026515021499\n",
      "Epoch: 5058 \t|| Train Loss: 0.13539669744461275 \t|| Test Loss: 0.3164924515048428\n",
      "Epoch: 5059 \t|| Train Loss: 0.13536292638501682 \t|| Test Loss: 0.3164509415071526\n",
      "Epoch: 5060 \t|| Train Loss: 0.13532817744795805 \t|| Test Loss: 0.31634074150984565\n",
      "Epoch: 5061 \t|| Train Loss: 0.1352940253879455 \t|| Test Loss: 0.3162992315121553\n",
      "Epoch: 5062 \t|| Train Loss: 0.13525965745130333 \t|| Test Loss: 0.3161890315148484\n",
      "Epoch: 5063 \t|| Train Loss: 0.13522512439087417 \t|| Test Loss: 0.31614752151715814\n",
      "Epoch: 5064 \t|| Train Loss: 0.13519113745464856 \t|| Test Loss: 0.31603732151985114\n",
      "Epoch: 5065 \t|| Train Loss: 0.13515623745643227 \t|| Test Loss: 0.31592712152254415\n",
      "Epoch: 5066 \t|| Train Loss: 0.13512260339536447 \t|| Test Loss: 0.3158856115248539\n",
      "Epoch: 5067 \t|| Train Loss: 0.13508771745977752 \t|| Test Loss: 0.3157754115275469\n",
      "Epoch: 5068 \t|| Train Loss: 0.13505370239829317 \t|| Test Loss: 0.31573390152985664\n",
      "Epoch: 5069 \t|| Train Loss: 0.1350191974631228 \t|| Test Loss: 0.31562370153254965\n",
      "Epoch: 5070 \t|| Train Loss: 0.13498480140122185 \t|| Test Loss: 0.31558219153485934\n",
      "Epoch: 5071 \t|| Train Loss: 0.13495067746646808 \t|| Test Loss: 0.3154719915375524\n",
      "Epoch: 5072 \t|| Train Loss: 0.13491590040415052 \t|| Test Loss: 0.31543048153986214\n",
      "Epoch: 5073 \t|| Train Loss: 0.13488215746981336 \t|| Test Loss: 0.31532028154255515\n",
      "Epoch: 5074 \t|| Train Loss: 0.134847257471597 \t|| Test Loss: 0.3152100815452481\n",
      "Epoch: 5075 \t|| Train Loss: 0.1348133794086408 \t|| Test Loss: 0.3151685715475579\n",
      "Epoch: 5076 \t|| Train Loss: 0.1347787374749423 \t|| Test Loss: 0.3150583715502509\n",
      "Epoch: 5077 \t|| Train Loss: 0.1347444784115695 \t|| Test Loss: 0.31501686155256065\n",
      "Epoch: 5078 \t|| Train Loss: 0.13471021747828754 \t|| Test Loss: 0.31490666155525365\n",
      "Epoch: 5079 \t|| Train Loss: 0.13467557741449815 \t|| Test Loss: 0.3148651515575634\n",
      "Epoch: 5080 \t|| Train Loss: 0.13464169748163282 \t|| Test Loss: 0.3147549515602564\n",
      "Epoch: 5081 \t|| Train Loss: 0.13460679748341653 \t|| Test Loss: 0.3146447515629494\n",
      "Epoch: 5082 \t|| Train Loss: 0.13457305641898848 \t|| Test Loss: 0.3146032415652592\n",
      "Epoch: 5083 \t|| Train Loss: 0.13453827748676178 \t|| Test Loss: 0.31449304156795216\n",
      "Epoch: 5084 \t|| Train Loss: 0.13450415542191715 \t|| Test Loss: 0.3144515315702619\n",
      "Epoch: 5085 \t|| Train Loss: 0.1344697574901071 \t|| Test Loss: 0.3143413315729549\n",
      "Epoch: 5086 \t|| Train Loss: 0.13443525442484586 \t|| Test Loss: 0.31429982157526465\n",
      "Epoch: 5087 \t|| Train Loss: 0.1344012374934523 \t|| Test Loss: 0.31418962157795766\n",
      "Epoch: 5088 \t|| Train Loss: 0.13436635342777453 \t|| Test Loss: 0.31414811158026745\n",
      "Epoch: 5089 \t|| Train Loss: 0.1343327174967976 \t|| Test Loss: 0.3140379115829604\n",
      "Epoch: 5090 \t|| Train Loss: 0.1342978174985813 \t|| Test Loss: 0.31392771158565347\n",
      "Epoch: 5091 \t|| Train Loss: 0.1342638324322648 \t|| Test Loss: 0.31388620158796315\n",
      "Epoch: 5092 \t|| Train Loss: 0.13422929750192653 \t|| Test Loss: 0.31377600159065616\n",
      "Epoch: 5093 \t|| Train Loss: 0.13419493143519348 \t|| Test Loss: 0.31373449159296596\n",
      "Epoch: 5094 \t|| Train Loss: 0.13416077750527183 \t|| Test Loss: 0.31362429159565897\n",
      "Epoch: 5095 \t|| Train Loss: 0.13412603043812216 \t|| Test Loss: 0.3135827815979687\n",
      "Epoch: 5096 \t|| Train Loss: 0.13409225750861709 \t|| Test Loss: 0.3134725816006617\n",
      "Epoch: 5097 \t|| Train Loss: 0.1340573575104008 \t|| Test Loss: 0.31336238160335467\n",
      "Epoch: 5098 \t|| Train Loss: 0.13402350944261243 \t|| Test Loss: 0.3133208716056644\n",
      "Epoch: 5099 \t|| Train Loss: 0.13398883751374605 \t|| Test Loss: 0.3132106716083574\n",
      "Epoch: 5100 \t|| Train Loss: 0.13395460844554113 \t|| Test Loss: 0.31316916161066716\n",
      "Epoch: 5101 \t|| Train Loss: 0.13392031751709133 \t|| Test Loss: 0.31305896161336016\n",
      "Epoch: 5102 \t|| Train Loss: 0.1338857074484698 \t|| Test Loss: 0.3130174516156699\n",
      "Epoch: 5103 \t|| Train Loss: 0.1338517975204366 \t|| Test Loss: 0.3129072516183629\n",
      "Epoch: 5104 \t|| Train Loss: 0.13381689752222026 \t|| Test Loss: 0.3127970516210559\n",
      "Epoch: 5105 \t|| Train Loss: 0.1337831864529601 \t|| Test Loss: 0.31275554162336566\n",
      "Epoch: 5106 \t|| Train Loss: 0.1337483775255655 \t|| Test Loss: 0.3126453416260586\n",
      "Epoch: 5107 \t|| Train Loss: 0.1337142854558888 \t|| Test Loss: 0.3126038316283684\n",
      "Epoch: 5108 \t|| Train Loss: 0.13367985752891082 \t|| Test Loss: 0.3124936316310614\n",
      "Epoch: 5109 \t|| Train Loss: 0.1336453844588175 \t|| Test Loss: 0.3124521216333711\n",
      "Epoch: 5110 \t|| Train Loss: 0.13361133753225607 \t|| Test Loss: 0.31234192163606417\n",
      "Epoch: 5111 \t|| Train Loss: 0.13357648346174616 \t|| Test Loss: 0.31230041163837396\n",
      "Epoch: 5112 \t|| Train Loss: 0.13354281753560135 \t|| Test Loss: 0.3121902116410669\n",
      "Epoch: 5113 \t|| Train Loss: 0.13350791753738506 \t|| Test Loss: 0.3120800116437599\n",
      "Epoch: 5114 \t|| Train Loss: 0.13347396246623647 \t|| Test Loss: 0.31203850164606967\n",
      "Epoch: 5115 \t|| Train Loss: 0.13343939754073028 \t|| Test Loss: 0.3119283016487627\n",
      "Epoch: 5116 \t|| Train Loss: 0.13340506146916511 \t|| Test Loss: 0.3118867916510724\n",
      "Epoch: 5117 \t|| Train Loss: 0.13337087754407556 \t|| Test Loss: 0.3117765916537654\n",
      "Epoch: 5118 \t|| Train Loss: 0.1333361604720938 \t|| Test Loss: 0.31173508165607516\n",
      "Epoch: 5119 \t|| Train Loss: 0.13330235754742087 \t|| Test Loss: 0.31162488165876817\n",
      "Epoch: 5120 \t|| Train Loss: 0.1332674575492045 \t|| Test Loss: 0.3115146816614612\n",
      "Epoch: 5121 \t|| Train Loss: 0.13323363947658412 \t|| Test Loss: 0.3114731716637709\n",
      "Epoch: 5122 \t|| Train Loss: 0.1331989375525498 \t|| Test Loss: 0.31136297166646393\n",
      "Epoch: 5123 \t|| Train Loss: 0.1331647384795128 \t|| Test Loss: 0.31132146166877367\n",
      "Epoch: 5124 \t|| Train Loss: 0.13313041755589508 \t|| Test Loss: 0.3112112616714667\n",
      "Epoch: 5125 \t|| Train Loss: 0.13309583748244144 \t|| Test Loss: 0.3111697516737764\n",
      "Epoch: 5126 \t|| Train Loss: 0.13306189755924033 \t|| Test Loss: 0.3110595516764694\n",
      "Epoch: 5127 \t|| Train Loss: 0.133026997561024 \t|| Test Loss: 0.31094935167916243\n",
      "Epoch: 5128 \t|| Train Loss: 0.13299331648693175 \t|| Test Loss: 0.3109078416814722\n",
      "Epoch: 5129 \t|| Train Loss: 0.1329584775643693 \t|| Test Loss: 0.3107976416841652\n",
      "Epoch: 5130 \t|| Train Loss: 0.13292441548986042 \t|| Test Loss: 0.3107561316864749\n",
      "Epoch: 5131 \t|| Train Loss: 0.13288995756771457 \t|| Test Loss: 0.31064593168916793\n",
      "Epoch: 5132 \t|| Train Loss: 0.13285551449278912 \t|| Test Loss: 0.3106044216914777\n",
      "Epoch: 5133 \t|| Train Loss: 0.13282143757105982 \t|| Test Loss: 0.3104942216941707\n",
      "Epoch: 5134 \t|| Train Loss: 0.1327866134957178 \t|| Test Loss: 0.3104527116964804\n",
      "Epoch: 5135 \t|| Train Loss: 0.13275291757440508 \t|| Test Loss: 0.31034251169917343\n",
      "Epoch: 5136 \t|| Train Loss: 0.13271801757618876 \t|| Test Loss: 0.31023231170186644\n",
      "Epoch: 5137 \t|| Train Loss: 0.1326840925002081 \t|| Test Loss: 0.3101908017041762\n",
      "Epoch: 5138 \t|| Train Loss: 0.13264949757953404 \t|| Test Loss: 0.3100806017068692\n",
      "Epoch: 5139 \t|| Train Loss: 0.13261519150313678 \t|| Test Loss: 0.3100390917091789\n",
      "Epoch: 5140 \t|| Train Loss: 0.1325809775828793 \t|| Test Loss: 0.30992889171187193\n",
      "Epoch: 5141 \t|| Train Loss: 0.13254629050606545 \t|| Test Loss: 0.3098873817141817\n",
      "Epoch: 5142 \t|| Train Loss: 0.1325124575862246 \t|| Test Loss: 0.3097771817168747\n",
      "Epoch: 5143 \t|| Train Loss: 0.13247755758800825 \t|| Test Loss: 0.30966698171956764\n",
      "Epoch: 5144 \t|| Train Loss: 0.13244376951055575 \t|| Test Loss: 0.30962547172187743\n",
      "Epoch: 5145 \t|| Train Loss: 0.13240903759135353 \t|| Test Loss: 0.30951527172457044\n",
      "Epoch: 5146 \t|| Train Loss: 0.1323748685134844 \t|| Test Loss: 0.3094737617268801\n",
      "Epoch: 5147 \t|| Train Loss: 0.1323405175946988 \t|| Test Loss: 0.3093635617295732\n",
      "Epoch: 5148 \t|| Train Loss: 0.1323059675164131 \t|| Test Loss: 0.309322051731883\n",
      "Epoch: 5149 \t|| Train Loss: 0.13227199759804406 \t|| Test Loss: 0.30921185173457594\n",
      "Epoch: 5150 \t|| Train Loss: 0.13223709759982774 \t|| Test Loss: 0.3091016517372689\n",
      "Epoch: 5151 \t|| Train Loss: 0.13220344652090338 \t|| Test Loss: 0.3090601417395787\n",
      "Epoch: 5152 \t|| Train Loss: 0.132168577603173 \t|| Test Loss: 0.3089499417422717\n",
      "Epoch: 5153 \t|| Train Loss: 0.13213454552383208 \t|| Test Loss: 0.30890843174458144\n",
      "Epoch: 5154 \t|| Train Loss: 0.13210005760651827 \t|| Test Loss: 0.30879823174727444\n",
      "Epoch: 5155 \t|| Train Loss: 0.13206564452676078 \t|| Test Loss: 0.3087567217495842\n",
      "Epoch: 5156 \t|| Train Loss: 0.13203153760986358 \t|| Test Loss: 0.30864652175227714\n",
      "Epoch: 5157 \t|| Train Loss: 0.13199674352968943 \t|| Test Loss: 0.30860501175458693\n",
      "Epoch: 5158 \t|| Train Loss: 0.13196301761320883 \t|| Test Loss: 0.30849481175727994\n",
      "Epoch: 5159 \t|| Train Loss: 0.1319281176149925 \t|| Test Loss: 0.30838461175997295\n",
      "Epoch: 5160 \t|| Train Loss: 0.13189422253417973 \t|| Test Loss: 0.3083431017622827\n",
      "Epoch: 5161 \t|| Train Loss: 0.1318595976183378 \t|| Test Loss: 0.3082329017649757\n",
      "Epoch: 5162 \t|| Train Loss: 0.13182532153710844 \t|| Test Loss: 0.30819139176728544\n",
      "Epoch: 5163 \t|| Train Loss: 0.13179107762168307 \t|| Test Loss: 0.30808119176997845\n",
      "Epoch: 5164 \t|| Train Loss: 0.13175642054003708 \t|| Test Loss: 0.3080396817722882\n",
      "Epoch: 5165 \t|| Train Loss: 0.13172255762502832 \t|| Test Loss: 0.3079294817749812\n",
      "Epoch: 5166 \t|| Train Loss: 0.131687657626812 \t|| Test Loss: 0.30781928177767415\n",
      "Epoch: 5167 \t|| Train Loss: 0.1316538995445274 \t|| Test Loss: 0.30777777177998394\n",
      "Epoch: 5168 \t|| Train Loss: 0.13161913763015728 \t|| Test Loss: 0.30766757178267695\n",
      "Epoch: 5169 \t|| Train Loss: 0.13158499854745606 \t|| Test Loss: 0.3076260617849867\n",
      "Epoch: 5170 \t|| Train Loss: 0.13155061763350256 \t|| Test Loss: 0.3075158617876797\n",
      "Epoch: 5171 \t|| Train Loss: 0.13151609755038476 \t|| Test Loss: 0.30747435178998944\n",
      "Epoch: 5172 \t|| Train Loss: 0.13148209763684782 \t|| Test Loss: 0.30736415179268245\n",
      "Epoch: 5173 \t|| Train Loss: 0.1314471976386315 \t|| Test Loss: 0.3072539517953754\n",
      "Epoch: 5174 \t|| Train Loss: 0.13141357655487504 \t|| Test Loss: 0.30721244179768514\n",
      "Epoch: 5175 \t|| Train Loss: 0.13137867764197675 \t|| Test Loss: 0.3071022418003782\n",
      "Epoch: 5176 \t|| Train Loss: 0.13134467555780374 \t|| Test Loss: 0.30706073180268795\n",
      "Epoch: 5177 \t|| Train Loss: 0.13131015764532203 \t|| Test Loss: 0.30695053180538096\n",
      "Epoch: 5178 \t|| Train Loss: 0.13127577456073242 \t|| Test Loss: 0.3069090218076907\n",
      "Epoch: 5179 \t|| Train Loss: 0.1312416376486673 \t|| Test Loss: 0.3067988218103837\n",
      "Epoch: 5180 \t|| Train Loss: 0.1312068735636611 \t|| Test Loss: 0.30675731181269345\n",
      "Epoch: 5181 \t|| Train Loss: 0.13117311765201262 \t|| Test Loss: 0.3066471118153864\n",
      "Epoch: 5182 \t|| Train Loss: 0.13113821765379624 \t|| Test Loss: 0.3065369118180794\n",
      "Epoch: 5183 \t|| Train Loss: 0.1311043525681514 \t|| Test Loss: 0.30649540182038915\n",
      "Epoch: 5184 \t|| Train Loss: 0.13106969765714155 \t|| Test Loss: 0.3063852018230822\n",
      "Epoch: 5185 \t|| Train Loss: 0.13103545157108004 \t|| Test Loss: 0.30634369182539195\n",
      "Epoch: 5186 \t|| Train Loss: 0.1310011776604868 \t|| Test Loss: 0.30623349182808496\n",
      "Epoch: 5187 \t|| Train Loss: 0.13096655057400874 \t|| Test Loss: 0.30619198183039464\n",
      "Epoch: 5188 \t|| Train Loss: 0.13093265766383205 \t|| Test Loss: 0.3060817818330877\n",
      "Epoch: 5189 \t|| Train Loss: 0.13089775766561573 \t|| Test Loss: 0.3059715818357807\n",
      "Epoch: 5190 \t|| Train Loss: 0.13086402957849902 \t|| Test Loss: 0.30593007183809046\n",
      "Epoch: 5191 \t|| Train Loss: 0.130829237668961 \t|| Test Loss: 0.30581987184078346\n",
      "Epoch: 5192 \t|| Train Loss: 0.13079512858142772 \t|| Test Loss: 0.3057783618430932\n",
      "Epoch: 5193 \t|| Train Loss: 0.1307607176723063 \t|| Test Loss: 0.30566816184578627\n",
      "Epoch: 5194 \t|| Train Loss: 0.1307262275843564 \t|| Test Loss: 0.30562665184809595\n",
      "Epoch: 5195 \t|| Train Loss: 0.13069219767565157 \t|| Test Loss: 0.30551645185078896\n",
      "Epoch: 5196 \t|| Train Loss: 0.13065732658728507 \t|| Test Loss: 0.3054749418530987\n",
      "Epoch: 5197 \t|| Train Loss: 0.13062367767899685 \t|| Test Loss: 0.3053647418557917\n",
      "Epoch: 5198 \t|| Train Loss: 0.13058877768078053 \t|| Test Loss: 0.3052545418584847\n",
      "Epoch: 5199 \t|| Train Loss: 0.1305548055917754 \t|| Test Loss: 0.3052130318607944\n",
      "Epoch: 5200 \t|| Train Loss: 0.13052025768412578 \t|| Test Loss: 0.3051028318634874\n",
      "Epoch: 5201 \t|| Train Loss: 0.13048590459470405 \t|| Test Loss: 0.3050613218657972\n",
      "Epoch: 5202 \t|| Train Loss: 0.13045173768747106 \t|| Test Loss: 0.3049511218684902\n",
      "Epoch: 5203 \t|| Train Loss: 0.13041700359763272 \t|| Test Loss: 0.3049096118707999\n",
      "Epoch: 5204 \t|| Train Loss: 0.13038321769081634 \t|| Test Loss: 0.3047994118734929\n",
      "Epoch: 5205 \t|| Train Loss: 0.1303483176926 \t|| Test Loss: 0.3046892118761859\n",
      "Epoch: 5206 \t|| Train Loss: 0.13031448260212303 \t|| Test Loss: 0.30464770187849566\n",
      "Epoch: 5207 \t|| Train Loss: 0.13027979769594528 \t|| Test Loss: 0.3045375018811887\n",
      "Epoch: 5208 \t|| Train Loss: 0.1302455816050517 \t|| Test Loss: 0.30449599188349846\n",
      "Epoch: 5209 \t|| Train Loss: 0.13021127769929056 \t|| Test Loss: 0.30438579188619147\n",
      "Epoch: 5210 \t|| Train Loss: 0.1301766806079804 \t|| Test Loss: 0.3043442818885012\n",
      "Epoch: 5211 \t|| Train Loss: 0.13014275770263584 \t|| Test Loss: 0.30423408189119416\n",
      "Epoch: 5212 \t|| Train Loss: 0.13010785770441952 \t|| Test Loss: 0.3041238818938872\n",
      "Epoch: 5213 \t|| Train Loss: 0.13007415961247065 \t|| Test Loss: 0.30408237189619697\n",
      "Epoch: 5214 \t|| Train Loss: 0.13003933770776477 \t|| Test Loss: 0.30397217189889\n",
      "Epoch: 5215 \t|| Train Loss: 0.13000525861539933 \t|| Test Loss: 0.3039306619011997\n",
      "Epoch: 5216 \t|| Train Loss: 0.12997081771111002 \t|| Test Loss: 0.3038204619038927\n",
      "Epoch: 5217 \t|| Train Loss: 0.12993635761832803 \t|| Test Loss: 0.30377895190620247\n",
      "Epoch: 5218 \t|| Train Loss: 0.12990229771445533 \t|| Test Loss: 0.3036687519088955\n",
      "Epoch: 5219 \t|| Train Loss: 0.12986745662125673 \t|| Test Loss: 0.3036272419112052\n",
      "Epoch: 5220 \t|| Train Loss: 0.1298337777178006 \t|| Test Loss: 0.30351704191389817\n",
      "Epoch: 5221 \t|| Train Loss: 0.12979887771958426 \t|| Test Loss: 0.30340684191659123\n",
      "Epoch: 5222 \t|| Train Loss: 0.129764935625747 \t|| Test Loss: 0.30336533191890097\n",
      "Epoch: 5223 \t|| Train Loss: 0.12973035772292954 \t|| Test Loss: 0.303255131921594\n",
      "Epoch: 5224 \t|| Train Loss: 0.1296960346286757 \t|| Test Loss: 0.3032136219239037\n",
      "Epoch: 5225 \t|| Train Loss: 0.12966183772627482 \t|| Test Loss: 0.30310342192659673\n",
      "Epoch: 5226 \t|| Train Loss: 0.12962713363160439 \t|| Test Loss: 0.30306191192890647\n",
      "Epoch: 5227 \t|| Train Loss: 0.12959331772962007 \t|| Test Loss: 0.3029517119315995\n",
      "Epoch: 5228 \t|| Train Loss: 0.12955841773140375 \t|| Test Loss: 0.3028415119342925\n",
      "Epoch: 5229 \t|| Train Loss: 0.12952461263609466 \t|| Test Loss: 0.3028000019366022\n",
      "Epoch: 5230 \t|| Train Loss: 0.129489897734749 \t|| Test Loss: 0.30268980193929523\n",
      "Epoch: 5231 \t|| Train Loss: 0.12945571163902334 \t|| Test Loss: 0.302648291941605\n",
      "Epoch: 5232 \t|| Train Loss: 0.1294213777380943 \t|| Test Loss: 0.30253809194429787\n",
      "Epoch: 5233 \t|| Train Loss: 0.12938681064195204 \t|| Test Loss: 0.30249658194660767\n",
      "Epoch: 5234 \t|| Train Loss: 0.12935285774143956 \t|| Test Loss: 0.3023863819493007\n",
      "Epoch: 5235 \t|| Train Loss: 0.12931795774322324 \t|| Test Loss: 0.3022761819519937\n",
      "Epoch: 5236 \t|| Train Loss: 0.1292842896464423 \t|| Test Loss: 0.3022346719543034\n",
      "Epoch: 5237 \t|| Train Loss: 0.1292494377465685 \t|| Test Loss: 0.30212447195699643\n",
      "Epoch: 5238 \t|| Train Loss: 0.12921538864937102 \t|| Test Loss: 0.3020829619593062\n",
      "Epoch: 5239 \t|| Train Loss: 0.1291809177499138 \t|| Test Loss: 0.3019727619619992\n",
      "Epoch: 5240 \t|| Train Loss: 0.12914648765229972 \t|| Test Loss: 0.301931251964309\n",
      "Epoch: 5241 \t|| Train Loss: 0.12911239775325906 \t|| Test Loss: 0.301821051967002\n",
      "Epoch: 5242 \t|| Train Loss: 0.12907758665522837 \t|| Test Loss: 0.30177954196931167\n",
      "Epoch: 5243 \t|| Train Loss: 0.12904387775660436 \t|| Test Loss: 0.3016693419720047\n",
      "Epoch: 5244 \t|| Train Loss: 0.12900897775838802 \t|| Test Loss: 0.30155914197469774\n",
      "Epoch: 5245 \t|| Train Loss: 0.12897506565971867 \t|| Test Loss: 0.3015176319770075\n",
      "Epoch: 5246 \t|| Train Loss: 0.12894045776173327 \t|| Test Loss: 0.3014074319797005\n",
      "Epoch: 5247 \t|| Train Loss: 0.12890616466264734 \t|| Test Loss: 0.30136592198201023\n",
      "Epoch: 5248 \t|| Train Loss: 0.12887193776507858 \t|| Test Loss: 0.30125572198470324\n",
      "Epoch: 5249 \t|| Train Loss: 0.12883726366557602 \t|| Test Loss: 0.301214211987013\n",
      "Epoch: 5250 \t|| Train Loss: 0.12880341776842383 \t|| Test Loss: 0.30110401198970593\n",
      "Epoch: 5251 \t|| Train Loss: 0.1287685177702075 \t|| Test Loss: 0.300993811992399\n",
      "Epoch: 5252 \t|| Train Loss: 0.1287347426700663 \t|| Test Loss: 0.30095230199470874\n",
      "Epoch: 5253 \t|| Train Loss: 0.12869999777355276 \t|| Test Loss: 0.30084210199740175\n",
      "Epoch: 5254 \t|| Train Loss: 0.12866584167299497 \t|| Test Loss: 0.3008005919997115\n",
      "Epoch: 5255 \t|| Train Loss: 0.12863147777689804 \t|| Test Loss: 0.3006903920024045\n",
      "Epoch: 5256 \t|| Train Loss: 0.12859694067592367 \t|| Test Loss: 0.30064888200471424\n",
      "Epoch: 5257 \t|| Train Loss: 0.12856295778024332 \t|| Test Loss: 0.3005386820074072\n",
      "Epoch: 5258 \t|| Train Loss: 0.128528057782027 \t|| Test Loss: 0.30042848201010025\n",
      "Epoch: 5259 \t|| Train Loss: 0.12849441968041395 \t|| Test Loss: 0.30038697201241\n",
      "Epoch: 5260 \t|| Train Loss: 0.12845953778537228 \t|| Test Loss: 0.300276772015103\n",
      "Epoch: 5261 \t|| Train Loss: 0.12842551868334265 \t|| Test Loss: 0.3002352620174127\n",
      "Epoch: 5262 \t|| Train Loss: 0.12839101778871753 \t|| Test Loss: 0.30012506202010575\n",
      "Epoch: 5263 \t|| Train Loss: 0.12835661768627132 \t|| Test Loss: 0.3000835520224155\n",
      "Epoch: 5264 \t|| Train Loss: 0.1283224977920628 \t|| Test Loss: 0.29997335202510844\n",
      "Epoch: 5265 \t|| Train Loss: 0.12828771668920003 \t|| Test Loss: 0.29993184202741824\n",
      "Epoch: 5266 \t|| Train Loss: 0.12825397779540806 \t|| Test Loss: 0.2998216420301112\n",
      "Epoch: 5267 \t|| Train Loss: 0.12821907779719174 \t|| Test Loss: 0.29971144203280425\n",
      "Epoch: 5268 \t|| Train Loss: 0.1281851956936903 \t|| Test Loss: 0.29966993203511394\n",
      "Epoch: 5269 \t|| Train Loss: 0.12815055780053702 \t|| Test Loss: 0.2995597320378069\n",
      "Epoch: 5270 \t|| Train Loss: 0.12811629469661898 \t|| Test Loss: 0.2995182220401167\n",
      "Epoch: 5271 \t|| Train Loss: 0.12808203780388228 \t|| Test Loss: 0.29940802204280975\n",
      "Epoch: 5272 \t|| Train Loss: 0.12804739369954765 \t|| Test Loss: 0.29936651204511944\n",
      "Epoch: 5273 \t|| Train Loss: 0.12801351780722756 \t|| Test Loss: 0.29925631204781244\n",
      "Epoch: 5274 \t|| Train Loss: 0.12797861780901126 \t|| Test Loss: 0.29914611205050545\n",
      "Epoch: 5275 \t|| Train Loss: 0.12794487270403795 \t|| Test Loss: 0.2991046020528152\n",
      "Epoch: 5276 \t|| Train Loss: 0.12791009781235652 \t|| Test Loss: 0.2989944020555082\n",
      "Epoch: 5277 \t|| Train Loss: 0.12787597170696663 \t|| Test Loss: 0.298952892057818\n",
      "Epoch: 5278 \t|| Train Loss: 0.12784157781570177 \t|| Test Loss: 0.298842692060511\n",
      "Epoch: 5279 \t|| Train Loss: 0.12780707070989533 \t|| Test Loss: 0.2988011820628207\n",
      "Epoch: 5280 \t|| Train Loss: 0.12777305781904708 \t|| Test Loss: 0.29869098206551375\n",
      "Epoch: 5281 \t|| Train Loss: 0.127738169712824 \t|| Test Loss: 0.2986494720678235\n",
      "Epoch: 5282 \t|| Train Loss: 0.12770453782239233 \t|| Test Loss: 0.29853927207051645\n",
      "Epoch: 5283 \t|| Train Loss: 0.127669637824176 \t|| Test Loss: 0.2984290720732095\n",
      "Epoch: 5284 \t|| Train Loss: 0.1276356487173143 \t|| Test Loss: 0.29838756207551925\n",
      "Epoch: 5285 \t|| Train Loss: 0.1276011178275213 \t|| Test Loss: 0.2982773620782122\n",
      "Epoch: 5286 \t|| Train Loss: 0.12756674772024298 \t|| Test Loss: 0.29823585208052195\n",
      "Epoch: 5287 \t|| Train Loss: 0.12753259783086657 \t|| Test Loss: 0.29812565208321495\n",
      "Epoch: 5288 \t|| Train Loss: 0.12749784672317163 \t|| Test Loss: 0.29808414208552475\n",
      "Epoch: 5289 \t|| Train Loss: 0.12746407783421182 \t|| Test Loss: 0.29797394208821776\n",
      "Epoch: 5290 \t|| Train Loss: 0.1274291778359955 \t|| Test Loss: 0.29786374209091065\n",
      "Epoch: 5291 \t|| Train Loss: 0.12739532572766193 \t|| Test Loss: 0.2978222320932205\n",
      "Epoch: 5292 \t|| Train Loss: 0.12736065783934078 \t|| Test Loss: 0.2977120320959135\n",
      "Epoch: 5293 \t|| Train Loss: 0.12732642473059064 \t|| Test Loss: 0.29767052209822326\n",
      "Epoch: 5294 \t|| Train Loss: 0.12729213784268603 \t|| Test Loss: 0.29756032210091626\n",
      "Epoch: 5295 \t|| Train Loss: 0.12725752373351934 \t|| Test Loss: 0.29751881210322595\n",
      "Epoch: 5296 \t|| Train Loss: 0.1272236178460313 \t|| Test Loss: 0.29740861210591896\n",
      "Epoch: 5297 \t|| Train Loss: 0.127188717847815 \t|| Test Loss: 0.29729841210861196\n",
      "Epoch: 5298 \t|| Train Loss: 0.1271550027380096 \t|| Test Loss: 0.2972569021109217\n",
      "Epoch: 5299 \t|| Train Loss: 0.12712019785116024 \t|| Test Loss: 0.2971467021136147\n",
      "Epoch: 5300 \t|| Train Loss: 0.1270861017409383 \t|| Test Loss: 0.2971051921159245\n",
      "Epoch: 5301 \t|| Train Loss: 0.12705167785450552 \t|| Test Loss: 0.29699499211861746\n",
      "Epoch: 5302 \t|| Train Loss: 0.12701720074386696 \t|| Test Loss: 0.29695348212092726\n",
      "Epoch: 5303 \t|| Train Loss: 0.1269831578578508 \t|| Test Loss: 0.2968432821236202\n",
      "Epoch: 5304 \t|| Train Loss: 0.12694829974679564 \t|| Test Loss: 0.29680177212593\n",
      "Epoch: 5305 \t|| Train Loss: 0.1269146378611961 \t|| Test Loss: 0.29669157212862296\n",
      "Epoch: 5306 \t|| Train Loss: 0.12687973786297974 \t|| Test Loss: 0.296581372131316\n",
      "Epoch: 5307 \t|| Train Loss: 0.12684577875128594 \t|| Test Loss: 0.2965398621336257\n",
      "Epoch: 5308 \t|| Train Loss: 0.12681121786632504 \t|| Test Loss: 0.2964296621363187\n",
      "Epoch: 5309 \t|| Train Loss: 0.12677687775421462 \t|| Test Loss: 0.29638815213862846\n",
      "Epoch: 5310 \t|| Train Loss: 0.12674269786967032 \t|| Test Loss: 0.2962779521413215\n",
      "Epoch: 5311 \t|| Train Loss: 0.1267079767571433 \t|| Test Loss: 0.2962364421436312\n",
      "Epoch: 5312 \t|| Train Loss: 0.12667417787301558 \t|| Test Loss: 0.2961262421463242\n",
      "Epoch: 5313 \t|| Train Loss: 0.12663927787479926 \t|| Test Loss: 0.2960160421490172\n",
      "Epoch: 5314 \t|| Train Loss: 0.12660545576163357 \t|| Test Loss: 0.295974532151327\n",
      "Epoch: 5315 \t|| Train Loss: 0.12657075787814454 \t|| Test Loss: 0.29586433215401997\n",
      "Epoch: 5316 \t|| Train Loss: 0.12653655476456227 \t|| Test Loss: 0.2958228221563297\n",
      "Epoch: 5317 \t|| Train Loss: 0.1265022378814898 \t|| Test Loss: 0.2957126221590227\n",
      "Epoch: 5318 \t|| Train Loss: 0.12646765376749097 \t|| Test Loss: 0.29567111216133246\n",
      "Epoch: 5319 \t|| Train Loss: 0.1264337178848351 \t|| Test Loss: 0.2955609121640255\n",
      "Epoch: 5320 \t|| Train Loss: 0.12639881788661875 \t|| Test Loss: 0.29545071216671853\n",
      "Epoch: 5321 \t|| Train Loss: 0.12636513277198125 \t|| Test Loss: 0.2954092021690283\n",
      "Epoch: 5322 \t|| Train Loss: 0.12633029788996403 \t|| Test Loss: 0.29529900217172117\n",
      "Epoch: 5323 \t|| Train Loss: 0.12629623177490992 \t|| Test Loss: 0.295257492174031\n",
      "Epoch: 5324 \t|| Train Loss: 0.1262617778933093 \t|| Test Loss: 0.29514729217672403\n",
      "Epoch: 5325 \t|| Train Loss: 0.1262273307778386 \t|| Test Loss: 0.2951057821790337\n",
      "Epoch: 5326 \t|| Train Loss: 0.12619325789665456 \t|| Test Loss: 0.2949955821817267\n",
      "Epoch: 5327 \t|| Train Loss: 0.12615842978076727 \t|| Test Loss: 0.2949540721840364\n",
      "Epoch: 5328 \t|| Train Loss: 0.12612473789999984 \t|| Test Loss: 0.29484387218672947\n",
      "Epoch: 5329 \t|| Train Loss: 0.1260898379017835 \t|| Test Loss: 0.2947336721894225\n",
      "Epoch: 5330 \t|| Train Loss: 0.12605590878525758 \t|| Test Loss: 0.2946921621917322\n",
      "Epoch: 5331 \t|| Train Loss: 0.12602131790512877 \t|| Test Loss: 0.29458196219442523\n",
      "Epoch: 5332 \t|| Train Loss: 0.12598700778818625 \t|| Test Loss: 0.29454045219673497\n",
      "Epoch: 5333 \t|| Train Loss: 0.12595279790847408 \t|| Test Loss: 0.294430252199428\n",
      "Epoch: 5334 \t|| Train Loss: 0.12591810679111495 \t|| Test Loss: 0.2943887422017377\n",
      "Epoch: 5335 \t|| Train Loss: 0.12588427791181933 \t|| Test Loss: 0.2942785422044307\n",
      "Epoch: 5336 \t|| Train Loss: 0.12584937791360298 \t|| Test Loss: 0.29416834220712373\n",
      "Epoch: 5337 \t|| Train Loss: 0.12581558579560523 \t|| Test Loss: 0.2941268322094335\n",
      "Epoch: 5338 \t|| Train Loss: 0.12578085791694824 \t|| Test Loss: 0.2940166322121265\n",
      "Epoch: 5339 \t|| Train Loss: 0.12574668479853393 \t|| Test Loss: 0.2939751222144362\n",
      "Epoch: 5340 \t|| Train Loss: 0.12571233792029354 \t|| Test Loss: 0.29386492221712923\n",
      "Epoch: 5341 \t|| Train Loss: 0.1256777838014626 \t|| Test Loss: 0.293823412219439\n",
      "Epoch: 5342 \t|| Train Loss: 0.12564381792363882 \t|| Test Loss: 0.293713212222132\n",
      "Epoch: 5343 \t|| Train Loss: 0.12560891792542248 \t|| Test Loss: 0.29360301222482504\n",
      "Epoch: 5344 \t|| Train Loss: 0.1255752628059529 \t|| Test Loss: 0.29356150222713473\n",
      "Epoch: 5345 \t|| Train Loss: 0.12554039792876776 \t|| Test Loss: 0.29345130222982774\n",
      "Epoch: 5346 \t|| Train Loss: 0.12550636180888158 \t|| Test Loss: 0.2934097922321374\n",
      "Epoch: 5347 \t|| Train Loss: 0.12547187793211304 \t|| Test Loss: 0.29329959223483043\n",
      "Epoch: 5348 \t|| Train Loss: 0.1254374608118103 \t|| Test Loss: 0.2932580822371402\n",
      "Epoch: 5349 \t|| Train Loss: 0.12540335793545831 \t|| Test Loss: 0.29314788223983324\n",
      "Epoch: 5350 \t|| Train Loss: 0.12536855981473896 \t|| Test Loss: 0.29310637224214303\n",
      "Epoch: 5351 \t|| Train Loss: 0.12533483793880357 \t|| Test Loss: 0.29299617224483604\n",
      "Epoch: 5352 \t|| Train Loss: 0.12529993794058725 \t|| Test Loss: 0.292885972247529\n",
      "Epoch: 5353 \t|| Train Loss: 0.12526603881922924 \t|| Test Loss: 0.2928444622498388\n",
      "Epoch: 5354 \t|| Train Loss: 0.12523141794393253 \t|| Test Loss: 0.2927342622525317\n",
      "Epoch: 5355 \t|| Train Loss: 0.1251971378221579 \t|| Test Loss: 0.2926927522548414\n",
      "Epoch: 5356 \t|| Train Loss: 0.12516289794727778 \t|| Test Loss: 0.2925825522575345\n",
      "Epoch: 5357 \t|| Train Loss: 0.1251282368250866 \t|| Test Loss: 0.2925410422598443\n",
      "Epoch: 5358 \t|| Train Loss: 0.1250943779506231 \t|| Test Loss: 0.2924308422625373\n",
      "Epoch: 5359 \t|| Train Loss: 0.12505947795240674 \t|| Test Loss: 0.2923206422652303\n",
      "Epoch: 5360 \t|| Train Loss: 0.1250257158295769 \t|| Test Loss: 0.29227913226754\n",
      "Epoch: 5361 \t|| Train Loss: 0.12499095795575203 \t|| Test Loss: 0.29216893227023294\n",
      "Epoch: 5362 \t|| Train Loss: 0.12495681483250556 \t|| Test Loss: 0.2921274222725427\n",
      "Epoch: 5363 \t|| Train Loss: 0.12492243795909727 \t|| Test Loss: 0.2920172222752357\n",
      "Epoch: 5364 \t|| Train Loss: 0.12488791383543425 \t|| Test Loss: 0.29197571227754543\n",
      "Epoch: 5365 \t|| Train Loss: 0.12485391796244256 \t|| Test Loss: 0.2918655122802385\n",
      "Epoch: 5366 \t|| Train Loss: 0.12481901796422623 \t|| Test Loss: 0.2917553122829315\n",
      "Epoch: 5367 \t|| Train Loss: 0.12478539283992451 \t|| Test Loss: 0.29171380228524124\n",
      "Epoch: 5368 \t|| Train Loss: 0.12475049796757151 \t|| Test Loss: 0.29160360228793425\n",
      "Epoch: 5369 \t|| Train Loss: 0.1247164918428532 \t|| Test Loss: 0.291562092290244\n",
      "Epoch: 5370 \t|| Train Loss: 0.12468197797091678 \t|| Test Loss: 0.291451892292937\n",
      "Epoch: 5371 \t|| Train Loss: 0.12464759084578189 \t|| Test Loss: 0.2914103822952467\n",
      "Epoch: 5372 \t|| Train Loss: 0.12461345797426204 \t|| Test Loss: 0.29130018229793975\n",
      "Epoch: 5373 \t|| Train Loss: 0.1245786898487106 \t|| Test Loss: 0.2912586723002495\n",
      "Epoch: 5374 \t|| Train Loss: 0.12454493797760731 \t|| Test Loss: 0.2911484723029425\n",
      "Epoch: 5375 \t|| Train Loss: 0.124510037979391 \t|| Test Loss: 0.2910382723056355\n",
      "Epoch: 5376 \t|| Train Loss: 0.12447616885320087 \t|| Test Loss: 0.29099676230794524\n",
      "Epoch: 5377 \t|| Train Loss: 0.12444151798273625 \t|| Test Loss: 0.29088656231063825\n",
      "Epoch: 5378 \t|| Train Loss: 0.12440726785612957 \t|| Test Loss: 0.290845052312948\n",
      "Epoch: 5379 \t|| Train Loss: 0.12437299798608155 \t|| Test Loss: 0.290734852315641\n",
      "Epoch: 5380 \t|| Train Loss: 0.12433836685905823 \t|| Test Loss: 0.2906933423179507\n",
      "Epoch: 5381 \t|| Train Loss: 0.12430447798942681 \t|| Test Loss: 0.2905831423206437\n",
      "Epoch: 5382 \t|| Train Loss: 0.1242695779912105 \t|| Test Loss: 0.2904729423233368\n",
      "Epoch: 5383 \t|| Train Loss: 0.12423584586354855 \t|| Test Loss: 0.29043143232564644\n",
      "Epoch: 5384 \t|| Train Loss: 0.12420105799455576 \t|| Test Loss: 0.29032123232833945\n",
      "Epoch: 5385 \t|| Train Loss: 0.12416694486647721 \t|| Test Loss: 0.2902797223306492\n",
      "Epoch: 5386 \t|| Train Loss: 0.12413253799790103 \t|| Test Loss: 0.2901695223333423\n",
      "Epoch: 5387 \t|| Train Loss: 0.12409804386940589 \t|| Test Loss: 0.29012801233565194\n",
      "Epoch: 5388 \t|| Train Loss: 0.1240640180012463 \t|| Test Loss: 0.29001781233834506\n",
      "Epoch: 5389 \t|| Train Loss: 0.12402914287233457 \t|| Test Loss: 0.28997630234065475\n",
      "Epoch: 5390 \t|| Train Loss: 0.12399549800459156 \t|| Test Loss: 0.28986610234334775\n",
      "Epoch: 5391 \t|| Train Loss: 0.12396059800637524 \t|| Test Loss: 0.28975590234604076\n",
      "Epoch: 5392 \t|| Train Loss: 0.12392662187682486 \t|| Test Loss: 0.28971439234835056\n",
      "Epoch: 5393 \t|| Train Loss: 0.12389207800972053 \t|| Test Loss: 0.28960419235104345\n",
      "Epoch: 5394 \t|| Train Loss: 0.12385772087975355 \t|| Test Loss: 0.28956268235335325\n",
      "Epoch: 5395 \t|| Train Loss: 0.1238235580130658 \t|| Test Loss: 0.28945248235604626\n",
      "Epoch: 5396 \t|| Train Loss: 0.12378881988268224 \t|| Test Loss: 0.289410972358356\n",
      "Epoch: 5397 \t|| Train Loss: 0.12375503801641108 \t|| Test Loss: 0.289300772361049\n",
      "Epoch: 5398 \t|| Train Loss: 0.12372013801819474 \t|| Test Loss: 0.289190572363742\n",
      "Epoch: 5399 \t|| Train Loss: 0.12368629888717253 \t|| Test Loss: 0.28914906236605176\n",
      "Epoch: 5400 \t|| Train Loss: 0.12365161802154001 \t|| Test Loss: 0.28903886236874476\n",
      "Epoch: 5401 \t|| Train Loss: 0.12361739789010122 \t|| Test Loss: 0.28899735237105445\n",
      "Epoch: 5402 \t|| Train Loss: 0.12358309802488529 \t|| Test Loss: 0.2888871523737475\n",
      "Epoch: 5403 \t|| Train Loss: 0.1235484968930299 \t|| Test Loss: 0.2888456423760572\n",
      "Epoch: 5404 \t|| Train Loss: 0.12351457802823056 \t|| Test Loss: 0.28873544237875026\n",
      "Epoch: 5405 \t|| Train Loss: 0.12347967803001422 \t|| Test Loss: 0.28862524238144327\n",
      "Epoch: 5406 \t|| Train Loss: 0.12344597589752015 \t|| Test Loss: 0.288583732383753\n",
      "Epoch: 5407 \t|| Train Loss: 0.12341115803335952 \t|| Test Loss: 0.28847353238644596\n",
      "Epoch: 5408 \t|| Train Loss: 0.12337707490044887 \t|| Test Loss: 0.28843202238875576\n",
      "Epoch: 5409 \t|| Train Loss: 0.12334263803670478 \t|| Test Loss: 0.28832182239144877\n",
      "Epoch: 5410 \t|| Train Loss: 0.12330817390337753 \t|| Test Loss: 0.2882803123937585\n",
      "Epoch: 5411 \t|| Train Loss: 0.12327411804005006 \t|| Test Loss: 0.28817011239645146\n",
      "Epoch: 5412 \t|| Train Loss: 0.12323927290630621 \t|| Test Loss: 0.28812860239876126\n",
      "Epoch: 5413 \t|| Train Loss: 0.12320559804339534 \t|| Test Loss: 0.2880184024014543\n",
      "Epoch: 5414 \t|| Train Loss: 0.123170698045179 \t|| Test Loss: 0.2879082024041473\n",
      "Epoch: 5415 \t|| Train Loss: 0.12313675191079651 \t|| Test Loss: 0.28786669240645707\n",
      "Epoch: 5416 \t|| Train Loss: 0.12310217804852426 \t|| Test Loss: 0.28775649240914997\n",
      "Epoch: 5417 \t|| Train Loss: 0.12306785091372519 \t|| Test Loss: 0.2877149824114597\n",
      "Epoch: 5418 \t|| Train Loss: 0.12303365805186954 \t|| Test Loss: 0.2876047824141527\n",
      "Epoch: 5419 \t|| Train Loss: 0.12299894991665387 \t|| Test Loss: 0.28756327241646246\n",
      "Epoch: 5420 \t|| Train Loss: 0.1229651380552148 \t|| Test Loss: 0.28745307241915546\n",
      "Epoch: 5421 \t|| Train Loss: 0.1229302380569985 \t|| Test Loss: 0.2873428724218485\n",
      "Epoch: 5422 \t|| Train Loss: 0.12289642892114416 \t|| Test Loss: 0.2873013624241582\n",
      "Epoch: 5423 \t|| Train Loss: 0.12286171806034377 \t|| Test Loss: 0.2871911624268513\n",
      "Epoch: 5424 \t|| Train Loss: 0.12282752792407284 \t|| Test Loss: 0.28714965242916096\n",
      "Epoch: 5425 \t|| Train Loss: 0.12279319806368902 \t|| Test Loss: 0.287039452431854\n",
      "Epoch: 5426 \t|| Train Loss: 0.12275862692700153 \t|| Test Loss: 0.2869979424341637\n",
      "Epoch: 5427 \t|| Train Loss: 0.12272467806703431 \t|| Test Loss: 0.2868877424368568\n",
      "Epoch: 5428 \t|| Train Loss: 0.12268977806881798 \t|| Test Loss: 0.2867775424395497\n",
      "Epoch: 5429 \t|| Train Loss: 0.12265610593149182 \t|| Test Loss: 0.2867360324418595\n",
      "Epoch: 5430 \t|| Train Loss: 0.12262125807216324 \t|| Test Loss: 0.2866258324445525\n",
      "Epoch: 5431 \t|| Train Loss: 0.1225872049344205 \t|| Test Loss: 0.2865843224468622\n",
      "Epoch: 5432 \t|| Train Loss: 0.12255273807550852 \t|| Test Loss: 0.2864741224495553\n",
      "Epoch: 5433 \t|| Train Loss: 0.12251830393734917 \t|| Test Loss: 0.286432612451865\n",
      "Epoch: 5434 \t|| Train Loss: 0.12248421807885382 \t|| Test Loss: 0.28632241245455803\n",
      "Epoch: 5435 \t|| Train Loss: 0.12244940294027787 \t|| Test Loss: 0.28628090245686777\n",
      "Epoch: 5436 \t|| Train Loss: 0.12241569808219907 \t|| Test Loss: 0.2861707024595608\n",
      "Epoch: 5437 \t|| Train Loss: 0.12238079808398275 \t|| Test Loss: 0.2860605024622538\n",
      "Epoch: 5438 \t|| Train Loss: 0.12234688194476814 \t|| Test Loss: 0.2860189924645635\n",
      "Epoch: 5439 \t|| Train Loss: 0.12231227808732803 \t|| Test Loss: 0.28590879246725653\n",
      "Epoch: 5440 \t|| Train Loss: 0.12227798094769682 \t|| Test Loss: 0.2858672824695662\n",
      "Epoch: 5441 \t|| Train Loss: 0.1222437580906733 \t|| Test Loss: 0.2857570824722593\n",
      "Epoch: 5442 \t|| Train Loss: 0.12220907995062553 \t|| Test Loss: 0.285715572474569\n",
      "Epoch: 5443 \t|| Train Loss: 0.12217523809401856 \t|| Test Loss: 0.28560537247726203\n",
      "Epoch: 5444 \t|| Train Loss: 0.12214033809580224 \t|| Test Loss: 0.2854951724799551\n",
      "Epoch: 5445 \t|| Train Loss: 0.1221065589551158 \t|| Test Loss: 0.2854536624822647\n",
      "Epoch: 5446 \t|| Train Loss: 0.1220718180991475 \t|| Test Loss: 0.28534346248495773\n",
      "Epoch: 5447 \t|| Train Loss: 0.12203765795804448 \t|| Test Loss: 0.28530195248726753\n",
      "Epoch: 5448 \t|| Train Loss: 0.12200329810249277 \t|| Test Loss: 0.2851917524899605\n",
      "Epoch: 5449 \t|| Train Loss: 0.12196875696097317 \t|| Test Loss: 0.2851502424922702\n",
      "Epoch: 5450 \t|| Train Loss: 0.12193477810583805 \t|| Test Loss: 0.28504004249496334\n",
      "Epoch: 5451 \t|| Train Loss: 0.1218998781076217 \t|| Test Loss: 0.2849298424976563\n",
      "Epoch: 5452 \t|| Train Loss: 0.12186623596546346 \t|| Test Loss: 0.284888332499966\n",
      "Epoch: 5453 \t|| Train Loss: 0.12183135811096699 \t|| Test Loss: 0.284778132502659\n",
      "Epoch: 5454 \t|| Train Loss: 0.12179733496839215 \t|| Test Loss: 0.28473662250496873\n",
      "Epoch: 5455 \t|| Train Loss: 0.12176283811431228 \t|| Test Loss: 0.28462642250766174\n",
      "Epoch: 5456 \t|| Train Loss: 0.1217284339713208 \t|| Test Loss: 0.2845849125099715\n",
      "Epoch: 5457 \t|| Train Loss: 0.12169431811765756 \t|| Test Loss: 0.2844747125126645\n",
      "Epoch: 5458 \t|| Train Loss: 0.12165953297424952 \t|| Test Loss: 0.2844332025149742\n",
      "Epoch: 5459 \t|| Train Loss: 0.12162579812100283 \t|| Test Loss: 0.28432300251766723\n",
      "Epoch: 5460 \t|| Train Loss: 0.12159089812278649 \t|| Test Loss: 0.28421280252036024\n",
      "Epoch: 5461 \t|| Train Loss: 0.12155701197873978 \t|| Test Loss: 0.28417129252267\n",
      "Epoch: 5462 \t|| Train Loss: 0.12152237812613176 \t|| Test Loss: 0.284061092525363\n",
      "Epoch: 5463 \t|| Train Loss: 0.12148811098166848 \t|| Test Loss: 0.28401958252767273\n",
      "Epoch: 5464 \t|| Train Loss: 0.12145385812947704 \t|| Test Loss: 0.2839093825303658\n",
      "Epoch: 5465 \t|| Train Loss: 0.12141920998459717 \t|| Test Loss: 0.28386787253267554\n",
      "Epoch: 5466 \t|| Train Loss: 0.1213853381328223 \t|| Test Loss: 0.28375767253536854\n",
      "Epoch: 5467 \t|| Train Loss: 0.12135043813460597 \t|| Test Loss: 0.2836474725380615\n",
      "Epoch: 5468 \t|| Train Loss: 0.12131668898908746 \t|| Test Loss: 0.2836059625403713\n",
      "Epoch: 5469 \t|| Train Loss: 0.12128191813795126 \t|| Test Loss: 0.28349576254306424\n",
      "Epoch: 5470 \t|| Train Loss: 0.12124778799201615 \t|| Test Loss: 0.28345425254537404\n",
      "Epoch: 5471 \t|| Train Loss: 0.12121339814129653 \t|| Test Loss: 0.28334405254806705\n",
      "Epoch: 5472 \t|| Train Loss: 0.1211788869949448 \t|| Test Loss: 0.2833025425503768\n",
      "Epoch: 5473 \t|| Train Loss: 0.12114487814464181 \t|| Test Loss: 0.2831923425530698\n",
      "Epoch: 5474 \t|| Train Loss: 0.12110998599787352 \t|| Test Loss: 0.28315083255537954\n",
      "Epoch: 5475 \t|| Train Loss: 0.12107635814798708 \t|| Test Loss: 0.2830406325580725\n",
      "Epoch: 5476 \t|| Train Loss: 0.12104145814977074 \t|| Test Loss: 0.2829304325607655\n",
      "Epoch: 5477 \t|| Train Loss: 0.12100746500236377 \t|| Test Loss: 0.2828889225630753\n",
      "Epoch: 5478 \t|| Train Loss: 0.12097293815311602 \t|| Test Loss: 0.28277872256576825\n",
      "Epoch: 5479 \t|| Train Loss: 0.12093856400529246 \t|| Test Loss: 0.282737212568078\n",
      "Epoch: 5480 \t|| Train Loss: 0.12090441815646132 \t|| Test Loss: 0.282627012570771\n",
      "Epoch: 5481 \t|| Train Loss: 0.12086966300822115 \t|| Test Loss: 0.28258550257308074\n",
      "Epoch: 5482 \t|| Train Loss: 0.12083589815980655 \t|| Test Loss: 0.28247530257577375\n",
      "Epoch: 5483 \t|| Train Loss: 0.12080099816159025 \t|| Test Loss: 0.28236510257846675\n",
      "Epoch: 5484 \t|| Train Loss: 0.12076714201271144 \t|| Test Loss: 0.2823235925807765\n",
      "Epoch: 5485 \t|| Train Loss: 0.12073247816493551 \t|| Test Loss: 0.2822133925834695\n",
      "Epoch: 5486 \t|| Train Loss: 0.12069824101564013 \t|| Test Loss: 0.28217188258577924\n",
      "Epoch: 5487 \t|| Train Loss: 0.12066395816828077 \t|| Test Loss: 0.28206168258847236\n",
      "Epoch: 5488 \t|| Train Loss: 0.12062934001856881 \t|| Test Loss: 0.282020172590782\n",
      "Epoch: 5489 \t|| Train Loss: 0.12059543817162606 \t|| Test Loss: 0.28190997259347506\n",
      "Epoch: 5490 \t|| Train Loss: 0.1205605381734097 \t|| Test Loss: 0.281799772596168\n",
      "Epoch: 5491 \t|| Train Loss: 0.1205268190230591 \t|| Test Loss: 0.2817582625984778\n",
      "Epoch: 5492 \t|| Train Loss: 0.120492018176755 \t|| Test Loss: 0.28164806260117076\n",
      "Epoch: 5493 \t|| Train Loss: 0.12045791802598778 \t|| Test Loss: 0.2816065526034805\n",
      "Epoch: 5494 \t|| Train Loss: 0.12042349818010027 \t|| Test Loss: 0.2814963526061735\n",
      "Epoch: 5495 \t|| Train Loss: 0.12038901702891647 \t|| Test Loss: 0.28145484260848325\n",
      "Epoch: 5496 \t|| Train Loss: 0.12035497818344557 \t|| Test Loss: 0.28134464261117625\n",
      "Epoch: 5497 \t|| Train Loss: 0.12032011603184514 \t|| Test Loss: 0.28130313261348605\n",
      "Epoch: 5498 \t|| Train Loss: 0.1202864581867908 \t|| Test Loss: 0.28119293261617906\n",
      "Epoch: 5499 \t|| Train Loss: 0.1202515581885745 \t|| Test Loss: 0.28108273261887207\n",
      "Epoch: 5500 \t|| Train Loss: 0.12021759503633542 \t|| Test Loss: 0.2810412226211818\n",
      "Epoch: 5501 \t|| Train Loss: 0.12018303819191974 \t|| Test Loss: 0.28093102262387476\n",
      "Epoch: 5502 \t|| Train Loss: 0.12014869403926412 \t|| Test Loss: 0.28088951262618456\n",
      "Epoch: 5503 \t|| Train Loss: 0.12011451819526504 \t|| Test Loss: 0.28077931262887756\n",
      "Epoch: 5504 \t|| Train Loss: 0.12007979304219281 \t|| Test Loss: 0.2807378026311873\n",
      "Epoch: 5505 \t|| Train Loss: 0.12004599819861031 \t|| Test Loss: 0.28062760263388026\n",
      "Epoch: 5506 \t|| Train Loss: 0.12001109820039398 \t|| Test Loss: 0.28051740263657327\n",
      "Epoch: 5507 \t|| Train Loss: 0.11997727204668307 \t|| Test Loss: 0.28047589263888306\n",
      "Epoch: 5508 \t|| Train Loss: 0.11994257820373926 \t|| Test Loss: 0.28036569264157607\n",
      "Epoch: 5509 \t|| Train Loss: 0.11990837104961176 \t|| Test Loss: 0.28032418264388576\n",
      "Epoch: 5510 \t|| Train Loss: 0.11987405820708452 \t|| Test Loss: 0.28021398264657876\n",
      "Epoch: 5511 \t|| Train Loss: 0.11983947005254045 \t|| Test Loss: 0.2801724726488885\n",
      "Epoch: 5512 \t|| Train Loss: 0.1198055382104298 \t|| Test Loss: 0.2800622726515815\n",
      "Epoch: 5513 \t|| Train Loss: 0.11977063821221348 \t|| Test Loss: 0.2799520726542745\n",
      "Epoch: 5514 \t|| Train Loss: 0.11973694905703074 \t|| Test Loss: 0.27991056265658426\n",
      "Epoch: 5515 \t|| Train Loss: 0.11970211821555873 \t|| Test Loss: 0.27980036265927727\n",
      "Epoch: 5516 \t|| Train Loss: 0.11966804805995941 \t|| Test Loss: 0.279758852661587\n",
      "Epoch: 5517 \t|| Train Loss: 0.11963359821890403 \t|| Test Loss: 0.27964865266428\n",
      "Epoch: 5518 \t|| Train Loss: 0.1195991470628881 \t|| Test Loss: 0.27960714266658976\n",
      "Epoch: 5519 \t|| Train Loss: 0.11956507822224931 \t|| Test Loss: 0.27949694266928277\n",
      "Epoch: 5520 \t|| Train Loss: 0.11953024606581679 \t|| Test Loss: 0.27945543267159256\n",
      "Epoch: 5521 \t|| Train Loss: 0.11949655822559455 \t|| Test Loss: 0.2793452326742855\n",
      "Epoch: 5522 \t|| Train Loss: 0.11946165822737823 \t|| Test Loss: 0.2792350326769785\n",
      "Epoch: 5523 \t|| Train Loss: 0.11942772507030708 \t|| Test Loss: 0.27919352267928826\n",
      "Epoch: 5524 \t|| Train Loss: 0.11939313823072353 \t|| Test Loss: 0.2790833226819813\n",
      "Epoch: 5525 \t|| Train Loss: 0.11935882407323577 \t|| Test Loss: 0.279041812684291\n",
      "Epoch: 5526 \t|| Train Loss: 0.11932461823406877 \t|| Test Loss: 0.2789316126869841\n",
      "Epoch: 5527 \t|| Train Loss: 0.11928992307616444 \t|| Test Loss: 0.27889010268929376\n",
      "Epoch: 5528 \t|| Train Loss: 0.11925609823741405 \t|| Test Loss: 0.2787799026919868\n",
      "Epoch: 5529 \t|| Train Loss: 0.11922119823919772 \t|| Test Loss: 0.2786697026946798\n",
      "Epoch: 5530 \t|| Train Loss: 0.11918740208065474 \t|| Test Loss: 0.2786281926969896\n",
      "Epoch: 5531 \t|| Train Loss: 0.11915267824254301 \t|| Test Loss: 0.2785179926996825\n",
      "Epoch: 5532 \t|| Train Loss: 0.11911850108358342 \t|| Test Loss: 0.27847648270199227\n",
      "Epoch: 5533 \t|| Train Loss: 0.11908415824588828 \t|| Test Loss: 0.2783662827046853\n",
      "Epoch: 5534 \t|| Train Loss: 0.11904960008651208 \t|| Test Loss: 0.278324772706995\n",
      "Epoch: 5535 \t|| Train Loss: 0.11901563824923356 \t|| Test Loss: 0.278214572709688\n",
      "Epoch: 5536 \t|| Train Loss: 0.11898073825101725 \t|| Test Loss: 0.27810437271238103\n",
      "Epoch: 5537 \t|| Train Loss: 0.11894707909100237 \t|| Test Loss: 0.2780628627146908\n",
      "Epoch: 5538 \t|| Train Loss: 0.11891221825436249 \t|| Test Loss: 0.27795266271738384\n",
      "Epoch: 5539 \t|| Train Loss: 0.11887817809393106 \t|| Test Loss: 0.2779111527196936\n",
      "Epoch: 5540 \t|| Train Loss: 0.11884369825770777 \t|| Test Loss: 0.27780095272238653\n",
      "Epoch: 5541 \t|| Train Loss: 0.11880927709685976 \t|| Test Loss: 0.27775944272469627\n",
      "Epoch: 5542 \t|| Train Loss: 0.11877517826105304 \t|| Test Loss: 0.2776492427273893\n",
      "Epoch: 5543 \t|| Train Loss: 0.11874037609978845 \t|| Test Loss: 0.277607732729699\n",
      "Epoch: 5544 \t|| Train Loss: 0.11870665826439833 \t|| Test Loss: 0.277497532732392\n",
      "Epoch: 5545 \t|| Train Loss: 0.118671758266182 \t|| Test Loss: 0.27738733273508503\n",
      "Epoch: 5546 \t|| Train Loss: 0.11863785510427874 \t|| Test Loss: 0.2773458227373948\n",
      "Epoch: 5547 \t|| Train Loss: 0.11860323826952728 \t|| Test Loss: 0.2772356227400878\n",
      "Epoch: 5548 \t|| Train Loss: 0.11856895410720743 \t|| Test Loss: 0.2771941127423975\n",
      "Epoch: 5549 \t|| Train Loss: 0.11853471827287251 \t|| Test Loss: 0.27708391274509053\n",
      "Epoch: 5550 \t|| Train Loss: 0.11850005311013609 \t|| Test Loss: 0.2770424027474003\n",
      "Epoch: 5551 \t|| Train Loss: 0.11846619827621781 \t|| Test Loss: 0.2769322027500933\n",
      "Epoch: 5552 \t|| Train Loss: 0.11843129827800147 \t|| Test Loss: 0.2768220027527863\n",
      "Epoch: 5553 \t|| Train Loss: 0.11839753211462636 \t|| Test Loss: 0.27678049275509603\n",
      "Epoch: 5554 \t|| Train Loss: 0.11836277828134675 \t|| Test Loss: 0.27667029275778904\n",
      "Epoch: 5555 \t|| Train Loss: 0.11832863111755505 \t|| Test Loss: 0.2766287827600988\n",
      "Epoch: 5556 \t|| Train Loss: 0.11829425828469202 \t|| Test Loss: 0.2765185827627918\n",
      "Epoch: 5557 \t|| Train Loss: 0.11825973012048374 \t|| Test Loss: 0.2764770727651016\n",
      "Epoch: 5558 \t|| Train Loss: 0.11822573828803731 \t|| Test Loss: 0.27636687276779454\n",
      "Epoch: 5559 \t|| Train Loss: 0.11819083828982095 \t|| Test Loss: 0.27625667277048754\n",
      "Epoch: 5560 \t|| Train Loss: 0.11815720912497403 \t|| Test Loss: 0.2762151627727973\n",
      "Epoch: 5561 \t|| Train Loss: 0.11812231829316625 \t|| Test Loss: 0.2761049627754903\n",
      "Epoch: 5562 \t|| Train Loss: 0.11808830812790272 \t|| Test Loss: 0.27606345277780003\n",
      "Epoch: 5563 \t|| Train Loss: 0.1180537982965115 \t|| Test Loss: 0.2759532527804931\n",
      "Epoch: 5564 \t|| Train Loss: 0.11801940713083141 \t|| Test Loss: 0.2759117427828028\n",
      "Epoch: 5565 \t|| Train Loss: 0.11798527829985679 \t|| Test Loss: 0.2758015427854958\n",
      "Epoch: 5566 \t|| Train Loss: 0.11795050613376008 \t|| Test Loss: 0.27576003278780553\n",
      "Epoch: 5567 \t|| Train Loss: 0.11791675830320207 \t|| Test Loss: 0.27564983279049854\n",
      "Epoch: 5568 \t|| Train Loss: 0.11788185830498572 \t|| Test Loss: 0.27553963279319155\n",
      "Epoch: 5569 \t|| Train Loss: 0.11784798513825039 \t|| Test Loss: 0.2754981227955013\n",
      "Epoch: 5570 \t|| Train Loss: 0.117813338308331 \t|| Test Loss: 0.2753879227981943\n",
      "Epoch: 5571 \t|| Train Loss: 0.11777908414117903 \t|| Test Loss: 0.27534641280050404\n",
      "Epoch: 5572 \t|| Train Loss: 0.11774481831167627 \t|| Test Loss: 0.27523621280319704\n",
      "Epoch: 5573 \t|| Train Loss: 0.11771018314410772 \t|| Test Loss: 0.2751947028055068\n",
      "Epoch: 5574 \t|| Train Loss: 0.11767629831502155 \t|| Test Loss: 0.2750845028081998\n",
      "Epoch: 5575 \t|| Train Loss: 0.11764139831680523 \t|| Test Loss: 0.2749743028108928\n",
      "Epoch: 5576 \t|| Train Loss: 0.11760766214859801 \t|| Test Loss: 0.27493279281320254\n",
      "Epoch: 5577 \t|| Train Loss: 0.11757287832015048 \t|| Test Loss: 0.27482259281589555\n",
      "Epoch: 5578 \t|| Train Loss: 0.1175387611515267 \t|| Test Loss: 0.2747810828182053\n",
      "Epoch: 5579 \t|| Train Loss: 0.11750435832349578 \t|| Test Loss: 0.2746708828208983\n",
      "Epoch: 5580 \t|| Train Loss: 0.1174698601544554 \t|| Test Loss: 0.27462937282320804\n",
      "Epoch: 5581 \t|| Train Loss: 0.11743583832684104 \t|| Test Loss: 0.274519172825901\n",
      "Epoch: 5582 \t|| Train Loss: 0.11740095915738406 \t|| Test Loss: 0.27447766282821073\n",
      "Epoch: 5583 \t|| Train Loss: 0.1173673183301863 \t|| Test Loss: 0.2743674628309038\n",
      "Epoch: 5584 \t|| Train Loss: 0.11733241833196997 \t|| Test Loss: 0.2742572628335968\n",
      "Epoch: 5585 \t|| Train Loss: 0.11729843816187438 \t|| Test Loss: 0.27421575283590655\n",
      "Epoch: 5586 \t|| Train Loss: 0.11726389833531525 \t|| Test Loss: 0.27410555283859955\n",
      "Epoch: 5587 \t|| Train Loss: 0.11722953716480303 \t|| Test Loss: 0.2740640428409093\n",
      "Epoch: 5588 \t|| Train Loss: 0.11719537833866053 \t|| Test Loss: 0.2739538428436023\n",
      "Epoch: 5589 \t|| Train Loss: 0.11716063616773172 \t|| Test Loss: 0.27391233284591204\n",
      "Epoch: 5590 \t|| Train Loss: 0.1171268583420058 \t|| Test Loss: 0.27380213284860505\n",
      "Epoch: 5591 \t|| Train Loss: 0.11709195834378947 \t|| Test Loss: 0.27369193285129806\n",
      "Epoch: 5592 \t|| Train Loss: 0.117058115172222 \t|| Test Loss: 0.2736504228536078\n",
      "Epoch: 5593 \t|| Train Loss: 0.11702343834713476 \t|| Test Loss: 0.2735402228563008\n",
      "Epoch: 5594 \t|| Train Loss: 0.1169892141751507 \t|| Test Loss: 0.27349871285861055\n",
      "Epoch: 5595 \t|| Train Loss: 0.11695491835048002 \t|| Test Loss: 0.27338851286130356\n",
      "Epoch: 5596 \t|| Train Loss: 0.11692031317807938 \t|| Test Loss: 0.2733470028636133\n",
      "Epoch: 5597 \t|| Train Loss: 0.1168863983538253 \t|| Test Loss: 0.2732368028663063\n",
      "Epoch: 5598 \t|| Train Loss: 0.11685149835560897 \t|| Test Loss: 0.2731266028689993\n",
      "Epoch: 5599 \t|| Train Loss: 0.11681779218256967 \t|| Test Loss: 0.27308509287130905\n",
      "Epoch: 5600 \t|| Train Loss: 0.11678297835895424 \t|| Test Loss: 0.27297489287400206\n",
      "Epoch: 5601 \t|| Train Loss: 0.11674889118549836 \t|| Test Loss: 0.2729333828763118\n",
      "Epoch: 5602 \t|| Train Loss: 0.11671445836229952 \t|| Test Loss: 0.2728231828790048\n",
      "Epoch: 5603 \t|| Train Loss: 0.11667999018842705 \t|| Test Loss: 0.27278167288131455\n",
      "Epoch: 5604 \t|| Train Loss: 0.11664593836564481 \t|| Test Loss: 0.27267147288400756\n",
      "Epoch: 5605 \t|| Train Loss: 0.1166110891913557 \t|| Test Loss: 0.2726299628863173\n",
      "Epoch: 5606 \t|| Train Loss: 0.11657741836899005 \t|| Test Loss: 0.2725197628890103\n",
      "Epoch: 5607 \t|| Train Loss: 0.11654251837077372 \t|| Test Loss: 0.27240956289170326\n",
      "Epoch: 5608 \t|| Train Loss: 0.11650856819584599 \t|| Test Loss: 0.27236805289401306\n",
      "Epoch: 5609 \t|| Train Loss: 0.11647399837411902 \t|| Test Loss: 0.27225785289670607\n",
      "Epoch: 5610 \t|| Train Loss: 0.11643966719877467 \t|| Test Loss: 0.2722163428990158\n",
      "Epoch: 5611 \t|| Train Loss: 0.11640547837746426 \t|| Test Loss: 0.2721061429017088\n",
      "Epoch: 5612 \t|| Train Loss: 0.11637076620170336 \t|| Test Loss: 0.27206463290401856\n",
      "Epoch: 5613 \t|| Train Loss: 0.11633695838080955 \t|| Test Loss: 0.2719544329067115\n",
      "Epoch: 5614 \t|| Train Loss: 0.1163020583825932 \t|| Test Loss: 0.27184423290940457\n",
      "Epoch: 5615 \t|| Train Loss: 0.11626824520619365 \t|| Test Loss: 0.2718027229117143\n",
      "Epoch: 5616 \t|| Train Loss: 0.11623353838593849 \t|| Test Loss: 0.2716925229144073\n",
      "Epoch: 5617 \t|| Train Loss: 0.11619934420912234 \t|| Test Loss: 0.27165101291671706\n",
      "Epoch: 5618 \t|| Train Loss: 0.11616501838928377 \t|| Test Loss: 0.27154081291941007\n",
      "Epoch: 5619 \t|| Train Loss: 0.11613044321205102 \t|| Test Loss: 0.27149930292171975\n",
      "Epoch: 5620 \t|| Train Loss: 0.11609649839262903 \t|| Test Loss: 0.2713891029244128\n",
      "Epoch: 5621 \t|| Train Loss: 0.1160615983944127 \t|| Test Loss: 0.2712789029271058\n",
      "Epoch: 5622 \t|| Train Loss: 0.11602792221654132 \t|| Test Loss: 0.27123739292941557\n",
      "Epoch: 5623 \t|| Train Loss: 0.11599307839775799 \t|| Test Loss: 0.2711271929321086\n",
      "Epoch: 5624 \t|| Train Loss: 0.11595902121946997 \t|| Test Loss: 0.2710856829344183\n",
      "Epoch: 5625 \t|| Train Loss: 0.11592455840110324 \t|| Test Loss: 0.2709754829371114\n",
      "Epoch: 5626 \t|| Train Loss: 0.11589012022239867 \t|| Test Loss: 0.27093397293942106\n",
      "Epoch: 5627 \t|| Train Loss: 0.11585603840444851 \t|| Test Loss: 0.27082377294211407\n",
      "Epoch: 5628 \t|| Train Loss: 0.11582121922532736 \t|| Test Loss: 0.2707822629444238\n",
      "Epoch: 5629 \t|| Train Loss: 0.1157875184077938 \t|| Test Loss: 0.2706720629471168\n",
      "Epoch: 5630 \t|| Train Loss: 0.11575261840957747 \t|| Test Loss: 0.27056186294980983\n",
      "Epoch: 5631 \t|| Train Loss: 0.11571869822981765 \t|| Test Loss: 0.27052035295211957\n",
      "Epoch: 5632 \t|| Train Loss: 0.11568409841292275 \t|| Test Loss: 0.2704101529548126\n",
      "Epoch: 5633 \t|| Train Loss: 0.11564979723274633 \t|| Test Loss: 0.2703686429571224\n",
      "Epoch: 5634 \t|| Train Loss: 0.11561557841626802 \t|| Test Loss: 0.2702584429598153\n",
      "Epoch: 5635 \t|| Train Loss: 0.115580896235675 \t|| Test Loss: 0.27021693296212507\n",
      "Epoch: 5636 \t|| Train Loss: 0.1155470584196133 \t|| Test Loss: 0.2701067329648181\n",
      "Epoch: 5637 \t|| Train Loss: 0.11551215842139698 \t|| Test Loss: 0.2699965329675111\n",
      "Epoch: 5638 \t|| Train Loss: 0.11547837524016531 \t|| Test Loss: 0.2699550229698208\n",
      "Epoch: 5639 \t|| Train Loss: 0.11544363842474223 \t|| Test Loss: 0.26984482297251383\n",
      "Epoch: 5640 \t|| Train Loss: 0.11540947424309397 \t|| Test Loss: 0.2698033129748235\n",
      "Epoch: 5641 \t|| Train Loss: 0.11537511842808752 \t|| Test Loss: 0.2696931129775165\n",
      "Epoch: 5642 \t|| Train Loss: 0.11534057324602265 \t|| Test Loss: 0.2696516029798263\n",
      "Epoch: 5643 \t|| Train Loss: 0.1153065984314328 \t|| Test Loss: 0.26954140298251933\n",
      "Epoch: 5644 \t|| Train Loss: 0.11527169843321645 \t|| Test Loss: 0.2694312029852123\n",
      "Epoch: 5645 \t|| Train Loss: 0.11523805225051294 \t|| Test Loss: 0.2693896929875221\n",
      "Epoch: 5646 \t|| Train Loss: 0.11520317843656173 \t|| Test Loss: 0.26927949299021503\n",
      "Epoch: 5647 \t|| Train Loss: 0.11516915125344163 \t|| Test Loss: 0.2692379829925248\n",
      "Epoch: 5648 \t|| Train Loss: 0.115134658439907 \t|| Test Loss: 0.2691277829952178\n",
      "Epoch: 5649 \t|| Train Loss: 0.11510025025637032 \t|| Test Loss: 0.2690862729975276\n",
      "Epoch: 5650 \t|| Train Loss: 0.11506613844325228 \t|| Test Loss: 0.26897607300022053\n",
      "Epoch: 5651 \t|| Train Loss: 0.11503134925929899 \t|| Test Loss: 0.26893456300253027\n",
      "Epoch: 5652 \t|| Train Loss: 0.11499761844659755 \t|| Test Loss: 0.26882436300522333\n",
      "Epoch: 5653 \t|| Train Loss: 0.11496271844838121 \t|| Test Loss: 0.26871416300791634\n",
      "Epoch: 5654 \t|| Train Loss: 0.11492882826378929 \t|| Test Loss: 0.2686726530102261\n",
      "Epoch: 5655 \t|| Train Loss: 0.11489419845172648 \t|| Test Loss: 0.26856245301291903\n",
      "Epoch: 5656 \t|| Train Loss: 0.11485992726671798 \t|| Test Loss: 0.26852094301522883\n",
      "Epoch: 5657 \t|| Train Loss: 0.11482567845507177 \t|| Test Loss: 0.26841074301792184\n",
      "Epoch: 5658 \t|| Train Loss: 0.11479102626964668 \t|| Test Loss: 0.2683692330202316\n",
      "Epoch: 5659 \t|| Train Loss: 0.11475715845841705 \t|| Test Loss: 0.2682590330229246\n",
      "Epoch: 5660 \t|| Train Loss: 0.11472225846020072 \t|| Test Loss: 0.2681488330256176\n",
      "Epoch: 5661 \t|| Train Loss: 0.11468850527413695 \t|| Test Loss: 0.26810732302792734\n",
      "Epoch: 5662 \t|| Train Loss: 0.11465373846354598 \t|| Test Loss: 0.26799712303062034\n",
      "Epoch: 5663 \t|| Train Loss: 0.1146196042770656 \t|| Test Loss: 0.2679556130329301\n",
      "Epoch: 5664 \t|| Train Loss: 0.11458521846689126 \t|| Test Loss: 0.26784541303562315\n",
      "Epoch: 5665 \t|| Train Loss: 0.11455070327999432 \t|| Test Loss: 0.26780390303793283\n",
      "Epoch: 5666 \t|| Train Loss: 0.11451669847023656 \t|| Test Loss: 0.26769370304062584\n",
      "Epoch: 5667 \t|| Train Loss: 0.11448180228292297 \t|| Test Loss: 0.2676521930429356\n",
      "Epoch: 5668 \t|| Train Loss: 0.1144481784735818 \t|| Test Loss: 0.2675419930456286\n",
      "Epoch: 5669 \t|| Train Loss: 0.11441327847536549 \t|| Test Loss: 0.2674317930483216\n",
      "Epoch: 5670 \t|| Train Loss: 0.11437928128741329 \t|| Test Loss: 0.26739028305063134\n",
      "Epoch: 5671 \t|| Train Loss: 0.11434475847871077 \t|| Test Loss: 0.26728008305332435\n",
      "Epoch: 5672 \t|| Train Loss: 0.11431038029034195 \t|| Test Loss: 0.2672385730556341\n",
      "Epoch: 5673 \t|| Train Loss: 0.11427623848205601 \t|| Test Loss: 0.2671283730583271\n",
      "Epoch: 5674 \t|| Train Loss: 0.11424147929327066 \t|| Test Loss: 0.2670868630606368\n",
      "Epoch: 5675 \t|| Train Loss: 0.1142077184854013 \t|| Test Loss: 0.2669766630633298\n",
      "Epoch: 5676 \t|| Train Loss: 0.11417281848718497 \t|| Test Loss: 0.2668664630660228\n",
      "Epoch: 5677 \t|| Train Loss: 0.11413895829776095 \t|| Test Loss: 0.26682495306833254\n",
      "Epoch: 5678 \t|| Train Loss: 0.11410429849053023 \t|| Test Loss: 0.2667147530710256\n",
      "Epoch: 5679 \t|| Train Loss: 0.1140700573006896 \t|| Test Loss: 0.2666732430733353\n",
      "Epoch: 5680 \t|| Train Loss: 0.11403577849387554 \t|| Test Loss: 0.26656304307602835\n",
      "Epoch: 5681 \t|| Train Loss: 0.11400115630361829 \t|| Test Loss: 0.26652153307833804\n",
      "Epoch: 5682 \t|| Train Loss: 0.11396725849722078 \t|| Test Loss: 0.26641133308103104\n",
      "Epoch: 5683 \t|| Train Loss: 0.11393235849900447 \t|| Test Loss: 0.26630113308372405\n",
      "Epoch: 5684 \t|| Train Loss: 0.11389863530810858 \t|| Test Loss: 0.26625962308603385\n",
      "Epoch: 5685 \t|| Train Loss: 0.11386383850234974 \t|| Test Loss: 0.2661494230887268\n",
      "Epoch: 5686 \t|| Train Loss: 0.11382973431103727 \t|| Test Loss: 0.2661079130910366\n",
      "Epoch: 5687 \t|| Train Loss: 0.11379531850569499 \t|| Test Loss: 0.26599771309372955\n",
      "Epoch: 5688 \t|| Train Loss: 0.11376083331396596 \t|| Test Loss: 0.26595620309603935\n",
      "Epoch: 5689 \t|| Train Loss: 0.1137267985090403 \t|| Test Loss: 0.26584600309873235\n",
      "Epoch: 5690 \t|| Train Loss: 0.11369193231689464 \t|| Test Loss: 0.2658044931010421\n",
      "Epoch: 5691 \t|| Train Loss: 0.11365827851238555 \t|| Test Loss: 0.2656942931037351\n",
      "Epoch: 5692 \t|| Train Loss: 0.11362337851416922 \t|| Test Loss: 0.26558409310642805\n",
      "Epoch: 5693 \t|| Train Loss: 0.11358941132138493 \t|| Test Loss: 0.26554258310873785\n",
      "Epoch: 5694 \t|| Train Loss: 0.11355485851751453 \t|| Test Loss: 0.26543238311143086\n",
      "Epoch: 5695 \t|| Train Loss: 0.11352051032431362 \t|| Test Loss: 0.2653908731137406\n",
      "Epoch: 5696 \t|| Train Loss: 0.11348633852085976 \t|| Test Loss: 0.2652806731164336\n",
      "Epoch: 5697 \t|| Train Loss: 0.11345160932724227 \t|| Test Loss: 0.26523916311874335\n",
      "Epoch: 5698 \t|| Train Loss: 0.11341781852420504 \t|| Test Loss: 0.26512896312143636\n",
      "Epoch: 5699 \t|| Train Loss: 0.11338291852598874 \t|| Test Loss: 0.26501876312412936\n",
      "Epoch: 5700 \t|| Train Loss: 0.11334908833173256 \t|| Test Loss: 0.2649772531264391\n",
      "Epoch: 5701 \t|| Train Loss: 0.113314398529334 \t|| Test Loss: 0.2648670531291321\n",
      "Epoch: 5702 \t|| Train Loss: 0.11328018733466125 \t|| Test Loss: 0.2648255431314418\n",
      "Epoch: 5703 \t|| Train Loss: 0.11324587853267927 \t|| Test Loss: 0.2647153431341348\n",
      "Epoch: 5704 \t|| Train Loss: 0.11321128633758995 \t|| Test Loss: 0.2646738331364446\n",
      "Epoch: 5705 \t|| Train Loss: 0.11317735853602454 \t|| Test Loss: 0.26456363313913756\n",
      "Epoch: 5706 \t|| Train Loss: 0.11314245853780822 \t|| Test Loss: 0.26445343314183056\n",
      "Epoch: 5707 \t|| Train Loss: 0.11310876534208023 \t|| Test Loss: 0.26441192314414036\n",
      "Epoch: 5708 \t|| Train Loss: 0.11307393854115348 \t|| Test Loss: 0.2643017231468333\n",
      "Epoch: 5709 \t|| Train Loss: 0.11303986434500893 \t|| Test Loss: 0.2642602131491431\n",
      "Epoch: 5710 \t|| Train Loss: 0.11300541854449875 \t|| Test Loss: 0.26415001315183606\n",
      "Epoch: 5711 \t|| Train Loss: 0.11297096334793759 \t|| Test Loss: 0.26410850315414586\n",
      "Epoch: 5712 \t|| Train Loss: 0.11293689854784401 \t|| Test Loss: 0.2639983031568388\n",
      "Epoch: 5713 \t|| Train Loss: 0.1129020623508663 \t|| Test Loss: 0.26395679315914855\n",
      "Epoch: 5714 \t|| Train Loss: 0.1128683785511893 \t|| Test Loss: 0.26384659316184156\n",
      "Epoch: 5715 \t|| Train Loss: 0.11283347855297299 \t|| Test Loss: 0.2637363931645346\n",
      "Epoch: 5716 \t|| Train Loss: 0.11279954135535657 \t|| Test Loss: 0.2636948831668443\n",
      "Epoch: 5717 \t|| Train Loss: 0.11276495855631825 \t|| Test Loss: 0.2635846831695373\n",
      "Epoch: 5718 \t|| Train Loss: 0.11273064035828526 \t|| Test Loss: 0.2635431731718471\n",
      "Epoch: 5719 \t|| Train Loss: 0.11269643855966352 \t|| Test Loss: 0.2634329731745401\n",
      "Epoch: 5720 \t|| Train Loss: 0.11266173936121396 \t|| Test Loss: 0.26339146317684986\n",
      "Epoch: 5721 \t|| Train Loss: 0.1126279185630088 \t|| Test Loss: 0.2632812631795428\n",
      "Epoch: 5722 \t|| Train Loss: 0.11259301856479247 \t|| Test Loss: 0.2631710631822358\n",
      "Epoch: 5723 \t|| Train Loss: 0.11255921836570422 \t|| Test Loss: 0.2631295531845456\n",
      "Epoch: 5724 \t|| Train Loss: 0.11252449856813773 \t|| Test Loss: 0.26301935318723857\n",
      "Epoch: 5725 \t|| Train Loss: 0.11249031736863291 \t|| Test Loss: 0.26297784318954837\n",
      "Epoch: 5726 \t|| Train Loss: 0.11245597857148301 \t|| Test Loss: 0.2628676431922413\n",
      "Epoch: 5727 \t|| Train Loss: 0.1124214163715616 \t|| Test Loss: 0.2628261331945511\n",
      "Epoch: 5728 \t|| Train Loss: 0.1123874585748283 \t|| Test Loss: 0.2627159331972441\n",
      "Epoch: 5729 \t|| Train Loss: 0.11235255857661194 \t|| Test Loss: 0.26260573319993713\n",
      "Epoch: 5730 \t|| Train Loss: 0.11231889537605189 \t|| Test Loss: 0.26256422320224687\n",
      "Epoch: 5731 \t|| Train Loss: 0.11228403857995721 \t|| Test Loss: 0.2624540232049398\n",
      "Epoch: 5732 \t|| Train Loss: 0.11224999437898057 \t|| Test Loss: 0.2624125132072496\n",
      "Epoch: 5733 \t|| Train Loss: 0.1122155185833025 \t|| Test Loss: 0.26230231320994263\n",
      "Epoch: 5734 \t|| Train Loss: 0.11218109338190922 \t|| Test Loss: 0.26226080321225237\n",
      "Epoch: 5735 \t|| Train Loss: 0.11214699858664776 \t|| Test Loss: 0.2621506032149453\n",
      "Epoch: 5736 \t|| Train Loss: 0.11211219238483794 \t|| Test Loss: 0.26210909321725506\n",
      "Epoch: 5737 \t|| Train Loss: 0.11207847858999305 \t|| Test Loss: 0.26199889321994807\n",
      "Epoch: 5738 \t|| Train Loss: 0.11204357859177672 \t|| Test Loss: 0.2618886932226411\n",
      "Epoch: 5739 \t|| Train Loss: 0.11200967138932823 \t|| Test Loss: 0.2618471832249508\n",
      "Epoch: 5740 \t|| Train Loss: 0.11197505859512198 \t|| Test Loss: 0.2617369832276438\n",
      "Epoch: 5741 \t|| Train Loss: 0.11194077039225689 \t|| Test Loss: 0.26169547322995357\n",
      "Epoch: 5742 \t|| Train Loss: 0.11190653859846729 \t|| Test Loss: 0.26158527323264663\n",
      "Epoch: 5743 \t|| Train Loss: 0.11187186939518559 \t|| Test Loss: 0.2615437632349564\n",
      "Epoch: 5744 \t|| Train Loss: 0.11183801860181256 \t|| Test Loss: 0.2614335632376494\n",
      "Epoch: 5745 \t|| Train Loss: 0.1118031186035962 \t|| Test Loss: 0.2613233632403424\n",
      "Epoch: 5746 \t|| Train Loss: 0.11176934839967587 \t|| Test Loss: 0.26128185324265213\n",
      "Epoch: 5747 \t|| Train Loss: 0.11173459860694149 \t|| Test Loss: 0.2611716532453451\n",
      "Epoch: 5748 \t|| Train Loss: 0.11170044740260457 \t|| Test Loss: 0.2611301432476549\n",
      "Epoch: 5749 \t|| Train Loss: 0.11166607861028677 \t|| Test Loss: 0.26101994325034783\n",
      "Epoch: 5750 \t|| Train Loss: 0.11163154640553323 \t|| Test Loss: 0.26097843325265757\n",
      "Epoch: 5751 \t|| Train Loss: 0.11159755861363205 \t|| Test Loss: 0.2608682332553506\n",
      "Epoch: 5752 \t|| Train Loss: 0.1115626586154157 \t|| Test Loss: 0.26075803325804364\n",
      "Epoch: 5753 \t|| Train Loss: 0.11152902541002355 \t|| Test Loss: 0.26071652326035333\n",
      "Epoch: 5754 \t|| Train Loss: 0.11149413861876098 \t|| Test Loss: 0.26060632326304634\n",
      "Epoch: 5755 \t|| Train Loss: 0.1114601244129522 \t|| Test Loss: 0.2605648132653561\n",
      "Epoch: 5756 \t|| Train Loss: 0.11142561862210627 \t|| Test Loss: 0.2604546132680491\n",
      "Epoch: 5757 \t|| Train Loss: 0.11139122341588088 \t|| Test Loss: 0.2604131032703589\n",
      "Epoch: 5758 \t|| Train Loss: 0.11135709862545151 \t|| Test Loss: 0.2603029032730519\n",
      "Epoch: 5759 \t|| Train Loss: 0.11132232241880957 \t|| Test Loss: 0.26026139327536163\n",
      "Epoch: 5760 \t|| Train Loss: 0.11128857862879679 \t|| Test Loss: 0.26015119327805464\n",
      "Epoch: 5761 \t|| Train Loss: 0.11125367863058047 \t|| Test Loss: 0.2600409932807476\n",
      "Epoch: 5762 \t|| Train Loss: 0.11121980142329986 \t|| Test Loss: 0.25999948328305733\n",
      "Epoch: 5763 \t|| Train Loss: 0.11118515863392572 \t|| Test Loss: 0.2598892832857504\n",
      "Epoch: 5764 \t|| Train Loss: 0.11115090042622855 \t|| Test Loss: 0.25984777328806014\n",
      "Epoch: 5765 \t|| Train Loss: 0.11111663863727102 \t|| Test Loss: 0.2597375732907531\n",
      "Epoch: 5766 \t|| Train Loss: 0.11108199942915724 \t|| Test Loss: 0.25969606329306283\n",
      "Epoch: 5767 \t|| Train Loss: 0.11104811864061628 \t|| Test Loss: 0.2595858632957559\n",
      "Epoch: 5768 \t|| Train Loss: 0.11101321864239995 \t|| Test Loss: 0.2594756632984489\n",
      "Epoch: 5769 \t|| Train Loss: 0.1109794784336475 \t|| Test Loss: 0.2594341533007586\n",
      "Epoch: 5770 \t|| Train Loss: 0.11094469864574523 \t|| Test Loss: 0.2593239533034516\n",
      "Epoch: 5771 \t|| Train Loss: 0.1109105774365762 \t|| Test Loss: 0.25928244330576133\n",
      "Epoch: 5772 \t|| Train Loss: 0.1108761786490905 \t|| Test Loss: 0.25917224330845434\n",
      "Epoch: 5773 \t|| Train Loss: 0.11084167643950489 \t|| Test Loss: 0.2591307333107641\n",
      "Epoch: 5774 \t|| Train Loss: 0.1108076586524358 \t|| Test Loss: 0.2590205333134571\n",
      "Epoch: 5775 \t|| Train Loss: 0.11077277544243358 \t|| Test Loss: 0.25897902331576683\n",
      "Epoch: 5776 \t|| Train Loss: 0.11073913865578104 \t|| Test Loss: 0.25886882331845984\n",
      "Epoch: 5777 \t|| Train Loss: 0.11070423865756471 \t|| Test Loss: 0.25875862332115285\n",
      "Epoch: 5778 \t|| Train Loss: 0.11067025444692384 \t|| Test Loss: 0.2587171133234626\n",
      "Epoch: 5779 \t|| Train Loss: 0.11063571866090997 \t|| Test Loss: 0.2586069133261556\n",
      "Epoch: 5780 \t|| Train Loss: 0.11060135344985253 \t|| Test Loss: 0.25856540332846534\n",
      "Epoch: 5781 \t|| Train Loss: 0.11056719866425527 \t|| Test Loss: 0.25845520333115835\n",
      "Epoch: 5782 \t|| Train Loss: 0.11053245245278123 \t|| Test Loss: 0.2584136933334681\n",
      "Epoch: 5783 \t|| Train Loss: 0.11049867866760055 \t|| Test Loss: 0.25830349333616115\n",
      "Epoch: 5784 \t|| Train Loss: 0.11046377866938421 \t|| Test Loss: 0.25819329333885416\n",
      "Epoch: 5785 \t|| Train Loss: 0.11042993145727151 \t|| Test Loss: 0.25815178334116384\n",
      "Epoch: 5786 \t|| Train Loss: 0.11039525867272948 \t|| Test Loss: 0.25804158334385685\n",
      "Epoch: 5787 \t|| Train Loss: 0.11036103046020021 \t|| Test Loss: 0.2580000733461666\n",
      "Epoch: 5788 \t|| Train Loss: 0.11032673867607476 \t|| Test Loss: 0.2578898733488596\n",
      "Epoch: 5789 \t|| Train Loss: 0.1102921294631289 \t|| Test Loss: 0.25784836335116934\n",
      "Epoch: 5790 \t|| Train Loss: 0.11025821867942005 \t|| Test Loss: 0.25773816335386235\n",
      "Epoch: 5791 \t|| Train Loss: 0.11022331868120369 \t|| Test Loss: 0.2576279633565554\n",
      "Epoch: 5792 \t|| Train Loss: 0.11018960846761916 \t|| Test Loss: 0.25758645335886515\n",
      "Epoch: 5793 \t|| Train Loss: 0.11015479868454899 \t|| Test Loss: 0.2574762533615581\n",
      "Epoch: 5794 \t|| Train Loss: 0.11012070747054785 \t|| Test Loss: 0.25743474336386785\n",
      "Epoch: 5795 \t|| Train Loss: 0.11008627868789427 \t|| Test Loss: 0.2573245433665609\n",
      "Epoch: 5796 \t|| Train Loss: 0.11005180647347652 \t|| Test Loss: 0.25728303336887065\n",
      "Epoch: 5797 \t|| Train Loss: 0.11001775869123953 \t|| Test Loss: 0.2571728333715636\n",
      "Epoch: 5798 \t|| Train Loss: 0.10998290547640521 \t|| Test Loss: 0.2571313233738734\n",
      "Epoch: 5799 \t|| Train Loss: 0.1099492386945848 \t|| Test Loss: 0.2570211233765664\n",
      "Epoch: 5800 \t|| Train Loss: 0.10991433869636849 \t|| Test Loss: 0.25691092337925936\n",
      "Epoch: 5801 \t|| Train Loss: 0.1098803844808955 \t|| Test Loss: 0.2568694133815691\n",
      "Epoch: 5802 \t|| Train Loss: 0.10984581869971373 \t|| Test Loss: 0.2567592133842621\n",
      "Epoch: 5803 \t|| Train Loss: 0.10981148348382416 \t|| Test Loss: 0.25671770338657185\n",
      "Epoch: 5804 \t|| Train Loss: 0.10977729870305901 \t|| Test Loss: 0.25660750338926486\n",
      "Epoch: 5805 \t|| Train Loss: 0.10974258248675288 \t|| Test Loss: 0.2565659933915746\n",
      "Epoch: 5806 \t|| Train Loss: 0.1097087787064043 \t|| Test Loss: 0.2564557933942676\n",
      "Epoch: 5807 \t|| Train Loss: 0.10967387870818797 \t|| Test Loss: 0.2563455933969606\n",
      "Epoch: 5808 \t|| Train Loss: 0.10964006149124314 \t|| Test Loss: 0.2563040833992704\n",
      "Epoch: 5809 \t|| Train Loss: 0.10960535871153324 \t|| Test Loss: 0.25619388340196336\n",
      "Epoch: 5810 \t|| Train Loss: 0.10957116049417184 \t|| Test Loss: 0.2561523734042731\n",
      "Epoch: 5811 \t|| Train Loss: 0.10953683871487851 \t|| Test Loss: 0.2560421734069661\n",
      "Epoch: 5812 \t|| Train Loss: 0.10950225949710053 \t|| Test Loss: 0.25600066340927585\n",
      "Epoch: 5813 \t|| Train Loss: 0.10946831871822378 \t|| Test Loss: 0.25589046341196886\n",
      "Epoch: 5814 \t|| Train Loss: 0.10943341872000745 \t|| Test Loss: 0.25578026341466187\n",
      "Epoch: 5815 \t|| Train Loss: 0.10939973850159079 \t|| Test Loss: 0.2557387534169716\n",
      "Epoch: 5816 \t|| Train Loss: 0.10936489872335273 \t|| Test Loss: 0.2556285534196646\n",
      "Epoch: 5817 \t|| Train Loss: 0.10933083750451948 \t|| Test Loss: 0.25558704342197436\n",
      "Epoch: 5818 \t|| Train Loss: 0.10929637872669799 \t|| Test Loss: 0.2554768434246674\n",
      "Epoch: 5819 \t|| Train Loss: 0.10926193650744817 \t|| Test Loss: 0.2554353334269771\n",
      "Epoch: 5820 \t|| Train Loss: 0.10922785873004326 \t|| Test Loss: 0.2553251334296701\n",
      "Epoch: 5821 \t|| Train Loss: 0.10919303551037685 \t|| Test Loss: 0.25528362343197986\n",
      "Epoch: 5822 \t|| Train Loss: 0.10915933873338854 \t|| Test Loss: 0.2551734234346729\n",
      "Epoch: 5823 \t|| Train Loss: 0.10912443873517222 \t|| Test Loss: 0.25506322343736587\n",
      "Epoch: 5824 \t|| Train Loss: 0.10909051451486715 \t|| Test Loss: 0.25502171343967567\n",
      "Epoch: 5825 \t|| Train Loss: 0.10905591873851747 \t|| Test Loss: 0.2549115134423686\n",
      "Epoch: 5826 \t|| Train Loss: 0.10902161351779582 \t|| Test Loss: 0.25487000344467836\n",
      "Epoch: 5827 \t|| Train Loss: 0.10898739874186276 \t|| Test Loss: 0.25475980344737137\n",
      "Epoch: 5828 \t|| Train Loss: 0.1089527125207245 \t|| Test Loss: 0.25471829344968117\n",
      "Epoch: 5829 \t|| Train Loss: 0.10891887874520804 \t|| Test Loss: 0.2546080934523742\n",
      "Epoch: 5830 \t|| Train Loss: 0.10888397874699171 \t|| Test Loss: 0.2544978934550671\n",
      "Epoch: 5831 \t|| Train Loss: 0.1088501915252148 \t|| Test Loss: 0.25445638345737687\n",
      "Epoch: 5832 \t|| Train Loss: 0.10881545875033698 \t|| Test Loss: 0.2543461834600699\n",
      "Epoch: 5833 \t|| Train Loss: 0.10878129052814348 \t|| Test Loss: 0.2543046734623796\n",
      "Epoch: 5834 \t|| Train Loss: 0.10874693875368224 \t|| Test Loss: 0.2541944734650726\n",
      "Epoch: 5835 \t|| Train Loss: 0.10871238953107216 \t|| Test Loss: 0.25415296346738236\n",
      "Epoch: 5836 \t|| Train Loss: 0.10867841875702752 \t|| Test Loss: 0.2540427634700754\n",
      "Epoch: 5837 \t|| Train Loss: 0.1086435187588112 \t|| Test Loss: 0.2539325634727684\n",
      "Epoch: 5838 \t|| Train Loss: 0.10860986853556245 \t|| Test Loss: 0.2538910534750781\n",
      "Epoch: 5839 \t|| Train Loss: 0.10857499876215645 \t|| Test Loss: 0.25378085347777113\n",
      "Epoch: 5840 \t|| Train Loss: 0.10854096753849114 \t|| Test Loss: 0.25373934348008087\n",
      "Epoch: 5841 \t|| Train Loss: 0.10850647876550175 \t|| Test Loss: 0.2536291434827739\n",
      "Epoch: 5842 \t|| Train Loss: 0.10847206654141983 \t|| Test Loss: 0.2535876334850836\n",
      "Epoch: 5843 \t|| Train Loss: 0.10843795876884701 \t|| Test Loss: 0.2534774334877766\n",
      "Epoch: 5844 \t|| Train Loss: 0.1084031655443485 \t|| Test Loss: 0.25343592349008637\n",
      "Epoch: 5845 \t|| Train Loss: 0.1083694387721923 \t|| Test Loss: 0.2533257234927794\n",
      "Epoch: 5846 \t|| Train Loss: 0.10833453877397595 \t|| Test Loss: 0.2532155234954724\n",
      "Epoch: 5847 \t|| Train Loss: 0.10830064454883878 \t|| Test Loss: 0.2531740134977821\n",
      "Epoch: 5848 \t|| Train Loss: 0.10826601877732123 \t|| Test Loss: 0.25306381350047513\n",
      "Epoch: 5849 \t|| Train Loss: 0.10823174355176746 \t|| Test Loss: 0.2530223035027849\n",
      "Epoch: 5850 \t|| Train Loss: 0.1081974987806665 \t|| Test Loss: 0.2529121035054779\n",
      "Epoch: 5851 \t|| Train Loss: 0.10816284255469617 \t|| Test Loss: 0.2528705935077876\n",
      "Epoch: 5852 \t|| Train Loss: 0.1081289787840118 \t|| Test Loss: 0.25276039351048063\n",
      "Epoch: 5853 \t|| Train Loss: 0.10809407878579544 \t|| Test Loss: 0.25265019351317364\n",
      "Epoch: 5854 \t|| Train Loss: 0.10806032155918643 \t|| Test Loss: 0.25260868351548343\n",
      "Epoch: 5855 \t|| Train Loss: 0.10802555878914073 \t|| Test Loss: 0.2524984835181764\n",
      "Epoch: 5856 \t|| Train Loss: 0.10799142056211512 \t|| Test Loss: 0.2524569735204861\n",
      "Epoch: 5857 \t|| Train Loss: 0.107957038792486 \t|| Test Loss: 0.2523467735231792\n",
      "Epoch: 5858 \t|| Train Loss: 0.10792251956504381 \t|| Test Loss: 0.2523052635254889\n",
      "Epoch: 5859 \t|| Train Loss: 0.10788851879583125 \t|| Test Loss: 0.25219506352818194\n",
      "Epoch: 5860 \t|| Train Loss: 0.10785361879761494 \t|| Test Loss: 0.25208486353087495\n",
      "Epoch: 5861 \t|| Train Loss: 0.1078199985695341 \t|| Test Loss: 0.25204335353318463\n",
      "Epoch: 5862 \t|| Train Loss: 0.10778509880096021 \t|| Test Loss: 0.25193315353587764\n",
      "Epoch: 5863 \t|| Train Loss: 0.10775109757246278 \t|| Test Loss: 0.2518916435381874\n",
      "Epoch: 5864 \t|| Train Loss: 0.10771657880430549 \t|| Test Loss: 0.2517814435408804\n",
      "Epoch: 5865 \t|| Train Loss: 0.10768219657539146 \t|| Test Loss: 0.25173993354319013\n",
      "Epoch: 5866 \t|| Train Loss: 0.10764805880765076 \t|| Test Loss: 0.25162973354588314\n",
      "Epoch: 5867 \t|| Train Loss: 0.10761329557832015 \t|| Test Loss: 0.2515882235481929\n",
      "Epoch: 5868 \t|| Train Loss: 0.10757953881099605 \t|| Test Loss: 0.2514780235508859\n",
      "Epoch: 5869 \t|| Train Loss: 0.10754463881277972 \t|| Test Loss: 0.2513678235535789\n",
      "Epoch: 5870 \t|| Train Loss: 0.10751077458281041 \t|| Test Loss: 0.25132631355588864\n",
      "Epoch: 5871 \t|| Train Loss: 0.10747611881612498 \t|| Test Loss: 0.25121611355858164\n",
      "Epoch: 5872 \t|| Train Loss: 0.10744187358573912 \t|| Test Loss: 0.2511746035608914\n",
      "Epoch: 5873 \t|| Train Loss: 0.10740759881947026 \t|| Test Loss: 0.2510644035635844\n",
      "Epoch: 5874 \t|| Train Loss: 0.1073729725886678 \t|| Test Loss: 0.25102289356589413\n",
      "Epoch: 5875 \t|| Train Loss: 0.10733907882281553 \t|| Test Loss: 0.25091269356858714\n",
      "Epoch: 5876 \t|| Train Loss: 0.1073041788245992 \t|| Test Loss: 0.25080249357128015\n",
      "Epoch: 5877 \t|| Train Loss: 0.1072704515931581 \t|| Test Loss: 0.2507609835735899\n",
      "Epoch: 5878 \t|| Train Loss: 0.10723565882794447 \t|| Test Loss: 0.2506507835762829\n",
      "Epoch: 5879 \t|| Train Loss: 0.10720155059608678 \t|| Test Loss: 0.25060927357859264\n",
      "Epoch: 5880 \t|| Train Loss: 0.10716713883128977 \t|| Test Loss: 0.25049907358128565\n",
      "Epoch: 5881 \t|| Train Loss: 0.10713264959901544 \t|| Test Loss: 0.2504575635835954\n",
      "Epoch: 5882 \t|| Train Loss: 0.107098618834635 \t|| Test Loss: 0.2503473635862884\n",
      "Epoch: 5883 \t|| Train Loss: 0.10706374860194415 \t|| Test Loss: 0.25030585358859814\n",
      "Epoch: 5884 \t|| Train Loss: 0.10703009883798029 \t|| Test Loss: 0.2501956535912912\n",
      "Epoch: 5885 \t|| Train Loss: 0.10699519883976398 \t|| Test Loss: 0.25008545359398415\n",
      "Epoch: 5886 \t|| Train Loss: 0.10696122760643441 \t|| Test Loss: 0.2500439435962939\n",
      "Epoch: 5887 \t|| Train Loss: 0.10692667884310922 \t|| Test Loss: 0.24993374359898687\n",
      "Epoch: 5888 \t|| Train Loss: 0.1068923266093631 \t|| Test Loss: 0.24989223360129667\n",
      "Epoch: 5889 \t|| Train Loss: 0.10685815884645451 \t|| Test Loss: 0.24978203360398962\n",
      "Epoch: 5890 \t|| Train Loss: 0.10682342561229179 \t|| Test Loss: 0.24974052360629942\n",
      "Epoch: 5891 \t|| Train Loss: 0.10678963884979979 \t|| Test Loss: 0.24963032360899237\n",
      "Epoch: 5892 \t|| Train Loss: 0.10675473885158346 \t|| Test Loss: 0.24952012361168538\n",
      "Epoch: 5893 \t|| Train Loss: 0.10672090461678208 \t|| Test Loss: 0.24947861361399512\n",
      "Epoch: 5894 \t|| Train Loss: 0.10668621885492872 \t|| Test Loss: 0.24936841361668818\n",
      "Epoch: 5895 \t|| Train Loss: 0.10665200361971076 \t|| Test Loss: 0.24932690361899787\n",
      "Epoch: 5896 \t|| Train Loss: 0.10661769885827402 \t|| Test Loss: 0.24921670362169093\n",
      "Epoch: 5897 \t|| Train Loss: 0.10658310262263944 \t|| Test Loss: 0.24917519362400062\n",
      "Epoch: 5898 \t|| Train Loss: 0.10654917886161927 \t|| Test Loss: 0.24906499362669368\n",
      "Epoch: 5899 \t|| Train Loss: 0.10651427886340295 \t|| Test Loss: 0.24895479362938663\n",
      "Epoch: 5900 \t|| Train Loss: 0.10648058162712973 \t|| Test Loss: 0.24891328363169637\n",
      "Epoch: 5901 \t|| Train Loss: 0.1064457588667482 \t|| Test Loss: 0.24880308363438938\n",
      "Epoch: 5902 \t|| Train Loss: 0.10641168063005842 \t|| Test Loss: 0.24876157363669918\n",
      "Epoch: 5903 \t|| Train Loss: 0.1063772388700935 \t|| Test Loss: 0.24865137363939213\n",
      "Epoch: 5904 \t|| Train Loss: 0.1063427796329871 \t|| Test Loss: 0.24860986364170187\n",
      "Epoch: 5905 \t|| Train Loss: 0.10630871887343878 \t|| Test Loss: 0.24849966364439494\n",
      "Epoch: 5906 \t|| Train Loss: 0.1062738786359158 \t|| Test Loss: 0.24845815364670468\n",
      "Epoch: 5907 \t|| Train Loss: 0.10624019887678404 \t|| Test Loss: 0.24834795364939763\n",
      "Epoch: 5908 \t|| Train Loss: 0.1062052988785677 \t|| Test Loss: 0.24823775365209064\n",
      "Epoch: 5909 \t|| Train Loss: 0.10617135764040606 \t|| Test Loss: 0.24819624365440043\n",
      "Epoch: 5910 \t|| Train Loss: 0.10613677888191297 \t|| Test Loss: 0.24808604365709344\n",
      "Epoch: 5911 \t|| Train Loss: 0.10610245664333476 \t|| Test Loss: 0.24804453365940318\n",
      "Epoch: 5912 \t|| Train Loss: 0.10606825888525825 \t|| Test Loss: 0.2479343336620962\n",
      "Epoch: 5913 \t|| Train Loss: 0.10603355564626342 \t|| Test Loss: 0.24789282366440593\n",
      "Epoch: 5914 \t|| Train Loss: 0.10599973888860355 \t|| Test Loss: 0.24778262366709894\n",
      "Epoch: 5915 \t|| Train Loss: 0.10596483889038719 \t|| Test Loss: 0.24767242366979195\n",
      "Epoch: 5916 \t|| Train Loss: 0.10593103465075374 \t|| Test Loss: 0.24763091367210163\n",
      "Epoch: 5917 \t|| Train Loss: 0.10589631889373248 \t|| Test Loss: 0.24752071367479464\n",
      "Epoch: 5918 \t|| Train Loss: 0.10586213365368238 \t|| Test Loss: 0.24747920367710438\n",
      "Epoch: 5919 \t|| Train Loss: 0.10582779889707775 \t|| Test Loss: 0.2473690036797974\n",
      "Epoch: 5920 \t|| Train Loss: 0.10579323265661107 \t|| Test Loss: 0.24732749368210713\n",
      "Epoch: 5921 \t|| Train Loss: 0.10575927890042303 \t|| Test Loss: 0.2472172936848002\n",
      "Epoch: 5922 \t|| Train Loss: 0.10572437890220669 \t|| Test Loss: 0.24710709368749312\n",
      "Epoch: 5923 \t|| Train Loss: 0.10569071166110137 \t|| Test Loss: 0.2470655836898029\n",
      "Epoch: 5924 \t|| Train Loss: 0.10565585890555196 \t|| Test Loss: 0.2469553836924959\n",
      "Epoch: 5925 \t|| Train Loss: 0.10562181066403006 \t|| Test Loss: 0.24691387369480564\n",
      "Epoch: 5926 \t|| Train Loss: 0.10558733890889724 \t|| Test Loss: 0.24680367369749864\n",
      "Epoch: 5927 \t|| Train Loss: 0.10555290966695874 \t|| Test Loss: 0.24676216369980844\n",
      "Epoch: 5928 \t|| Train Loss: 0.1055188189122425 \t|| Test Loss: 0.2466519637025014\n",
      "Epoch: 5929 \t|| Train Loss: 0.10548400866988743 \t|| Test Loss: 0.2466104537048112\n",
      "Epoch: 5930 \t|| Train Loss: 0.1054502989155878 \t|| Test Loss: 0.24650025370750414\n",
      "Epoch: 5931 \t|| Train Loss: 0.10541539891737144 \t|| Test Loss: 0.24639005371019715\n",
      "Epoch: 5932 \t|| Train Loss: 0.10538148767437772 \t|| Test Loss: 0.2463485437125069\n",
      "Epoch: 5933 \t|| Train Loss: 0.10534687892071673 \t|| Test Loss: 0.2462383437151999\n",
      "Epoch: 5934 \t|| Train Loss: 0.1053125866773064 \t|| Test Loss: 0.24619683371750964\n",
      "Epoch: 5935 \t|| Train Loss: 0.10527835892406201 \t|| Test Loss: 0.2460866337202027\n",
      "Epoch: 5936 \t|| Train Loss: 0.10524368568023508 \t|| Test Loss: 0.2460451237225124\n",
      "Epoch: 5937 \t|| Train Loss: 0.10520983892740728 \t|| Test Loss: 0.24593492372520545\n",
      "Epoch: 5938 \t|| Train Loss: 0.10517493892919094 \t|| Test Loss: 0.2458247237278984\n",
      "Epoch: 5939 \t|| Train Loss: 0.10514116468472537 \t|| Test Loss: 0.2457832137302082\n",
      "Epoch: 5940 \t|| Train Loss: 0.10510641893253622 \t|| Test Loss: 0.24567301373290115\n",
      "Epoch: 5941 \t|| Train Loss: 0.10507226368765403 \t|| Test Loss: 0.24563150373521095\n",
      "Epoch: 5942 \t|| Train Loss: 0.10503789893588147 \t|| Test Loss: 0.24552130373790396\n",
      "Epoch: 5943 \t|| Train Loss: 0.10500336269058272 \t|| Test Loss: 0.2454797937402137\n",
      "Epoch: 5944 \t|| Train Loss: 0.10496937893922675 \t|| Test Loss: 0.24536959374290665\n",
      "Epoch: 5945 \t|| Train Loss: 0.10493447894101045 \t|| Test Loss: 0.24525939374559966\n",
      "Epoch: 5946 \t|| Train Loss: 0.10490084169507301 \t|| Test Loss: 0.24521788374790945\n",
      "Epoch: 5947 \t|| Train Loss: 0.10486595894435571 \t|| Test Loss: 0.2451076837506024\n",
      "Epoch: 5948 \t|| Train Loss: 0.1048319406980017 \t|| Test Loss: 0.24506617375291215\n",
      "Epoch: 5949 \t|| Train Loss: 0.10479743894770097 \t|| Test Loss: 0.24495597375560516\n",
      "Epoch: 5950 \t|| Train Loss: 0.1047630397009304 \t|| Test Loss: 0.2449144637579149\n",
      "Epoch: 5951 \t|| Train Loss: 0.10472891895104626 \t|| Test Loss: 0.2448042637606079\n",
      "Epoch: 5952 \t|| Train Loss: 0.10469413870385905 \t|| Test Loss: 0.24476275376291765\n",
      "Epoch: 5953 \t|| Train Loss: 0.10466039895439154 \t|| Test Loss: 0.2446525537656107\n",
      "Epoch: 5954 \t|| Train Loss: 0.10462549895617519 \t|| Test Loss: 0.24454235376830366\n",
      "Epoch: 5955 \t|| Train Loss: 0.10459161770834935 \t|| Test Loss: 0.2445008437706134\n",
      "Epoch: 5956 \t|| Train Loss: 0.10455697895952047 \t|| Test Loss: 0.2443906437733064\n",
      "Epoch: 5957 \t|| Train Loss: 0.10452271671127802 \t|| Test Loss: 0.24434913377561615\n",
      "Epoch: 5958 \t|| Train Loss: 0.10448845896286577 \t|| Test Loss: 0.24423893377830921\n",
      "Epoch: 5959 \t|| Train Loss: 0.10445381571420673 \t|| Test Loss: 0.2441974237806189\n",
      "Epoch: 5960 \t|| Train Loss: 0.10441993896621102 \t|| Test Loss: 0.2440872237833119\n",
      "Epoch: 5961 \t|| Train Loss: 0.1043850389679947 \t|| Test Loss: 0.24397702378600492\n",
      "Epoch: 5962 \t|| Train Loss: 0.10435129471869702 \t|| Test Loss: 0.24393551378831466\n",
      "Epoch: 5963 \t|| Train Loss: 0.10431651897133995 \t|| Test Loss: 0.24382531379100772\n",
      "Epoch: 5964 \t|| Train Loss: 0.10428239372162569 \t|| Test Loss: 0.24378380379331746\n",
      "Epoch: 5965 \t|| Train Loss: 0.10424799897468522 \t|| Test Loss: 0.2436736037960104\n",
      "Epoch: 5966 \t|| Train Loss: 0.10421349272455438 \t|| Test Loss: 0.2436320937983202\n",
      "Epoch: 5967 \t|| Train Loss: 0.10417947897803051 \t|| Test Loss: 0.24352189380101316\n",
      "Epoch: 5968 \t|| Train Loss: 0.10414459172748305 \t|| Test Loss: 0.24348038380332296\n",
      "Epoch: 5969 \t|| Train Loss: 0.10411095898137579 \t|| Test Loss: 0.2433701838060159\n",
      "Epoch: 5970 \t|| Train Loss: 0.10407605898315946 \t|| Test Loss: 0.24325998380870892\n",
      "Epoch: 5971 \t|| Train Loss: 0.10404207073197336 \t|| Test Loss: 0.24321847381101866\n",
      "Epoch: 5972 \t|| Train Loss: 0.10400753898650472 \t|| Test Loss: 0.24310827381371172\n",
      "Epoch: 5973 \t|| Train Loss: 0.10397316973490203 \t|| Test Loss: 0.2430667638160214\n",
      "Epoch: 5974 \t|| Train Loss: 0.10393901898985 \t|| Test Loss: 0.24295656381871442\n",
      "Epoch: 5975 \t|| Train Loss: 0.1039042687378307 \t|| Test Loss: 0.24291505382102416\n",
      "Epoch: 5976 \t|| Train Loss: 0.1038704989931953 \t|| Test Loss: 0.24280485382371716\n",
      "Epoch: 5977 \t|| Train Loss: 0.10383559899497893 \t|| Test Loss: 0.24269465382641017\n",
      "Epoch: 5978 \t|| Train Loss: 0.10380174774232098 \t|| Test Loss: 0.2426531438287199\n",
      "Epoch: 5979 \t|| Train Loss: 0.10376707899832423 \t|| Test Loss: 0.24254294383141292\n",
      "Epoch: 5980 \t|| Train Loss: 0.10373284674524967 \t|| Test Loss: 0.24250143383372266\n",
      "Epoch: 5981 \t|| Train Loss: 0.1036985590016695 \t|| Test Loss: 0.24239123383641567\n",
      "Epoch: 5982 \t|| Train Loss: 0.10366394574817836 \t|| Test Loss: 0.2423497238387254\n",
      "Epoch: 5983 \t|| Train Loss: 0.10363003900501477 \t|| Test Loss: 0.24223952384141842\n",
      "Epoch: 5984 \t|| Train Loss: 0.10359513900679844 \t|| Test Loss: 0.24212932384411143\n",
      "Epoch: 5985 \t|| Train Loss: 0.10356142475266865 \t|| Test Loss: 0.24208781384642117\n",
      "Epoch: 5986 \t|| Train Loss: 0.10352661901014373 \t|| Test Loss: 0.24197761384911418\n",
      "Epoch: 5987 \t|| Train Loss: 0.10349252375559734 \t|| Test Loss: 0.24193610385142392\n",
      "Epoch: 5988 \t|| Train Loss: 0.10345809901348899 \t|| Test Loss: 0.24182590385411692\n",
      "Epoch: 5989 \t|| Train Loss: 0.10342362275852601 \t|| Test Loss: 0.24178439385642667\n",
      "Epoch: 5990 \t|| Train Loss: 0.10338957901683425 \t|| Test Loss: 0.24167419385911965\n",
      "Epoch: 5991 \t|| Train Loss: 0.1033547217614547 \t|| Test Loss: 0.24163268386142941\n",
      "Epoch: 5992 \t|| Train Loss: 0.10332105902017955 \t|| Test Loss: 0.24152248386412242\n",
      "Epoch: 5993 \t|| Train Loss: 0.10328615902196321 \t|| Test Loss: 0.24141228386681543\n",
      "Epoch: 5994 \t|| Train Loss: 0.10325220076594499 \t|| Test Loss: 0.24137077386912523\n",
      "Epoch: 5995 \t|| Train Loss: 0.10321763902530852 \t|| Test Loss: 0.24126057387181818\n",
      "Epoch: 5996 \t|| Train Loss: 0.10318329976887368 \t|| Test Loss: 0.24121906387412792\n",
      "Epoch: 5997 \t|| Train Loss: 0.10314911902865376 \t|| Test Loss: 0.24110886387682093\n",
      "Epoch: 5998 \t|| Train Loss: 0.10311439877180235 \t|| Test Loss: 0.24106735387913067\n",
      "Epoch: 5999 \t|| Train Loss: 0.10308059903199904 \t|| Test Loss: 0.24095715388182368\n",
      "Epoch: 6000 \t|| Train Loss: 0.10304569903378269 \t|| Test Loss: 0.24084695388451668\n",
      "Epoch: 6001 \t|| Train Loss: 0.10301187777629264 \t|| Test Loss: 0.24080544388682643\n",
      "Epoch: 6002 \t|| Train Loss: 0.10297717903712797 \t|| Test Loss: 0.24069524388951943\n",
      "Epoch: 6003 \t|| Train Loss: 0.10294297677922133 \t|| Test Loss: 0.24065373389182917\n",
      "Epoch: 6004 \t|| Train Loss: 0.10290865904047326 \t|| Test Loss: 0.24054353389452224\n",
      "Epoch: 6005 \t|| Train Loss: 0.10287407578215002 \t|| Test Loss: 0.24050202389683198\n",
      "Epoch: 6006 \t|| Train Loss: 0.10284013904381853 \t|| Test Loss: 0.24039182389952493\n",
      "Epoch: 6007 \t|| Train Loss: 0.1028052390456022 \t|| Test Loss: 0.24028162390221794\n",
      "Epoch: 6008 \t|| Train Loss: 0.10277155478664028 \t|| Test Loss: 0.24024011390452768\n",
      "Epoch: 6009 \t|| Train Loss: 0.10273671904894746 \t|| Test Loss: 0.2401299139072207\n",
      "Epoch: 6010 \t|| Train Loss: 0.102702653789569 \t|| Test Loss: 0.24008840390953043\n",
      "Epoch: 6011 \t|| Train Loss: 0.10266819905229274 \t|| Test Loss: 0.23997820391222344\n",
      "Epoch: 6012 \t|| Train Loss: 0.10263375279249767 \t|| Test Loss: 0.23993669391453318\n",
      "Epoch: 6013 \t|| Train Loss: 0.10259967905563801 \t|| Test Loss: 0.23982649391722624\n",
      "Epoch: 6014 \t|| Train Loss: 0.10256485179542633 \t|| Test Loss: 0.23978498391953593\n",
      "Epoch: 6015 \t|| Train Loss: 0.10253115905898329 \t|| Test Loss: 0.23967478392222893\n",
      "Epoch: 6016 \t|| Train Loss: 0.10249625906076694 \t|| Test Loss: 0.23956458392492194\n",
      "Epoch: 6017 \t|| Train Loss: 0.10246233079991662 \t|| Test Loss: 0.23952307392723168\n",
      "Epoch: 6018 \t|| Train Loss: 0.10242773906411222 \t|| Test Loss: 0.23941287392992466\n",
      "Epoch: 6019 \t|| Train Loss: 0.10239342980284531 \t|| Test Loss: 0.2393713639322345\n",
      "Epoch: 6020 \t|| Train Loss: 0.10235921906745751 \t|| Test Loss: 0.23926116393492744\n",
      "Epoch: 6021 \t|| Train Loss: 0.102324528805774 \t|| Test Loss: 0.23921965393723718\n",
      "Epoch: 6022 \t|| Train Loss: 0.10229069907080275 \t|| Test Loss: 0.2391094539399302\n",
      "Epoch: 6023 \t|| Train Loss: 0.10225579907258645 \t|| Test Loss: 0.2389992539426232\n",
      "Epoch: 6024 \t|| Train Loss: 0.10222200781026429 \t|| Test Loss: 0.23895774394493294\n",
      "Epoch: 6025 \t|| Train Loss: 0.1021872790759317 \t|| Test Loss: 0.23884754394762595\n",
      "Epoch: 6026 \t|| Train Loss: 0.10215310681319298 \t|| Test Loss: 0.2388060339499357\n",
      "Epoch: 6027 \t|| Train Loss: 0.10211875907927699 \t|| Test Loss: 0.2386958339526287\n",
      "Epoch: 6028 \t|| Train Loss: 0.10208420581612163 \t|| Test Loss: 0.23865432395493844\n",
      "Epoch: 6029 \t|| Train Loss: 0.10205023908262226 \t|| Test Loss: 0.23854412395763144\n",
      "Epoch: 6030 \t|| Train Loss: 0.10201533908440592 \t|| Test Loss: 0.23843392396032445\n",
      "Epoch: 6031 \t|| Train Loss: 0.10198168482061194 \t|| Test Loss: 0.2383924139626342\n",
      "Epoch: 6032 \t|| Train Loss: 0.10194681908775119 \t|| Test Loss: 0.2382822139653272\n",
      "Epoch: 6033 \t|| Train Loss: 0.10191278382354063 \t|| Test Loss: 0.23824070396763694\n",
      "Epoch: 6034 \t|| Train Loss: 0.10187829909109647 \t|| Test Loss: 0.23813050397032995\n",
      "Epoch: 6035 \t|| Train Loss: 0.1018438828264693 \t|| Test Loss: 0.2380889939726397\n",
      "Epoch: 6036 \t|| Train Loss: 0.10180977909444175 \t|| Test Loss: 0.2379787939753327\n",
      "Epoch: 6037 \t|| Train Loss: 0.101774981829398 \t|| Test Loss: 0.23793728397764244\n",
      "Epoch: 6038 \t|| Train Loss: 0.10174125909778704 \t|| Test Loss: 0.23782708398033545\n",
      "Epoch: 6039 \t|| Train Loss: 0.10170635909957068 \t|| Test Loss: 0.2377168839830285\n",
      "Epoch: 6040 \t|| Train Loss: 0.10167246083388828 \t|| Test Loss: 0.2376753739853382\n",
      "Epoch: 6041 \t|| Train Loss: 0.10163783910291598 \t|| Test Loss: 0.2375651739880312\n",
      "Epoch: 6042 \t|| Train Loss: 0.10160355983681697 \t|| Test Loss: 0.23752366399034094\n",
      "Epoch: 6043 \t|| Train Loss: 0.10156931910626124 \t|| Test Loss: 0.23741346399303395\n",
      "Epoch: 6044 \t|| Train Loss: 0.10153465883974563 \t|| Test Loss: 0.2373719539953437\n",
      "Epoch: 6045 \t|| Train Loss: 0.10150079910960652 \t|| Test Loss: 0.2372617539980367\n",
      "Epoch: 6046 \t|| Train Loss: 0.10146589911139019 \t|| Test Loss: 0.2371515540007297\n",
      "Epoch: 6047 \t|| Train Loss: 0.10143213784423595 \t|| Test Loss: 0.23711004400303942\n",
      "Epoch: 6048 \t|| Train Loss: 0.10139737911473545 \t|| Test Loss: 0.23699984400573246\n",
      "Epoch: 6049 \t|| Train Loss: 0.10136323684716461 \t|| Test Loss: 0.2369583340080422\n",
      "Epoch: 6050 \t|| Train Loss: 0.10132885911808073 \t|| Test Loss: 0.2368481340107352\n",
      "Epoch: 6051 \t|| Train Loss: 0.10129433585009331 \t|| Test Loss: 0.23680662401304495\n",
      "Epoch: 6052 \t|| Train Loss: 0.101260339121426 \t|| Test Loss: 0.23669642401573796\n",
      "Epoch: 6053 \t|| Train Loss: 0.10122543912320967 \t|| Test Loss: 0.23658622401843094\n",
      "Epoch: 6054 \t|| Train Loss: 0.10119181485458358 \t|| Test Loss: 0.2365447140207407\n",
      "Epoch: 6055 \t|| Train Loss: 0.10115691912655496 \t|| Test Loss: 0.2364345140234337\n",
      "Epoch: 6056 \t|| Train Loss: 0.10112291385751226 \t|| Test Loss: 0.23639300402574345\n",
      "Epoch: 6057 \t|| Train Loss: 0.10108839912990022 \t|| Test Loss: 0.23628280402843646\n",
      "Epoch: 6058 \t|| Train Loss: 0.10105401286044095 \t|| Test Loss: 0.2362412940307462\n",
      "Epoch: 6059 \t|| Train Loss: 0.1010198791332455 \t|| Test Loss: 0.2361310940334392\n",
      "Epoch: 6060 \t|| Train Loss: 0.10098511186336964 \t|| Test Loss: 0.23608958403574895\n",
      "Epoch: 6061 \t|| Train Loss: 0.10095135913659077 \t|| Test Loss: 0.23597938403844196\n",
      "Epoch: 6062 \t|| Train Loss: 0.10091645913837444 \t|| Test Loss: 0.23586918404113497\n",
      "Epoch: 6063 \t|| Train Loss: 0.10088259086785993 \t|| Test Loss: 0.2358276740434447\n",
      "Epoch: 6064 \t|| Train Loss: 0.10084793914171972 \t|| Test Loss: 0.23571747404613771\n",
      "Epoch: 6065 \t|| Train Loss: 0.1008136898707886 \t|| Test Loss: 0.23567596404844746\n",
      "Epoch: 6066 \t|| Train Loss: 0.10077941914506501 \t|| Test Loss: 0.23556576405114046\n",
      "Epoch: 6067 \t|| Train Loss: 0.10074478887371732 \t|| Test Loss: 0.2355242540534502\n",
      "Epoch: 6068 \t|| Train Loss: 0.10071089914841025 \t|| Test Loss: 0.2354140540561432\n",
      "Epoch: 6069 \t|| Train Loss: 0.10067599915019394 \t|| Test Loss: 0.23530385405883622\n",
      "Epoch: 6070 \t|| Train Loss: 0.10064226787820758 \t|| Test Loss: 0.23526234406114596\n",
      "Epoch: 6071 \t|| Train Loss: 0.10060747915353922 \t|| Test Loss: 0.23515214406383897\n",
      "Epoch: 6072 \t|| Train Loss: 0.10057336688113624 \t|| Test Loss: 0.2351106340661487\n",
      "Epoch: 6073 \t|| Train Loss: 0.10053895915688446 \t|| Test Loss: 0.23500043406884172\n",
      "Epoch: 6074 \t|| Train Loss: 0.10050446588406495 \t|| Test Loss: 0.23495892407115146\n",
      "Epoch: 6075 \t|| Train Loss: 0.10047043916022977 \t|| Test Loss: 0.23484872407384447\n",
      "Epoch: 6076 \t|| Train Loss: 0.10043556488699364 \t|| Test Loss: 0.2348072140761542\n",
      "Epoch: 6077 \t|| Train Loss: 0.10040191916357503 \t|| Test Loss: 0.23469701407884722\n",
      "Epoch: 6078 \t|| Train Loss: 0.10036701916535869 \t|| Test Loss: 0.23458681408154022\n",
      "Epoch: 6079 \t|| Train Loss: 0.10033304389148393 \t|| Test Loss: 0.23454530408384996\n",
      "Epoch: 6080 \t|| Train Loss: 0.10029849916870397 \t|| Test Loss: 0.23443510408654297\n",
      "Epoch: 6081 \t|| Train Loss: 0.10026414289441261 \t|| Test Loss: 0.2343935940888527\n",
      "Epoch: 6082 \t|| Train Loss: 0.10022997917204923 \t|| Test Loss: 0.23428339409154572\n",
      "Epoch: 6083 \t|| Train Loss: 0.10019524189734128 \t|| Test Loss: 0.23424188409385546\n",
      "Epoch: 6084 \t|| Train Loss: 0.10016145917539454 \t|| Test Loss: 0.23413168409654844\n",
      "Epoch: 6085 \t|| Train Loss: 0.10012655917717819 \t|| Test Loss: 0.23402148409924148\n",
      "Epoch: 6086 \t|| Train Loss: 0.10009272090183156 \t|| Test Loss: 0.2339799741015512\n",
      "Epoch: 6087 \t|| Train Loss: 0.10005803918052347 \t|| Test Loss: 0.23386977410424423\n",
      "Epoch: 6088 \t|| Train Loss: 0.10002381990476025 \t|| Test Loss: 0.23382826410655397\n",
      "Epoch: 6089 \t|| Train Loss: 0.09998951918386874 \t|| Test Loss: 0.23371806410924698\n",
      "Epoch: 6090 \t|| Train Loss: 0.09995491890768893 \t|| Test Loss: 0.23367655411155672\n",
      "Epoch: 6091 \t|| Train Loss: 0.09992099918721402 \t|| Test Loss: 0.23356635411424972\n",
      "Epoch: 6092 \t|| Train Loss: 0.09988609918899768 \t|| Test Loss: 0.23345615411694273\n",
      "Epoch: 6093 \t|| Train Loss: 0.09985239791217923 \t|| Test Loss: 0.23341464411925247\n",
      "Epoch: 6094 \t|| Train Loss: 0.09981757919234295 \t|| Test Loss: 0.23330444412194545\n",
      "Epoch: 6095 \t|| Train Loss: 0.0997834969151079 \t|| Test Loss: 0.23326293412425522\n",
      "Epoch: 6096 \t|| Train Loss: 0.09974905919568823 \t|| Test Loss: 0.23315273412694823\n",
      "Epoch: 6097 \t|| Train Loss: 0.09971459591803658 \t|| Test Loss: 0.23311122412925797\n",
      "Epoch: 6098 \t|| Train Loss: 0.0996805391990335 \t|| Test Loss: 0.23300102413195098\n",
      "Epoch: 6099 \t|| Train Loss: 0.09964569492096527 \t|| Test Loss: 0.2329595141342607\n",
      "Epoch: 6100 \t|| Train Loss: 0.09961201920237878 \t|| Test Loss: 0.23284931413695373\n",
      "Epoch: 6101 \t|| Train Loss: 0.09957711920416243 \t|| Test Loss: 0.23273911413964674\n",
      "Epoch: 6102 \t|| Train Loss: 0.09954317392545556 \t|| Test Loss: 0.23269760414195648\n",
      "Epoch: 6103 \t|| Train Loss: 0.09950859920750771 \t|| Test Loss: 0.23258740414464948\n",
      "Epoch: 6104 \t|| Train Loss: 0.09947427292838425 \t|| Test Loss: 0.23254589414695923\n",
      "Epoch: 6105 \t|| Train Loss: 0.09944007921085302 \t|| Test Loss: 0.23243569414965223\n",
      "Epoch: 6106 \t|| Train Loss: 0.09940537193131292 \t|| Test Loss: 0.23239418415196197\n",
      "Epoch: 6107 \t|| Train Loss: 0.09937155921419825 \t|| Test Loss: 0.23228398415465498\n",
      "Epoch: 6108 \t|| Train Loss: 0.09933665921598193 \t|| Test Loss: 0.232173784157348\n",
      "Epoch: 6109 \t|| Train Loss: 0.09930285093580321 \t|| Test Loss: 0.23213227415965773\n",
      "Epoch: 6110 \t|| Train Loss: 0.0992681392193272 \t|| Test Loss: 0.23202207416235074\n",
      "Epoch: 6111 \t|| Train Loss: 0.09923394993873189 \t|| Test Loss: 0.23198056416466048\n",
      "Epoch: 6112 \t|| Train Loss: 0.09919961922267248 \t|| Test Loss: 0.2318703641673535\n",
      "Epoch: 6113 \t|| Train Loss: 0.09916504894166057 \t|| Test Loss: 0.23182885416966323\n",
      "Epoch: 6114 \t|| Train Loss: 0.09913109922601775 \t|| Test Loss: 0.23171865417235624\n",
      "Epoch: 6115 \t|| Train Loss: 0.09909619922780143 \t|| Test Loss: 0.23160845417504924\n",
      "Epoch: 6116 \t|| Train Loss: 0.09906252794615086 \t|| Test Loss: 0.23156694417735899\n",
      "Epoch: 6117 \t|| Train Loss: 0.0990276792311467 \t|| Test Loss: 0.23145674418005197\n",
      "Epoch: 6118 \t|| Train Loss: 0.09899362694907955 \t|| Test Loss: 0.23141523418236173\n",
      "Epoch: 6119 \t|| Train Loss: 0.09895915923449197 \t|| Test Loss: 0.23130503418505471\n",
      "Epoch: 6120 \t|| Train Loss: 0.09892472595200823 \t|| Test Loss: 0.23126352418736448\n",
      "Epoch: 6121 \t|| Train Loss: 0.09889063923783725 \t|| Test Loss: 0.23115332419005746\n",
      "Epoch: 6122 \t|| Train Loss: 0.09885582495493692 \t|| Test Loss: 0.23111181419236723\n",
      "Epoch: 6123 \t|| Train Loss: 0.09882211924118253 \t|| Test Loss: 0.2310016141950602\n",
      "Epoch: 6124 \t|| Train Loss: 0.0987872192429662 \t|| Test Loss: 0.23089141419775325\n",
      "Epoch: 6125 \t|| Train Loss: 0.0987533039594272 \t|| Test Loss: 0.230849904200063\n",
      "Epoch: 6126 \t|| Train Loss: 0.09871869924631146 \t|| Test Loss: 0.23073970420275597\n",
      "Epoch: 6127 \t|| Train Loss: 0.0986844029623559 \t|| Test Loss: 0.23069819420506574\n",
      "Epoch: 6128 \t|| Train Loss: 0.09865017924965673 \t|| Test Loss: 0.23058799420775875\n",
      "Epoch: 6129 \t|| Train Loss: 0.09861550196528458 \t|| Test Loss: 0.2305464842100685\n",
      "Epoch: 6130 \t|| Train Loss: 0.09858165925300202 \t|| Test Loss: 0.2304362842127615\n",
      "Epoch: 6131 \t|| Train Loss: 0.09854675925478569 \t|| Test Loss: 0.23032608421545447\n",
      "Epoch: 6132 \t|| Train Loss: 0.09851298096977487 \t|| Test Loss: 0.23028457421776424\n",
      "Epoch: 6133 \t|| Train Loss: 0.09847823925813096 \t|| Test Loss: 0.23017437422045722\n",
      "Epoch: 6134 \t|| Train Loss: 0.09844407997270355 \t|| Test Loss: 0.230132864222767\n",
      "Epoch: 6135 \t|| Train Loss: 0.09840971926147622 \t|| Test Loss: 0.23002266422546\n",
      "Epoch: 6136 \t|| Train Loss: 0.09837517897563224 \t|| Test Loss: 0.22998115422776974\n",
      "Epoch: 6137 \t|| Train Loss: 0.0983411992648215 \t|| Test Loss: 0.22987095423046275\n",
      "Epoch: 6138 \t|| Train Loss: 0.09830629926660517 \t|| Test Loss: 0.22976075423315576\n",
      "Epoch: 6139 \t|| Train Loss: 0.0982726579801225 \t|| Test Loss: 0.2297192442354655\n",
      "Epoch: 6140 \t|| Train Loss: 0.09823777926995045 \t|| Test Loss: 0.2296090442381585\n",
      "Epoch: 6141 \t|| Train Loss: 0.0982037569830512 \t|| Test Loss: 0.22956753424046825\n",
      "Epoch: 6142 \t|| Train Loss: 0.09816925927329571 \t|| Test Loss: 0.22945733424316123\n",
      "Epoch: 6143 \t|| Train Loss: 0.09813485598597987 \t|| Test Loss: 0.22941582424547097\n",
      "Epoch: 6144 \t|| Train Loss: 0.098100739276641 \t|| Test Loss: 0.22930562424816398\n",
      "Epoch: 6145 \t|| Train Loss: 0.09806595498890858 \t|| Test Loss: 0.22926411425047374\n",
      "Epoch: 6146 \t|| Train Loss: 0.09803221927998627 \t|| Test Loss: 0.22915391425316672\n",
      "Epoch: 6147 \t|| Train Loss: 0.09799731928176994 \t|| Test Loss: 0.22904371425585973\n",
      "Epoch: 6148 \t|| Train Loss: 0.09796343399339884 \t|| Test Loss: 0.2290022042581695\n",
      "Epoch: 6149 \t|| Train Loss: 0.09792879928511522 \t|| Test Loss: 0.2288920042608625\n",
      "Epoch: 6150 \t|| Train Loss: 0.09789453299632753 \t|| Test Loss: 0.22885049426317222\n",
      "Epoch: 6151 \t|| Train Loss: 0.09786027928846049 \t|| Test Loss: 0.22874029426586526\n",
      "Epoch: 6152 \t|| Train Loss: 0.09782563199925622 \t|| Test Loss: 0.22869878426817497\n",
      "Epoch: 6153 \t|| Train Loss: 0.09779175929180575 \t|| Test Loss: 0.228588584270868\n",
      "Epoch: 6154 \t|| Train Loss: 0.09775685929358943 \t|| Test Loss: 0.228478384273561\n",
      "Epoch: 6155 \t|| Train Loss: 0.0977231110037465 \t|| Test Loss: 0.22843687427587073\n",
      "Epoch: 6156 \t|| Train Loss: 0.0976883392969347 \t|| Test Loss: 0.22832667427856376\n",
      "Epoch: 6157 \t|| Train Loss: 0.0976542100066752 \t|| Test Loss: 0.2282851642808735\n",
      "Epoch: 6158 \t|| Train Loss: 0.09761981930027998 \t|| Test Loss: 0.22817496428356648\n",
      "Epoch: 6159 \t|| Train Loss: 0.09758530900960385 \t|| Test Loss: 0.22813345428587625\n",
      "Epoch: 6160 \t|| Train Loss: 0.09755129930362527 \t|| Test Loss: 0.22802325428856926\n",
      "Epoch: 6161 \t|| Train Loss: 0.09751640801253256 \t|| Test Loss: 0.227981744290879\n",
      "Epoch: 6162 \t|| Train Loss: 0.09748277930697051 \t|| Test Loss: 0.22787154429357198\n",
      "Epoch: 6163 \t|| Train Loss: 0.09744787930875419 \t|| Test Loss: 0.22776134429626502\n",
      "Epoch: 6164 \t|| Train Loss: 0.09741388701702283 \t|| Test Loss: 0.22771983429857476\n",
      "Epoch: 6165 \t|| Train Loss: 0.09737935931209948 \t|| Test Loss: 0.22760963430126777\n",
      "Epoch: 6166 \t|| Train Loss: 0.09734498601995152 \t|| Test Loss: 0.2275681243035775\n",
      "Epoch: 6167 \t|| Train Loss: 0.09731083931544474 \t|| Test Loss: 0.2274579243062705\n",
      "Epoch: 6168 \t|| Train Loss: 0.09727608502288021 \t|| Test Loss: 0.22741641430858026\n",
      "Epoch: 6169 \t|| Train Loss: 0.09724231931879002 \t|| Test Loss: 0.22730621431127326\n",
      "Epoch: 6170 \t|| Train Loss: 0.09720741932057368 \t|| Test Loss: 0.22719601431396627\n",
      "Epoch: 6171 \t|| Train Loss: 0.09717356402737048 \t|| Test Loss: 0.227154504316276\n",
      "Epoch: 6172 \t|| Train Loss: 0.09713889932391895 \t|| Test Loss: 0.22704430431896902\n",
      "Epoch: 6173 \t|| Train Loss: 0.09710466303029917 \t|| Test Loss: 0.22700279432127876\n",
      "Epoch: 6174 \t|| Train Loss: 0.09707037932726423 \t|| Test Loss: 0.22689259432397177\n",
      "Epoch: 6175 \t|| Train Loss: 0.09703576203322786 \t|| Test Loss: 0.22685108432628148\n",
      "Epoch: 6176 \t|| Train Loss: 0.0970018593306095 \t|| Test Loss: 0.22674088432897452\n",
      "Epoch: 6177 \t|| Train Loss: 0.09696695933239316 \t|| Test Loss: 0.2266306843316675\n",
      "Epoch: 6178 \t|| Train Loss: 0.09693324103771815 \t|| Test Loss: 0.22658917433397724\n",
      "Epoch: 6179 \t|| Train Loss: 0.09689843933573844 \t|| Test Loss: 0.22647897433667025\n",
      "Epoch: 6180 \t|| Train Loss: 0.09686434004064683 \t|| Test Loss: 0.22643746433898\n",
      "Epoch: 6181 \t|| Train Loss: 0.09682991933908372 \t|| Test Loss: 0.226327264341673\n",
      "Epoch: 6182 \t|| Train Loss: 0.0967954390435755 \t|| Test Loss: 0.22628575434398276\n",
      "Epoch: 6183 \t|| Train Loss: 0.09676139934242899 \t|| Test Loss: 0.22617555434667574\n",
      "Epoch: 6184 \t|| Train Loss: 0.0967265380465042 \t|| Test Loss: 0.2261340443489855\n",
      "Epoch: 6185 \t|| Train Loss: 0.09669287934577427 \t|| Test Loss: 0.2260238443516785\n",
      "Epoch: 6186 \t|| Train Loss: 0.09665797934755793 \t|| Test Loss: 0.2259136443543715\n",
      "Epoch: 6187 \t|| Train Loss: 0.0966240170509945 \t|| Test Loss: 0.22587213435668124\n",
      "Epoch: 6188 \t|| Train Loss: 0.09658945935090321 \t|| Test Loss: 0.22576193435937425\n",
      "Epoch: 6189 \t|| Train Loss: 0.09655511605392317 \t|| Test Loss: 0.225720424361684\n",
      "Epoch: 6190 \t|| Train Loss: 0.09652093935424848 \t|| Test Loss: 0.22561022436437703\n",
      "Epoch: 6191 \t|| Train Loss: 0.09648621505685186 \t|| Test Loss: 0.22556871436668677\n",
      "Epoch: 6192 \t|| Train Loss: 0.09645241935759374 \t|| Test Loss: 0.22545851436937978\n",
      "Epoch: 6193 \t|| Train Loss: 0.09641751935937744 \t|| Test Loss: 0.22534831437207276\n",
      "Epoch: 6194 \t|| Train Loss: 0.09638369406134215 \t|| Test Loss: 0.2253068043743825\n",
      "Epoch: 6195 \t|| Train Loss: 0.0963489993627227 \t|| Test Loss: 0.2251966043770755\n",
      "Epoch: 6196 \t|| Train Loss: 0.09631479306427082 \t|| Test Loss: 0.22515509437938527\n",
      "Epoch: 6197 \t|| Train Loss: 0.09628047936606798 \t|| Test Loss: 0.22504489438207828\n",
      "Epoch: 6198 \t|| Train Loss: 0.09624589206719951 \t|| Test Loss: 0.22500338438438802\n",
      "Epoch: 6199 \t|| Train Loss: 0.09621195936941326 \t|| Test Loss: 0.224893184387081\n",
      "Epoch: 6200 \t|| Train Loss: 0.09617705937119692 \t|| Test Loss: 0.22478298438977404\n",
      "Epoch: 6201 \t|| Train Loss: 0.0961433710716898 \t|| Test Loss: 0.22474147439208378\n",
      "Epoch: 6202 \t|| Train Loss: 0.0961085393745422 \t|| Test Loss: 0.22463127439477676\n",
      "Epoch: 6203 \t|| Train Loss: 0.09607447007461847 \t|| Test Loss: 0.22458976439708653\n",
      "Epoch: 6204 \t|| Train Loss: 0.09604001937788746 \t|| Test Loss: 0.2244795643997795\n",
      "Epoch: 6205 \t|| Train Loss: 0.09600556907754716 \t|| Test Loss: 0.22443805440208928\n",
      "Epoch: 6206 \t|| Train Loss: 0.09597149938123276 \t|| Test Loss: 0.22432785440478228\n",
      "Epoch: 6207 \t|| Train Loss: 0.09593666808047584 \t|| Test Loss: 0.224286344407092\n",
      "Epoch: 6208 \t|| Train Loss: 0.09590297938457802 \t|| Test Loss: 0.22417614440978503\n",
      "Epoch: 6209 \t|| Train Loss: 0.09586807938636169 \t|| Test Loss: 0.22406594441247804\n",
      "Epoch: 6210 \t|| Train Loss: 0.09583414708496613 \t|| Test Loss: 0.22402443441478775\n",
      "Epoch: 6211 \t|| Train Loss: 0.09579955938970697 \t|| Test Loss: 0.22391423441748076\n",
      "Epoch: 6212 \t|| Train Loss: 0.0957652460878948 \t|| Test Loss: 0.22387272441979053\n",
      "Epoch: 6213 \t|| Train Loss: 0.09573103939305223 \t|| Test Loss: 0.2237625244224835\n",
      "Epoch: 6214 \t|| Train Loss: 0.09569634509082349 \t|| Test Loss: 0.22372101442479328\n",
      "Epoch: 6215 \t|| Train Loss: 0.09566251939639751 \t|| Test Loss: 0.22361081442748626\n",
      "Epoch: 6216 \t|| Train Loss: 0.09562761939818118 \t|| Test Loss: 0.22350061443017927\n",
      "Epoch: 6217 \t|| Train Loss: 0.09559382409531378 \t|| Test Loss: 0.223459104432489\n",
      "Epoch: 6218 \t|| Train Loss: 0.09555909940152645 \t|| Test Loss: 0.22334890443518202\n",
      "Epoch: 6219 \t|| Train Loss: 0.09552492309824247 \t|| Test Loss: 0.22330739443749176\n",
      "Epoch: 6220 \t|| Train Loss: 0.09549057940487173 \t|| Test Loss: 0.2231971944401848\n",
      "Epoch: 6221 \t|| Train Loss: 0.09545602210117114 \t|| Test Loss: 0.22315568444249453\n",
      "Epoch: 6222 \t|| Train Loss: 0.09542205940821699 \t|| Test Loss: 0.22304548444518751\n",
      "Epoch: 6223 \t|| Train Loss: 0.09538715941000067 \t|| Test Loss: 0.22293528444788055\n",
      "Epoch: 6224 \t|| Train Loss: 0.09535350110566143 \t|| Test Loss: 0.2228937744501903\n",
      "Epoch: 6225 \t|| Train Loss: 0.09531863941334595 \t|| Test Loss: 0.22278357445288327\n",
      "Epoch: 6226 \t|| Train Loss: 0.09528460010859012 \t|| Test Loss: 0.222742064455193\n",
      "Epoch: 6227 \t|| Train Loss: 0.09525011941669122 \t|| Test Loss: 0.22263186445788602\n",
      "Epoch: 6228 \t|| Train Loss: 0.09521569911151881 \t|| Test Loss: 0.2225903544601957\n",
      "Epoch: 6229 \t|| Train Loss: 0.0951815994200365 \t|| Test Loss: 0.22248015446288877\n",
      "Epoch: 6230 \t|| Train Loss: 0.09514679811444748 \t|| Test Loss: 0.2224386444651985\n",
      "Epoch: 6231 \t|| Train Loss: 0.09511307942338176 \t|| Test Loss: 0.22232844446789155\n",
      "Epoch: 6232 \t|| Train Loss: 0.09507817942516543 \t|| Test Loss: 0.22221824447058453\n",
      "Epoch: 6233 \t|| Train Loss: 0.09504427711893777 \t|| Test Loss: 0.2221767344728943\n",
      "Epoch: 6234 \t|| Train Loss: 0.09500965942851072 \t|| Test Loss: 0.22206653447558722\n",
      "Epoch: 6235 \t|| Train Loss: 0.09497537612186646 \t|| Test Loss: 0.22202502447789704\n",
      "Epoch: 6236 \t|| Train Loss: 0.09494113943185598 \t|| Test Loss: 0.22191482448059002\n",
      "Epoch: 6237 \t|| Train Loss: 0.09490647512479515 \t|| Test Loss: 0.22187331448289976\n",
      "Epoch: 6238 \t|| Train Loss: 0.09487261943520124 \t|| Test Loss: 0.22176311448559277\n",
      "Epoch: 6239 \t|| Train Loss: 0.09483771943698492 \t|| Test Loss: 0.22165291448828578\n",
      "Epoch: 6240 \t|| Train Loss: 0.09480395412928541 \t|| Test Loss: 0.22161140449059552\n",
      "Epoch: 6241 \t|| Train Loss: 0.0947691994403302 \t|| Test Loss: 0.22150120449328853\n",
      "Epoch: 6242 \t|| Train Loss: 0.09473505313221411 \t|| Test Loss: 0.22145969449559827\n",
      "Epoch: 6243 \t|| Train Loss: 0.09470067944367547 \t|| Test Loss: 0.22134949449829128\n",
      "Epoch: 6244 \t|| Train Loss: 0.09466615213514278 \t|| Test Loss: 0.22130798450060102\n",
      "Epoch: 6245 \t|| Train Loss: 0.09463215944702072 \t|| Test Loss: 0.22119778450329403\n",
      "Epoch: 6246 \t|| Train Loss: 0.09459725944880443 \t|| Test Loss: 0.22108758450598703\n",
      "Epoch: 6247 \t|| Train Loss: 0.09456363113963309 \t|| Test Loss: 0.2210460745082968\n",
      "Epoch: 6248 \t|| Train Loss: 0.09452873945214968 \t|| Test Loss: 0.22093587451098978\n",
      "Epoch: 6249 \t|| Train Loss: 0.09449473014256175 \t|| Test Loss: 0.22089436451329955\n",
      "Epoch: 6250 \t|| Train Loss: 0.09446021945549496 \t|| Test Loss: 0.22078416451599253\n",
      "Epoch: 6251 \t|| Train Loss: 0.09442582914549044 \t|| Test Loss: 0.2207426545183023\n",
      "Epoch: 6252 \t|| Train Loss: 0.09439169945884023 \t|| Test Loss: 0.22063245452099528\n",
      "Epoch: 6253 \t|| Train Loss: 0.09435692814841913 \t|| Test Loss: 0.22059094452330502\n",
      "Epoch: 6254 \t|| Train Loss: 0.0943231794621855 \t|| Test Loss: 0.22048074452599803\n",
      "Epoch: 6255 \t|| Train Loss: 0.09428827946396919 \t|| Test Loss: 0.22037054452869104\n",
      "Epoch: 6256 \t|| Train Loss: 0.09425440715290942 \t|| Test Loss: 0.22032903453100078\n",
      "Epoch: 6257 \t|| Train Loss: 0.09421975946731445 \t|| Test Loss: 0.22021883453369379\n",
      "Epoch: 6258 \t|| Train Loss: 0.09418550615583811 \t|| Test Loss: 0.22017732453600355\n",
      "Epoch: 6259 \t|| Train Loss: 0.09415123947065972 \t|| Test Loss: 0.22006712453869656\n",
      "Epoch: 6260 \t|| Train Loss: 0.09411660515876678 \t|| Test Loss: 0.2200256145410063\n",
      "Epoch: 6261 \t|| Train Loss: 0.09408271947400501 \t|| Test Loss: 0.21991541454369928\n",
      "Epoch: 6262 \t|| Train Loss: 0.09404781947578868 \t|| Test Loss: 0.21980521454639232\n",
      "Epoch: 6263 \t|| Train Loss: 0.09401408416325709 \t|| Test Loss: 0.21976370454870203\n",
      "Epoch: 6264 \t|| Train Loss: 0.09397929947913394 \t|| Test Loss: 0.21965350455139504\n",
      "Epoch: 6265 \t|| Train Loss: 0.09394518316618576 \t|| Test Loss: 0.21961199455370473\n",
      "Epoch: 6266 \t|| Train Loss: 0.09391077948247922 \t|| Test Loss: 0.2195017945563978\n",
      "Epoch: 6267 \t|| Train Loss: 0.09387628216911445 \t|| Test Loss: 0.21946028455870753\n",
      "Epoch: 6268 \t|| Train Loss: 0.0938422594858245 \t|| Test Loss: 0.21935008456140054\n",
      "Epoch: 6269 \t|| Train Loss: 0.09380738117204312 \t|| Test Loss: 0.2193085745637103\n",
      "Epoch: 6270 \t|| Train Loss: 0.09377373948916977 \t|| Test Loss: 0.21919837456640331\n",
      "Epoch: 6271 \t|| Train Loss: 0.09373883949095344 \t|| Test Loss: 0.21908817456909624\n",
      "Epoch: 6272 \t|| Train Loss: 0.09370486017653341 \t|| Test Loss: 0.21904666457140604\n",
      "Epoch: 6273 \t|| Train Loss: 0.09367031949429871 \t|| Test Loss: 0.218936464574099\n",
      "Epoch: 6274 \t|| Train Loss: 0.09363595917946209 \t|| Test Loss: 0.21889495457640878\n",
      "Epoch: 6275 \t|| Train Loss: 0.09360179949764398 \t|| Test Loss: 0.2187847545791018\n",
      "Epoch: 6276 \t|| Train Loss: 0.09356705818239078 \t|| Test Loss: 0.21874324458141153\n",
      "Epoch: 6277 \t|| Train Loss: 0.09353327950098926 \t|| Test Loss: 0.21863304458410454\n",
      "Epoch: 6278 \t|| Train Loss: 0.09349837950277293 \t|| Test Loss: 0.21852284458679755\n",
      "Epoch: 6279 \t|| Train Loss: 0.09346453718688105 \t|| Test Loss: 0.2184813345891073\n",
      "Epoch: 6280 \t|| Train Loss: 0.09342985950611819 \t|| Test Loss: 0.2183711345918003\n",
      "Epoch: 6281 \t|| Train Loss: 0.09339563618980974 \t|| Test Loss: 0.21832962459411004\n",
      "Epoch: 6282 \t|| Train Loss: 0.09336133950946347 \t|| Test Loss: 0.21821942459680305\n",
      "Epoch: 6283 \t|| Train Loss: 0.09332673519273844 \t|| Test Loss: 0.2181779145991128\n",
      "Epoch: 6284 \t|| Train Loss: 0.09329281951280874 \t|| Test Loss: 0.2180677146018058\n",
      "Epoch: 6285 \t|| Train Loss: 0.09325791951459242 \t|| Test Loss: 0.2179575146044988\n",
      "Epoch: 6286 \t|| Train Loss: 0.09322421419722872 \t|| Test Loss: 0.21791600460680854\n",
      "Epoch: 6287 \t|| Train Loss: 0.09318939951793768 \t|| Test Loss: 0.21780580460950155\n",
      "Epoch: 6288 \t|| Train Loss: 0.0931553132001574 \t|| Test Loss: 0.2177642946118113\n",
      "Epoch: 6289 \t|| Train Loss: 0.09312087952128296 \t|| Test Loss: 0.21765409461450425\n",
      "Epoch: 6290 \t|| Train Loss: 0.09308641220308608 \t|| Test Loss: 0.21761258461681404\n",
      "Epoch: 6291 \t|| Train Loss: 0.09305235952462823 \t|| Test Loss: 0.21750238461950705\n",
      "Epoch: 6292 \t|| Train Loss: 0.09301751120601476 \t|| Test Loss: 0.21746087462181674\n",
      "Epoch: 6293 \t|| Train Loss: 0.0929838395279735 \t|| Test Loss: 0.21735067462450974\n",
      "Epoch: 6294 \t|| Train Loss: 0.09294893952975718 \t|| Test Loss: 0.2172404746272028\n",
      "Epoch: 6295 \t|| Train Loss: 0.09291499021050506 \t|| Test Loss: 0.21719896462951255\n",
      "Epoch: 6296 \t|| Train Loss: 0.09288041953310247 \t|| Test Loss: 0.21708876463220558\n",
      "Epoch: 6297 \t|| Train Loss: 0.09284608921343375 \t|| Test Loss: 0.2170472546345153\n",
      "Epoch: 6298 \t|| Train Loss: 0.09281189953644772 \t|| Test Loss: 0.21693705463720833\n",
      "Epoch: 6299 \t|| Train Loss: 0.09277718821636242 \t|| Test Loss: 0.21689554463951805\n",
      "Epoch: 6300 \t|| Train Loss: 0.09274337953979302 \t|| Test Loss: 0.21678534464221105\n",
      "Epoch: 6301 \t|| Train Loss: 0.09270847954157667 \t|| Test Loss: 0.2166751446449041\n",
      "Epoch: 6302 \t|| Train Loss: 0.09267466722085271 \t|| Test Loss: 0.2166336346472138\n",
      "Epoch: 6303 \t|| Train Loss: 0.09263995954492195 \t|| Test Loss: 0.2165234346499068\n",
      "Epoch: 6304 \t|| Train Loss: 0.09260576622378139 \t|| Test Loss: 0.2164819246522165\n",
      "Epoch: 6305 \t|| Train Loss: 0.09257143954826721 \t|| Test Loss: 0.21637172465490956\n",
      "Epoch: 6306 \t|| Train Loss: 0.09253686522671006 \t|| Test Loss: 0.21633021465721924\n",
      "Epoch: 6307 \t|| Train Loss: 0.09250291955161248 \t|| Test Loss: 0.2162200146599123\n",
      "Epoch: 6308 \t|| Train Loss: 0.09246801955339615 \t|| Test Loss: 0.21610981466260526\n",
      "Epoch: 6309 \t|| Train Loss: 0.09243434423120035 \t|| Test Loss: 0.21606830466491506\n",
      "Epoch: 6310 \t|| Train Loss: 0.09239949955674143 \t|| Test Loss: 0.215958104667608\n",
      "Epoch: 6311 \t|| Train Loss: 0.09236544323412904 \t|| Test Loss: 0.2159165946699178\n",
      "Epoch: 6312 \t|| Train Loss: 0.0923309795600867 \t|| Test Loss: 0.21580639467261076\n",
      "Epoch: 6313 \t|| Train Loss: 0.09229654223705773 \t|| Test Loss: 0.21576488467492055\n",
      "Epoch: 6314 \t|| Train Loss: 0.09226245956343197 \t|| Test Loss: 0.21565468467761356\n",
      "Epoch: 6315 \t|| Train Loss: 0.09222764123998642 \t|| Test Loss: 0.2156131746799233\n",
      "Epoch: 6316 \t|| Train Loss: 0.09219393956677725 \t|| Test Loss: 0.2155029746826163\n",
      "Epoch: 6317 \t|| Train Loss: 0.09215903956856092 \t|| Test Loss: 0.21539277468530932\n",
      "Epoch: 6318 \t|| Train Loss: 0.0921251202444767 \t|| Test Loss: 0.21535126468761906\n",
      "Epoch: 6319 \t|| Train Loss: 0.0920905195719062 \t|| Test Loss: 0.21524106469031207\n",
      "Epoch: 6320 \t|| Train Loss: 0.09205621924740537 \t|| Test Loss: 0.2151995546926218\n",
      "Epoch: 6321 \t|| Train Loss: 0.09202199957525148 \t|| Test Loss: 0.21508935469531482\n",
      "Epoch: 6322 \t|| Train Loss: 0.09198731825033406 \t|| Test Loss: 0.21504784469762456\n",
      "Epoch: 6323 \t|| Train Loss: 0.09195347957859674 \t|| Test Loss: 0.21493764470031756\n",
      "Epoch: 6324 \t|| Train Loss: 0.09191857958038041 \t|| Test Loss: 0.21482744470301052\n",
      "Epoch: 6325 \t|| Train Loss: 0.09188479725482436 \t|| Test Loss: 0.2147859347053203\n",
      "Epoch: 6326 \t|| Train Loss: 0.09185005958372569 \t|| Test Loss: 0.21467573470801327\n",
      "Epoch: 6327 \t|| Train Loss: 0.09181589625775305 \t|| Test Loss: 0.214634224710323\n",
      "Epoch: 6328 \t|| Train Loss: 0.09178153958707096 \t|| Test Loss: 0.21452402471301601\n",
      "Epoch: 6329 \t|| Train Loss: 0.09174699526068171 \t|| Test Loss: 0.21448251471532576\n",
      "Epoch: 6330 \t|| Train Loss: 0.09171301959041625 \t|| Test Loss: 0.21437231471801876\n",
      "Epoch: 6331 \t|| Train Loss: 0.09167811959219992 \t|| Test Loss: 0.21426211472071183\n",
      "Epoch: 6332 \t|| Train Loss: 0.091644474265172 \t|| Test Loss: 0.2142206047230215\n",
      "Epoch: 6333 \t|| Train Loss: 0.09160959959554517 \t|| Test Loss: 0.21411040472571458\n",
      "Epoch: 6334 \t|| Train Loss: 0.09157557326810069 \t|| Test Loss: 0.21406889472802432\n",
      "Epoch: 6335 \t|| Train Loss: 0.09154107959889046 \t|| Test Loss: 0.21395869473071727\n",
      "Epoch: 6336 \t|| Train Loss: 0.09150667227102938 \t|| Test Loss: 0.21391718473302707\n",
      "Epoch: 6337 \t|| Train Loss: 0.09147255960223573 \t|| Test Loss: 0.21380698473572002\n",
      "Epoch: 6338 \t|| Train Loss: 0.09143777127395805 \t|| Test Loss: 0.21376547473802981\n",
      "Epoch: 6339 \t|| Train Loss: 0.09140403960558101 \t|| Test Loss: 0.21365527474072277\n",
      "Epoch: 6340 \t|| Train Loss: 0.09136913960736467 \t|| Test Loss: 0.21354507474341577\n",
      "Epoch: 6341 \t|| Train Loss: 0.09133525027844834 \t|| Test Loss: 0.21350356474572552\n",
      "Epoch: 6342 \t|| Train Loss: 0.09130061961070995 \t|| Test Loss: 0.21339336474841852\n",
      "Epoch: 6343 \t|| Train Loss: 0.09126634928137703 \t|| Test Loss: 0.21335185475072835\n",
      "Epoch: 6344 \t|| Train Loss: 0.09123209961405523 \t|| Test Loss: 0.21324165475342133\n",
      "Epoch: 6345 \t|| Train Loss: 0.0911974482843057 \t|| Test Loss: 0.213200144755731\n",
      "Epoch: 6346 \t|| Train Loss: 0.0911635796174005 \t|| Test Loss: 0.21308994475842402\n",
      "Epoch: 6347 \t|| Train Loss: 0.09112867961918417 \t|| Test Loss: 0.21297974476111703\n",
      "Epoch: 6348 \t|| Train Loss: 0.091094927288796 \t|| Test Loss: 0.21293823476342683\n",
      "Epoch: 6349 \t|| Train Loss: 0.09106015962252943 \t|| Test Loss: 0.21282803476611983\n",
      "Epoch: 6350 \t|| Train Loss: 0.09102602629172467 \t|| Test Loss: 0.21278652476842952\n",
      "Epoch: 6351 \t|| Train Loss: 0.09099163962587471 \t|| Test Loss: 0.21267632477112253\n",
      "Epoch: 6352 \t|| Train Loss: 0.09095712529465336 \t|| Test Loss: 0.21263481477343232\n",
      "Epoch: 6353 \t|| Train Loss: 0.09092311962921998 \t|| Test Loss: 0.21252461477612533\n",
      "Epoch: 6354 \t|| Train Loss: 0.09088822429758205 \t|| Test Loss: 0.21248310477843507\n",
      "Epoch: 6355 \t|| Train Loss: 0.09085459963256526 \t|| Test Loss: 0.21237290478112808\n",
      "Epoch: 6356 \t|| Train Loss: 0.09081969963434892 \t|| Test Loss: 0.2122627047838211\n",
      "Epoch: 6357 \t|| Train Loss: 0.09078570330207233 \t|| Test Loss: 0.21222119478613083\n",
      "Epoch: 6358 \t|| Train Loss: 0.0907511796376942 \t|| Test Loss: 0.21211099478882384\n",
      "Epoch: 6359 \t|| Train Loss: 0.09071680230500102 \t|| Test Loss: 0.2120694847911336\n",
      "Epoch: 6360 \t|| Train Loss: 0.09068265964103947 \t|| Test Loss: 0.21195928479382653\n",
      "Epoch: 6361 \t|| Train Loss: 0.0906479013079297 \t|| Test Loss: 0.21191777479613627\n",
      "Epoch: 6362 \t|| Train Loss: 0.09061413964438476 \t|| Test Loss: 0.21180757479882933\n",
      "Epoch: 6363 \t|| Train Loss: 0.09057923964616842 \t|| Test Loss: 0.21169737480152234\n",
      "Epoch: 6364 \t|| Train Loss: 0.09054538031241999 \t|| Test Loss: 0.21165586480383203\n",
      "Epoch: 6365 \t|| Train Loss: 0.09051071964951368 \t|| Test Loss: 0.21154566480652504\n",
      "Epoch: 6366 \t|| Train Loss: 0.09047647931534866 \t|| Test Loss: 0.21150415480883478\n",
      "Epoch: 6367 \t|| Train Loss: 0.09044219965285896 \t|| Test Loss: 0.21139395481152778\n",
      "Epoch: 6368 \t|| Train Loss: 0.09040757831827735 \t|| Test Loss: 0.21135244481383753\n",
      "Epoch: 6369 \t|| Train Loss: 0.09037367965620423 \t|| Test Loss: 0.21124224481653053\n",
      "Epoch: 6370 \t|| Train Loss: 0.09033877965798791 \t|| Test Loss: 0.21113204481922354\n",
      "Epoch: 6371 \t|| Train Loss: 0.09030505732276764 \t|| Test Loss: 0.21109053482153328\n",
      "Epoch: 6372 \t|| Train Loss: 0.09027025966133317 \t|| Test Loss: 0.2109803348242263\n",
      "Epoch: 6373 \t|| Train Loss: 0.09023615632569633 \t|| Test Loss: 0.21093882482653603\n",
      "Epoch: 6374 \t|| Train Loss: 0.09020173966467845 \t|| Test Loss: 0.21082862482922904\n",
      "Epoch: 6375 \t|| Train Loss: 0.090167255328625 \t|| Test Loss: 0.21078711483153878\n",
      "Epoch: 6376 \t|| Train Loss: 0.09013321966802373 \t|| Test Loss: 0.2106769148342318\n",
      "Epoch: 6377 \t|| Train Loss: 0.09009835433155368 \t|| Test Loss: 0.21063540483654153\n",
      "Epoch: 6378 \t|| Train Loss: 0.090064699671369 \t|| Test Loss: 0.21052520483923454\n",
      "Epoch: 6379 \t|| Train Loss: 0.09002979967315267 \t|| Test Loss: 0.21041500484192754\n",
      "Epoch: 6380 \t|| Train Loss: 0.08999583333604397 \t|| Test Loss: 0.21037349484423729\n",
      "Epoch: 6381 \t|| Train Loss: 0.08996127967649796 \t|| Test Loss: 0.2102632948469303\n",
      "Epoch: 6382 \t|| Train Loss: 0.08992693233897266 \t|| Test Loss: 0.21022178484924003\n",
      "Epoch: 6383 \t|| Train Loss: 0.08989275967984323 \t|| Test Loss: 0.21011158485193304\n",
      "Epoch: 6384 \t|| Train Loss: 0.08985803134190133 \t|| Test Loss: 0.21007007485424278\n",
      "Epoch: 6385 \t|| Train Loss: 0.0898242396831885 \t|| Test Loss: 0.2099598748569358\n",
      "Epoch: 6386 \t|| Train Loss: 0.08978933968497216 \t|| Test Loss: 0.20984967485962885\n",
      "Epoch: 6387 \t|| Train Loss: 0.08975551034639165 \t|| Test Loss: 0.20980816486193854\n",
      "Epoch: 6388 \t|| Train Loss: 0.08972081968831744 \t|| Test Loss: 0.20969796486463155\n",
      "Epoch: 6389 \t|| Train Loss: 0.0896866093493203 \t|| Test Loss: 0.2096564548669413\n",
      "Epoch: 6390 \t|| Train Loss: 0.0896522996916627 \t|| Test Loss: 0.2095462548696343\n",
      "Epoch: 6391 \t|| Train Loss: 0.089617708352249 \t|| Test Loss: 0.20950474487194404\n",
      "Epoch: 6392 \t|| Train Loss: 0.08958377969500798 \t|| Test Loss: 0.20939454487463705\n",
      "Epoch: 6393 \t|| Train Loss: 0.08954887969679166 \t|| Test Loss: 0.20928434487733005\n",
      "Epoch: 6394 \t|| Train Loss: 0.08951518735673929 \t|| Test Loss: 0.2092428348796398\n",
      "Epoch: 6395 \t|| Train Loss: 0.08948035970013693 \t|| Test Loss: 0.20913263488233275\n",
      "Epoch: 6396 \t|| Train Loss: 0.08944628635966798 \t|| Test Loss: 0.20909112488464254\n",
      "Epoch: 6397 \t|| Train Loss: 0.08941183970348218 \t|| Test Loss: 0.20898092488733555\n",
      "Epoch: 6398 \t|| Train Loss: 0.08937738536259665 \t|| Test Loss: 0.2089394148896453\n",
      "Epoch: 6399 \t|| Train Loss: 0.08934331970682748 \t|| Test Loss: 0.2088292148923383\n",
      "Epoch: 6400 \t|| Train Loss: 0.08930848436552533 \t|| Test Loss: 0.20878770489464804\n",
      "Epoch: 6401 \t|| Train Loss: 0.08927479971017274 \t|| Test Loss: 0.20867750489734105\n",
      "Epoch: 6402 \t|| Train Loss: 0.08923989971195642 \t|| Test Loss: 0.20856730490003406\n",
      "Epoch: 6403 \t|| Train Loss: 0.0892059633700156 \t|| Test Loss: 0.2085257949023438\n",
      "Epoch: 6404 \t|| Train Loss: 0.08917137971530169 \t|| Test Loss: 0.2084155949050368\n",
      "Epoch: 6405 \t|| Train Loss: 0.0891370623729443 \t|| Test Loss: 0.20837408490734655\n",
      "Epoch: 6406 \t|| Train Loss: 0.08910285971864698 \t|| Test Loss: 0.20826388491003955\n",
      "Epoch: 6407 \t|| Train Loss: 0.08906816137587299 \t|| Test Loss: 0.2082223749123493\n",
      "Epoch: 6408 \t|| Train Loss: 0.08903433972199223 \t|| Test Loss: 0.2081121749150423\n",
      "Epoch: 6409 \t|| Train Loss: 0.08899943972377591 \t|| Test Loss: 0.2080019749177353\n",
      "Epoch: 6410 \t|| Train Loss: 0.08896564038036328 \t|| Test Loss: 0.20796046492004505\n",
      "Epoch: 6411 \t|| Train Loss: 0.08893091972712118 \t|| Test Loss: 0.20785026492273811\n",
      "Epoch: 6412 \t|| Train Loss: 0.08889673938329197 \t|| Test Loss: 0.2078087549250478\n",
      "Epoch: 6413 \t|| Train Loss: 0.08886239973046646 \t|| Test Loss: 0.2076985549277408\n",
      "Epoch: 6414 \t|| Train Loss: 0.08882783838622063 \t|| Test Loss: 0.2076570449300505\n",
      "Epoch: 6415 \t|| Train Loss: 0.08879387973381174 \t|| Test Loss: 0.20754684493274356\n",
      "Epoch: 6416 \t|| Train Loss: 0.08875897973559539 \t|| Test Loss: 0.20743664493543656\n",
      "Epoch: 6417 \t|| Train Loss: 0.08872531739071093 \t|| Test Loss: 0.2073951349377463\n",
      "Epoch: 6418 \t|| Train Loss: 0.08869045973894067 \t|| Test Loss: 0.2072849349404393\n",
      "Epoch: 6419 \t|| Train Loss: 0.0886564163936396 \t|| Test Loss: 0.20724342494274905\n",
      "Epoch: 6420 \t|| Train Loss: 0.08862193974228595 \t|| Test Loss: 0.20713322494544206\n",
      "Epoch: 6421 \t|| Train Loss: 0.08858751539656828 \t|| Test Loss: 0.2070917149477518\n",
      "Epoch: 6422 \t|| Train Loss: 0.08855341974563122 \t|| Test Loss: 0.2069815149504448\n",
      "Epoch: 6423 \t|| Train Loss: 0.08851861439949699 \t|| Test Loss: 0.20694000495275455\n",
      "Epoch: 6424 \t|| Train Loss: 0.0884848997489765 \t|| Test Loss: 0.20682980495544756\n",
      "Epoch: 6425 \t|| Train Loss: 0.08844999975076016 \t|| Test Loss: 0.2067196049581405\n",
      "Epoch: 6426 \t|| Train Loss: 0.08841609340398726 \t|| Test Loss: 0.2066780949604503\n",
      "Epoch: 6427 \t|| Train Loss: 0.08838147975410544 \t|| Test Loss: 0.20656789496314332\n",
      "Epoch: 6428 \t|| Train Loss: 0.08834719240691595 \t|| Test Loss: 0.206526384965453\n",
      "Epoch: 6429 \t|| Train Loss: 0.08831295975745071 \t|| Test Loss: 0.20641618496814607\n",
      "Epoch: 6430 \t|| Train Loss: 0.08827829140984463 \t|| Test Loss: 0.20637467497045575\n",
      "Epoch: 6431 \t|| Train Loss: 0.08824443976079599 \t|| Test Loss: 0.20626447497314881\n",
      "Epoch: 6432 \t|| Train Loss: 0.08820953976257964 \t|| Test Loss: 0.20615427497584182\n",
      "Epoch: 6433 \t|| Train Loss: 0.08817577041433491 \t|| Test Loss: 0.20611276497815156\n",
      "Epoch: 6434 \t|| Train Loss: 0.08814101976592492 \t|| Test Loss: 0.20600256498084457\n",
      "Epoch: 6435 \t|| Train Loss: 0.0881068694172636 \t|| Test Loss: 0.2059610549831543\n",
      "Epoch: 6436 \t|| Train Loss: 0.0880724997692702 \t|| Test Loss: 0.20585085498584732\n",
      "Epoch: 6437 \t|| Train Loss: 0.08803796842019228 \t|| Test Loss: 0.20580934498815706\n",
      "Epoch: 6438 \t|| Train Loss: 0.08800397977261547 \t|| Test Loss: 0.20569914499085007\n",
      "Epoch: 6439 \t|| Train Loss: 0.08796907977439913 \t|| Test Loss: 0.20558894499354308\n",
      "Epoch: 6440 \t|| Train Loss: 0.08793544742468257 \t|| Test Loss: 0.20554743499585282\n",
      "Epoch: 6441 \t|| Train Loss: 0.08790055977774443 \t|| Test Loss: 0.20543723499854583\n",
      "Epoch: 6442 \t|| Train Loss: 0.08786654642761124 \t|| Test Loss: 0.20539572500085557\n",
      "Epoch: 6443 \t|| Train Loss: 0.0878320397810897 \t|| Test Loss: 0.20528552500354857\n",
      "Epoch: 6444 \t|| Train Loss: 0.08779764543053994 \t|| Test Loss: 0.20524401500585832\n",
      "Epoch: 6445 \t|| Train Loss: 0.08776351978443497 \t|| Test Loss: 0.20513381500855132\n",
      "Epoch: 6446 \t|| Train Loss: 0.08772874443346863 \t|| Test Loss: 0.20509230501086106\n",
      "Epoch: 6447 \t|| Train Loss: 0.08769499978778024 \t|| Test Loss: 0.20498210501355407\n",
      "Epoch: 6448 \t|| Train Loss: 0.0876600997895639 \t|| Test Loss: 0.20487190501624708\n",
      "Epoch: 6449 \t|| Train Loss: 0.08762622343795891 \t|| Test Loss: 0.20483039501855682\n",
      "Epoch: 6450 \t|| Train Loss: 0.08759157979290919 \t|| Test Loss: 0.20472019502124983\n",
      "Epoch: 6451 \t|| Train Loss: 0.08755732244088761 \t|| Test Loss: 0.20467868502355957\n",
      "Epoch: 6452 \t|| Train Loss: 0.08752305979625445 \t|| Test Loss: 0.20456848502625258\n",
      "Epoch: 6453 \t|| Train Loss: 0.08748842144381627 \t|| Test Loss: 0.20452697502856226\n",
      "Epoch: 6454 \t|| Train Loss: 0.08745453979959973 \t|| Test Loss: 0.20441677503125527\n",
      "Epoch: 6455 \t|| Train Loss: 0.08741963980138341 \t|| Test Loss: 0.20430657503394833\n",
      "Epoch: 6456 \t|| Train Loss: 0.08738590044830656 \t|| Test Loss: 0.20426506503625808\n",
      "Epoch: 6457 \t|| Train Loss: 0.08735111980472868 \t|| Test Loss: 0.20415486503895108\n",
      "Epoch: 6458 \t|| Train Loss: 0.08731699945123524 \t|| Test Loss: 0.20411335504126077\n",
      "Epoch: 6459 \t|| Train Loss: 0.08728259980807394 \t|| Test Loss: 0.20400315504395383\n",
      "Epoch: 6460 \t|| Train Loss: 0.08724809845416392 \t|| Test Loss: 0.20396164504626357\n",
      "Epoch: 6461 \t|| Train Loss: 0.08721407981141922 \t|| Test Loss: 0.20385144504895653\n",
      "Epoch: 6462 \t|| Train Loss: 0.08717919745709261 \t|| Test Loss: 0.20380993505126627\n",
      "Epoch: 6463 \t|| Train Loss: 0.08714555981476449 \t|| Test Loss: 0.20369973505395933\n",
      "Epoch: 6464 \t|| Train Loss: 0.08711065981654817 \t|| Test Loss: 0.20358953505665234\n",
      "Epoch: 6465 \t|| Train Loss: 0.0870766764615829 \t|| Test Loss: 0.20354802505896208\n",
      "Epoch: 6466 \t|| Train Loss: 0.08704213981989344 \t|| Test Loss: 0.2034378250616551\n",
      "Epoch: 6467 \t|| Train Loss: 0.0870077754645116 \t|| Test Loss: 0.20339631506396483\n",
      "Epoch: 6468 \t|| Train Loss: 0.08697361982323872 \t|| Test Loss: 0.20328611506665784\n",
      "Epoch: 6469 \t|| Train Loss: 0.08693887446744027 \t|| Test Loss: 0.20324460506896752\n",
      "Epoch: 6470 \t|| Train Loss: 0.08690509982658398 \t|| Test Loss: 0.20313440507166058\n",
      "Epoch: 6471 \t|| Train Loss: 0.08687019982836765 \t|| Test Loss: 0.2030242050743536\n",
      "Epoch: 6472 \t|| Train Loss: 0.08683635347193056 \t|| Test Loss: 0.20298269507666333\n",
      "Epoch: 6473 \t|| Train Loss: 0.08680167983171291 \t|| Test Loss: 0.20287249507935634\n",
      "Epoch: 6474 \t|| Train Loss: 0.08676745247485923 \t|| Test Loss: 0.20283098508166608\n",
      "Epoch: 6475 \t|| Train Loss: 0.08673315983505821 \t|| Test Loss: 0.2027207850843591\n",
      "Epoch: 6476 \t|| Train Loss: 0.08669855147778792 \t|| Test Loss: 0.20267927508666883\n",
      "Epoch: 6477 \t|| Train Loss: 0.08666463983840347 \t|| Test Loss: 0.20256907508936184\n",
      "Epoch: 6478 \t|| Train Loss: 0.08662973984018714 \t|| Test Loss: 0.20245887509205485\n",
      "Epoch: 6479 \t|| Train Loss: 0.08659603048227822 \t|| Test Loss: 0.2024173650943646\n",
      "Epoch: 6480 \t|| Train Loss: 0.08656121984353242 \t|| Test Loss: 0.2023071650970576\n",
      "Epoch: 6481 \t|| Train Loss: 0.0865271294852069 \t|| Test Loss: 0.20226565509936734\n",
      "Epoch: 6482 \t|| Train Loss: 0.0864926998468777 \t|| Test Loss: 0.2021554551020603\n",
      "Epoch: 6483 \t|| Train Loss: 0.08645822848813559 \t|| Test Loss: 0.20211394510437009\n",
      "Epoch: 6484 \t|| Train Loss: 0.08642417985022297 \t|| Test Loss: 0.2020037451070631\n",
      "Epoch: 6485 \t|| Train Loss: 0.08638932749106426 \t|| Test Loss: 0.20196223510937283\n",
      "Epoch: 6486 \t|| Train Loss: 0.08635565985356825 \t|| Test Loss: 0.20185203511206584\n",
      "Epoch: 6487 \t|| Train Loss: 0.08632075985535191 \t|| Test Loss: 0.20174183511475885\n",
      "Epoch: 6488 \t|| Train Loss: 0.08628680649555455 \t|| Test Loss: 0.20170032511706854\n",
      "Epoch: 6489 \t|| Train Loss: 0.08625223985869719 \t|| Test Loss: 0.20159012511976154\n",
      "Epoch: 6490 \t|| Train Loss: 0.08621790549848322 \t|| Test Loss: 0.20154861512207134\n",
      "Epoch: 6491 \t|| Train Loss: 0.08618371986204246 \t|| Test Loss: 0.2014384151247643\n",
      "Epoch: 6492 \t|| Train Loss: 0.08614900450141191 \t|| Test Loss: 0.20139690512707403\n",
      "Epoch: 6493 \t|| Train Loss: 0.08611519986538774 \t|| Test Loss: 0.20128670512976704\n",
      "Epoch: 6494 \t|| Train Loss: 0.08608029986717139 \t|| Test Loss: 0.2011765051324601\n",
      "Epoch: 6495 \t|| Train Loss: 0.08604648350590219 \t|| Test Loss: 0.20113499513476985\n",
      "Epoch: 6496 \t|| Train Loss: 0.08601177987051668 \t|| Test Loss: 0.20102479513746285\n",
      "Epoch: 6497 \t|| Train Loss: 0.08597758250883089 \t|| Test Loss: 0.2009832851397726\n",
      "Epoch: 6498 \t|| Train Loss: 0.08594325987386195 \t|| Test Loss: 0.2008730851424656\n",
      "Epoch: 6499 \t|| Train Loss: 0.08590868151175957 \t|| Test Loss: 0.20083157514477534\n",
      "Epoch: 6500 \t|| Train Loss: 0.08587473987720723 \t|| Test Loss: 0.2007213751474683\n",
      "Epoch: 6501 \t|| Train Loss: 0.0858398398789909 \t|| Test Loss: 0.2006111751501613\n",
      "Epoch: 6502 \t|| Train Loss: 0.08580616051624985 \t|| Test Loss: 0.2005696651524711\n",
      "Epoch: 6503 \t|| Train Loss: 0.08577131988233616 \t|| Test Loss: 0.2004594651551641\n",
      "Epoch: 6504 \t|| Train Loss: 0.08573725951917852 \t|| Test Loss: 0.2004179551574738\n",
      "Epoch: 6505 \t|| Train Loss: 0.08570279988568144 \t|| Test Loss: 0.20030775516016686\n",
      "Epoch: 6506 \t|| Train Loss: 0.08566835852210722 \t|| Test Loss: 0.20026624516247654\n",
      "Epoch: 6507 \t|| Train Loss: 0.08563427988902672 \t|| Test Loss: 0.2001560451651696\n",
      "Epoch: 6508 \t|| Train Loss: 0.0855994575250359 \t|| Test Loss: 0.2001145351674793\n",
      "Epoch: 6509 \t|| Train Loss: 0.08556575989237199 \t|| Test Loss: 0.20000433517017235\n",
      "Epoch: 6510 \t|| Train Loss: 0.08553085989415565 \t|| Test Loss: 0.19989413517286533\n",
      "Epoch: 6511 \t|| Train Loss: 0.0854969365295262 \t|| Test Loss: 0.1998526251751751\n",
      "Epoch: 6512 \t|| Train Loss: 0.08546233989750093 \t|| Test Loss: 0.19974242517786808\n",
      "Epoch: 6513 \t|| Train Loss: 0.08542803553245487 \t|| Test Loss: 0.19970091518017782\n",
      "Epoch: 6514 \t|| Train Loss: 0.0853938199008462 \t|| Test Loss: 0.1995907151828708\n",
      "Epoch: 6515 \t|| Train Loss: 0.08535913453538355 \t|| Test Loss: 0.19954920518518054\n",
      "Epoch: 6516 \t|| Train Loss: 0.08532529990419148 \t|| Test Loss: 0.19943900518787355\n",
      "Epoch: 6517 \t|| Train Loss: 0.08529039990597514 \t|| Test Loss: 0.19932880519056656\n",
      "Epoch: 6518 \t|| Train Loss: 0.08525661353987385 \t|| Test Loss: 0.19928729519287633\n",
      "Epoch: 6519 \t|| Train Loss: 0.08522187990932042 \t|| Test Loss: 0.1991770951955693\n",
      "Epoch: 6520 \t|| Train Loss: 0.08518771254280252 \t|| Test Loss: 0.19913558519787908\n",
      "Epoch: 6521 \t|| Train Loss: 0.08515335991266569 \t|| Test Loss: 0.19902538520057214\n",
      "Epoch: 6522 \t|| Train Loss: 0.08511881154573123 \t|| Test Loss: 0.1989838752028818\n",
      "Epoch: 6523 \t|| Train Loss: 0.08508483991601097 \t|| Test Loss: 0.19887367520557486\n",
      "Epoch: 6524 \t|| Train Loss: 0.08504993991779465 \t|| Test Loss: 0.19876347520826781\n",
      "Epoch: 6525 \t|| Train Loss: 0.08501629055022149 \t|| Test Loss: 0.19872196521057756\n",
      "Epoch: 6526 \t|| Train Loss: 0.08498141992113992 \t|| Test Loss: 0.19861176521327056\n",
      "Epoch: 6527 \t|| Train Loss: 0.0849473895531502 \t|| Test Loss: 0.1985702552155803\n",
      "Epoch: 6528 \t|| Train Loss: 0.08491289992448518 \t|| Test Loss: 0.19846005521827334\n",
      "Epoch: 6529 \t|| Train Loss: 0.08487848855607887 \t|| Test Loss: 0.19841854522058305\n",
      "Epoch: 6530 \t|| Train Loss: 0.08484437992783048 \t|| Test Loss: 0.19830834522327606\n",
      "Epoch: 6531 \t|| Train Loss: 0.08480958755900754 \t|| Test Loss: 0.1982668352255858\n",
      "Epoch: 6532 \t|| Train Loss: 0.08477585993117574 \t|| Test Loss: 0.19815663522827884\n",
      "Epoch: 6533 \t|| Train Loss: 0.08474095993295941 \t|| Test Loss: 0.19804643523097185\n",
      "Epoch: 6534 \t|| Train Loss: 0.08470706656349783 \t|| Test Loss: 0.19800492523328156\n",
      "Epoch: 6535 \t|| Train Loss: 0.08467243993630469 \t|| Test Loss: 0.19789472523597462\n",
      "Epoch: 6536 \t|| Train Loss: 0.08463816556642652 \t|| Test Loss: 0.19785321523828436\n",
      "Epoch: 6537 \t|| Train Loss: 0.08460391993964993 \t|| Test Loss: 0.19774301524097732\n",
      "Epoch: 6538 \t|| Train Loss: 0.08456926456935521 \t|| Test Loss: 0.19770150524328706\n",
      "Epoch: 6539 \t|| Train Loss: 0.08453539994299522 \t|| Test Loss: 0.19759130524598006\n",
      "Epoch: 6540 \t|| Train Loss: 0.0845004999447789 \t|| Test Loss: 0.19748110524867307\n",
      "Epoch: 6541 \t|| Train Loss: 0.08446674357384547 \t|| Test Loss: 0.19743959525098284\n",
      "Epoch: 6542 \t|| Train Loss: 0.08443197994812417 \t|| Test Loss: 0.19732939525367582\n",
      "Epoch: 6543 \t|| Train Loss: 0.08439784257677416 \t|| Test Loss: 0.19728788525598556\n",
      "Epoch: 6544 \t|| Train Loss: 0.08436345995146945 \t|| Test Loss: 0.19717768525867857\n",
      "Epoch: 6545 \t|| Train Loss: 0.08432894157970286 \t|| Test Loss: 0.19713617526098834\n",
      "Epoch: 6546 \t|| Train Loss: 0.08429493995481473 \t|| Test Loss: 0.19702597526368135\n",
      "Epoch: 6547 \t|| Train Loss: 0.08426004058263152 \t|| Test Loss: 0.1969844652659911\n",
      "Epoch: 6548 \t|| Train Loss: 0.08422641995816 \t|| Test Loss: 0.1968742652686841\n",
      "Epoch: 6549 \t|| Train Loss: 0.08419151995994364 \t|| Test Loss: 0.19676406527137708\n",
      "Epoch: 6550 \t|| Train Loss: 0.08415751958712184 \t|| Test Loss: 0.19672255527368682\n",
      "Epoch: 6551 \t|| Train Loss: 0.08412299996328892 \t|| Test Loss: 0.19661235527637982\n",
      "Epoch: 6552 \t|| Train Loss: 0.08408861859005048 \t|| Test Loss: 0.19657084527868957\n",
      "Epoch: 6553 \t|| Train Loss: 0.0840544799666342 \t|| Test Loss: 0.19646064528138263\n",
      "Epoch: 6554 \t|| Train Loss: 0.08401971759297919 \t|| Test Loss: 0.19641913528369237\n",
      "Epoch: 6555 \t|| Train Loss: 0.08398595996997947 \t|| Test Loss: 0.19630893528638532\n",
      "Epoch: 6556 \t|| Train Loss: 0.08395105997176316 \t|| Test Loss: 0.19619873528907833\n",
      "Epoch: 6557 \t|| Train Loss: 0.08391719659746948 \t|| Test Loss: 0.19615722529138807\n",
      "Epoch: 6558 \t|| Train Loss: 0.08388253997510844 \t|| Test Loss: 0.19604702529408108\n",
      "Epoch: 6559 \t|| Train Loss: 0.08384829560039816 \t|| Test Loss: 0.19600551529639082\n",
      "Epoch: 6560 \t|| Train Loss: 0.0838140199784537 \t|| Test Loss: 0.19589531529908383\n",
      "Epoch: 6561 \t|| Train Loss: 0.08377939460332684 \t|| Test Loss: 0.19585380530139357\n",
      "Epoch: 6562 \t|| Train Loss: 0.08374549998179898 \t|| Test Loss: 0.1957436053040866\n",
      "Epoch: 6563 \t|| Train Loss: 0.08371059998358263 \t|| Test Loss: 0.1956334053067796\n",
      "Epoch: 6564 \t|| Train Loss: 0.08367687360781714 \t|| Test Loss: 0.19559189530908933\n",
      "Epoch: 6565 \t|| Train Loss: 0.08364207998692792 \t|| Test Loss: 0.19548169531178233\n",
      "Epoch: 6566 \t|| Train Loss: 0.08360797261074582 \t|| Test Loss: 0.19544018531409207\n",
      "Epoch: 6567 \t|| Train Loss: 0.08357355999027319 \t|| Test Loss: 0.19532998531678508\n",
      "Epoch: 6568 \t|| Train Loss: 0.08353907161367449 \t|| Test Loss: 0.19528847531909482\n",
      "Epoch: 6569 \t|| Train Loss: 0.08350503999361845 \t|| Test Loss: 0.19517827532178783\n",
      "Epoch: 6570 \t|| Train Loss: 0.08347017061660318 \t|| Test Loss: 0.19513676532409757\n",
      "Epoch: 6571 \t|| Train Loss: 0.08343651999696373 \t|| Test Loss: 0.19502656532679058\n",
      "Epoch: 6572 \t|| Train Loss: 0.0834016199987474 \t|| Test Loss: 0.19491636532948364\n",
      "Epoch: 6573 \t|| Train Loss: 0.08336764962109347 \t|| Test Loss: 0.19487485533179333\n",
      "Epoch: 6574 \t|| Train Loss: 0.08333310000209268 \t|| Test Loss: 0.19476465533448634\n",
      "Epoch: 6575 \t|| Train Loss: 0.08329874862402216 \t|| Test Loss: 0.1947231453367961\n",
      "Epoch: 6576 \t|| Train Loss: 0.08326458000543795 \t|| Test Loss: 0.19461294533948909\n",
      "Epoch: 6577 \t|| Train Loss: 0.08322984762695086 \t|| Test Loss: 0.19457143534179885\n",
      "Epoch: 6578 \t|| Train Loss: 0.08319606000878323 \t|| Test Loss: 0.19446123534449183\n",
      "Epoch: 6579 \t|| Train Loss: 0.08316116001056689 \t|| Test Loss: 0.19435103534718484\n",
      "Epoch: 6580 \t|| Train Loss: 0.08312732663144112 \t|| Test Loss: 0.19430952534949458\n",
      "Epoch: 6581 \t|| Train Loss: 0.08309264001391217 \t|| Test Loss: 0.1941993253521876\n",
      "Epoch: 6582 \t|| Train Loss: 0.0830584256343698 \t|| Test Loss: 0.19415781535449733\n",
      "Epoch: 6583 \t|| Train Loss: 0.08302412001725744 \t|| Test Loss: 0.19404761535719034\n",
      "Epoch: 6584 \t|| Train Loss: 0.08298952463729849 \t|| Test Loss: 0.19400610535950008\n",
      "Epoch: 6585 \t|| Train Loss: 0.08295560002060272 \t|| Test Loss: 0.1938959053621931\n",
      "Epoch: 6586 \t|| Train Loss: 0.08292070002238638 \t|| Test Loss: 0.19378570536488607\n",
      "Epoch: 6587 \t|| Train Loss: 0.08288700364178879 \t|| Test Loss: 0.19374419536719584\n",
      "Epoch: 6588 \t|| Train Loss: 0.08285218002573166 \t|| Test Loss: 0.19363399536988884\n",
      "Epoch: 6589 \t|| Train Loss: 0.08281810264471748 \t|| Test Loss: 0.19359248537219864\n",
      "Epoch: 6590 \t|| Train Loss: 0.08278366002907693 \t|| Test Loss: 0.1934822853748916\n",
      "Epoch: 6591 \t|| Train Loss: 0.08274920164764614 \t|| Test Loss: 0.19344077537720136\n",
      "Epoch: 6592 \t|| Train Loss: 0.08271514003242222 \t|| Test Loss: 0.19333057537989434\n",
      "Epoch: 6593 \t|| Train Loss: 0.08268030065057483 \t|| Test Loss: 0.19328906538220408\n",
      "Epoch: 6594 \t|| Train Loss: 0.08264662003576748 \t|| Test Loss: 0.1931788653848971\n",
      "Epoch: 6595 \t|| Train Loss: 0.08261172003755116 \t|| Test Loss: 0.1930686653875901\n",
      "Epoch: 6596 \t|| Train Loss: 0.0825777796550651 \t|| Test Loss: 0.19302715538989987\n",
      "Epoch: 6597 \t|| Train Loss: 0.08254320004089642 \t|| Test Loss: 0.19291695539259285\n",
      "Epoch: 6598 \t|| Train Loss: 0.0825088786579938 \t|| Test Loss: 0.1928754453949026\n",
      "Epoch: 6599 \t|| Train Loss: 0.0824746800442417 \t|| Test Loss: 0.1927652453975956\n",
      "Epoch: 6600 \t|| Train Loss: 0.08243997766092247 \t|| Test Loss: 0.19272373539990534\n",
      "Epoch: 6601 \t|| Train Loss: 0.08240616004758698 \t|| Test Loss: 0.19261353540259835\n",
      "Epoch: 6602 \t|| Train Loss: 0.08237126004937065 \t|| Test Loss: 0.19250333540529135\n",
      "Epoch: 6603 \t|| Train Loss: 0.08233745666541278 \t|| Test Loss: 0.1924618254076011\n",
      "Epoch: 6604 \t|| Train Loss: 0.08230274005271591 \t|| Test Loss: 0.19235162541029413\n",
      "Epoch: 6605 \t|| Train Loss: 0.08226855566834144 \t|| Test Loss: 0.19231011541260384\n",
      "Epoch: 6606 \t|| Train Loss: 0.08223422005606121 \t|| Test Loss: 0.1921999154152969\n",
      "Epoch: 6607 \t|| Train Loss: 0.08219965467127015 \t|| Test Loss: 0.1921584054176066\n",
      "Epoch: 6608 \t|| Train Loss: 0.08216570005940646 \t|| Test Loss: 0.1920482054202996\n",
      "Epoch: 6609 \t|| Train Loss: 0.08213080006119013 \t|| Test Loss: 0.1919380054229926\n",
      "Epoch: 6610 \t|| Train Loss: 0.08209713367576041 \t|| Test Loss: 0.19189649542530235\n",
      "Epoch: 6611 \t|| Train Loss: 0.08206228006453542 \t|| Test Loss: 0.19178629542799536\n",
      "Epoch: 6612 \t|| Train Loss: 0.08202823267868911 \t|| Test Loss: 0.1917447854303051\n",
      "Epoch: 6613 \t|| Train Loss: 0.08199376006788069 \t|| Test Loss: 0.1916345854329981\n",
      "Epoch: 6614 \t|| Train Loss: 0.08195933168161779 \t|| Test Loss: 0.19159307543530787\n",
      "Epoch: 6615 \t|| Train Loss: 0.08192524007122595 \t|| Test Loss: 0.19148287543800088\n",
      "Epoch: 6616 \t|| Train Loss: 0.08189043068454646 \t|| Test Loss: 0.1914413654403106\n",
      "Epoch: 6617 \t|| Train Loss: 0.08185672007457122 \t|| Test Loss: 0.1913311654430036\n",
      "Epoch: 6618 \t|| Train Loss: 0.0818218200763549 \t|| Test Loss: 0.1912209654456966\n",
      "Epoch: 6619 \t|| Train Loss: 0.08178790968903676 \t|| Test Loss: 0.19117945544800635\n",
      "Epoch: 6620 \t|| Train Loss: 0.08175330007970018 \t|| Test Loss: 0.19106925545069936\n",
      "Epoch: 6621 \t|| Train Loss: 0.08171900869196544 \t|| Test Loss: 0.1910277454530091\n",
      "Epoch: 6622 \t|| Train Loss: 0.08168478008304546 \t|| Test Loss: 0.1909175454557021\n",
      "Epoch: 6623 \t|| Train Loss: 0.08165010769489413 \t|| Test Loss: 0.19087603545801185\n",
      "Epoch: 6624 \t|| Train Loss: 0.08161626008639071 \t|| Test Loss: 0.19076583546070486\n",
      "Epoch: 6625 \t|| Train Loss: 0.08158136008817439 \t|| Test Loss: 0.19065563546339787\n",
      "Epoch: 6626 \t|| Train Loss: 0.0815475866993844 \t|| Test Loss: 0.1906141254657076\n",
      "Epoch: 6627 \t|| Train Loss: 0.08151284009151967 \t|| Test Loss: 0.19050392546840061\n",
      "Epoch: 6628 \t|| Train Loss: 0.08147868570231309 \t|| Test Loss: 0.19046241547071036\n",
      "Epoch: 6629 \t|| Train Loss: 0.08144432009486494 \t|| Test Loss: 0.19035221547340336\n",
      "Epoch: 6630 \t|| Train Loss: 0.08140978470524178 \t|| Test Loss: 0.19031070547571313\n",
      "Epoch: 6631 \t|| Train Loss: 0.0813758000982102 \t|| Test Loss: 0.19020050547840617\n",
      "Epoch: 6632 \t|| Train Loss: 0.08134090009999388 \t|| Test Loss: 0.19009030548109912\n",
      "Epoch: 6633 \t|| Train Loss: 0.08130726370973207 \t|| Test Loss: 0.19004879548340886\n",
      "Epoch: 6634 \t|| Train Loss: 0.08127238010333915 \t|| Test Loss: 0.18993859548610187\n",
      "Epoch: 6635 \t|| Train Loss: 0.08123836271266074 \t|| Test Loss: 0.1898970854884116\n",
      "Epoch: 6636 \t|| Train Loss: 0.08120386010668443 \t|| Test Loss: 0.18978688549110462\n",
      "Epoch: 6637 \t|| Train Loss: 0.08116946171558943 \t|| Test Loss: 0.18974537549341436\n",
      "Epoch: 6638 \t|| Train Loss: 0.0811353401100297 \t|| Test Loss: 0.18963517549610737\n",
      "Epoch: 6639 \t|| Train Loss: 0.08110056071851812 \t|| Test Loss: 0.1895936654984171\n",
      "Epoch: 6640 \t|| Train Loss: 0.08106682011337497 \t|| Test Loss: 0.18948346550111014\n",
      "Epoch: 6641 \t|| Train Loss: 0.08103192011515864 \t|| Test Loss: 0.18937326550380312\n",
      "Epoch: 6642 \t|| Train Loss: 0.08099803972300841 \t|| Test Loss: 0.18933175550611286\n",
      "Epoch: 6643 \t|| Train Loss: 0.08096340011850392 \t|| Test Loss: 0.18922155550880587\n",
      "Epoch: 6644 \t|| Train Loss: 0.0809291387259371 \t|| Test Loss: 0.1891800455111156\n",
      "Epoch: 6645 \t|| Train Loss: 0.08089488012184919 \t|| Test Loss: 0.1890698455138086\n",
      "Epoch: 6646 \t|| Train Loss: 0.08086023772886577 \t|| Test Loss: 0.1890283355161184\n",
      "Epoch: 6647 \t|| Train Loss: 0.08082636012519447 \t|| Test Loss: 0.18891813551881137\n",
      "Epoch: 6648 \t|| Train Loss: 0.08079146012697813 \t|| Test Loss: 0.18880793552150435\n",
      "Epoch: 6649 \t|| Train Loss: 0.08075771673335605 \t|| Test Loss: 0.1887664255238141\n",
      "Epoch: 6650 \t|| Train Loss: 0.08072294013032341 \t|| Test Loss: 0.18865622552650713\n",
      "Epoch: 6651 \t|| Train Loss: 0.08068881573628474 \t|| Test Loss: 0.18861471552881687\n",
      "Epoch: 6652 \t|| Train Loss: 0.08065442013366868 \t|| Test Loss: 0.18850451553150988\n",
      "Epoch: 6653 \t|| Train Loss: 0.08061991473921341 \t|| Test Loss: 0.18846300553381962\n",
      "Epoch: 6654 \t|| Train Loss: 0.08058590013701396 \t|| Test Loss: 0.18835280553651262\n",
      "Epoch: 6655 \t|| Train Loss: 0.0805510137421421 \t|| Test Loss: 0.18831129553882237\n",
      "Epoch: 6656 \t|| Train Loss: 0.08051738014035924 \t|| Test Loss: 0.18820109554151537\n",
      "Epoch: 6657 \t|| Train Loss: 0.0804824801421429 \t|| Test Loss: 0.18809089554420838\n",
      "Epoch: 6658 \t|| Train Loss: 0.08044849274663239 \t|| Test Loss: 0.18804938554651812\n",
      "Epoch: 6659 \t|| Train Loss: 0.08041396014548817 \t|| Test Loss: 0.18793918554921113\n",
      "Epoch: 6660 \t|| Train Loss: 0.08037959174956108 \t|| Test Loss: 0.18789767555152087\n",
      "Epoch: 6661 \t|| Train Loss: 0.08034544014883345 \t|| Test Loss: 0.18778747555421388\n",
      "Epoch: 6662 \t|| Train Loss: 0.08031069075248976 \t|| Test Loss: 0.18774596555652365\n",
      "Epoch: 6663 \t|| Train Loss: 0.08027692015217873 \t|| Test Loss: 0.18763576555921663\n",
      "Epoch: 6664 \t|| Train Loss: 0.08024202015396238 \t|| Test Loss: 0.18752556556190964\n",
      "Epoch: 6665 \t|| Train Loss: 0.08020816975698004 \t|| Test Loss: 0.18748405556421938\n",
      "Epoch: 6666 \t|| Train Loss: 0.08017350015730769 \t|| Test Loss: 0.18737385556691244\n",
      "Epoch: 6667 \t|| Train Loss: 0.08013926875990873 \t|| Test Loss: 0.18733234556922213\n",
      "Epoch: 6668 \t|| Train Loss: 0.08010498016065296 \t|| Test Loss: 0.18722214557191513\n",
      "Epoch: 6669 \t|| Train Loss: 0.08007036776283742 \t|| Test Loss: 0.18718063557422487\n",
      "Epoch: 6670 \t|| Train Loss: 0.08003646016399821 \t|| Test Loss: 0.18707043557691788\n",
      "Epoch: 6671 \t|| Train Loss: 0.08000156016578189 \t|| Test Loss: 0.1869602355796109\n",
      "Epoch: 6672 \t|| Train Loss: 0.07996784676732768 \t|| Test Loss: 0.18691872558192063\n",
      "Epoch: 6673 \t|| Train Loss: 0.07993304016912714 \t|| Test Loss: 0.18680852558461364\n",
      "Epoch: 6674 \t|| Train Loss: 0.0798989457702564 \t|| Test Loss: 0.18676701558692335\n",
      "Epoch: 6675 \t|| Train Loss: 0.07986452017247243 \t|| Test Loss: 0.1866568155896164\n",
      "Epoch: 6676 \t|| Train Loss: 0.07983004477318507 \t|| Test Loss: 0.1866153055919261\n",
      "Epoch: 6677 \t|| Train Loss: 0.07979600017581771 \t|| Test Loss: 0.18650510559461914\n",
      "Epoch: 6678 \t|| Train Loss: 0.07976114377611374 \t|| Test Loss: 0.18646359559692888\n",
      "Epoch: 6679 \t|| Train Loss: 0.07972748017916297 \t|| Test Loss: 0.18635339559962188\n",
      "Epoch: 6680 \t|| Train Loss: 0.07969258018094665 \t|| Test Loss: 0.1862431956023149\n",
      "Epoch: 6681 \t|| Train Loss: 0.07965862278060404 \t|| Test Loss: 0.1862016856046246\n",
      "Epoch: 6682 \t|| Train Loss: 0.07962406018429193 \t|| Test Loss: 0.1860914856073176\n",
      "Epoch: 6683 \t|| Train Loss: 0.07958972178353271 \t|| Test Loss: 0.18604997560962738\n",
      "Epoch: 6684 \t|| Train Loss: 0.07955554018763719 \t|| Test Loss: 0.1859397756123204\n",
      "Epoch: 6685 \t|| Train Loss: 0.0795208207864614 \t|| Test Loss: 0.18589826561463013\n",
      "Epoch: 6686 \t|| Train Loss: 0.07948702019098249 \t|| Test Loss: 0.18578806561732314\n",
      "Epoch: 6687 \t|| Train Loss: 0.07945212019276615 \t|| Test Loss: 0.18567786562001615\n",
      "Epoch: 6688 \t|| Train Loss: 0.07941829979095169 \t|| Test Loss: 0.1856363556223259\n",
      "Epoch: 6689 \t|| Train Loss: 0.07938360019611142 \t|| Test Loss: 0.1855261556250189\n",
      "Epoch: 6690 \t|| Train Loss: 0.07934939879388037 \t|| Test Loss: 0.18548464562732864\n",
      "Epoch: 6691 \t|| Train Loss: 0.07931508019945668 \t|| Test Loss: 0.18537444563002164\n",
      "Epoch: 6692 \t|| Train Loss: 0.07928049779680905 \t|| Test Loss: 0.18533293563233139\n",
      "Epoch: 6693 \t|| Train Loss: 0.07924656020280198 \t|| Test Loss: 0.1852227356350244\n",
      "Epoch: 6694 \t|| Train Loss: 0.07921166020458563 \t|| Test Loss: 0.18511253563771737\n",
      "Epoch: 6695 \t|| Train Loss: 0.07917797680129934 \t|| Test Loss: 0.18507102564002714\n",
      "Epoch: 6696 \t|| Train Loss: 0.0791431402079309 \t|| Test Loss: 0.1849608256427201\n",
      "Epoch: 6697 \t|| Train Loss: 0.07910907580422803 \t|| Test Loss: 0.1849193156450299\n",
      "Epoch: 6698 \t|| Train Loss: 0.07907462021127618 \t|| Test Loss: 0.1848091156477229\n",
      "Epoch: 6699 \t|| Train Loss: 0.07904017480715672 \t|| Test Loss: 0.18476760565003264\n",
      "Epoch: 6700 \t|| Train Loss: 0.07900610021462146 \t|| Test Loss: 0.18465740565272565\n",
      "Epoch: 6701 \t|| Train Loss: 0.0789712738100854 \t|| Test Loss: 0.1846158956550354\n",
      "Epoch: 6702 \t|| Train Loss: 0.07893758021796672 \t|| Test Loss: 0.1845056956577284\n",
      "Epoch: 6703 \t|| Train Loss: 0.0789026802197504 \t|| Test Loss: 0.1843954956604214\n",
      "Epoch: 6704 \t|| Train Loss: 0.07886875281457568 \t|| Test Loss: 0.18435398566273115\n",
      "Epoch: 6705 \t|| Train Loss: 0.07883416022309567 \t|| Test Loss: 0.18424378566542418\n",
      "Epoch: 6706 \t|| Train Loss: 0.07879985181750437 \t|| Test Loss: 0.1842022756677339\n",
      "Epoch: 6707 \t|| Train Loss: 0.07876564022644093 \t|| Test Loss: 0.1840920756704269\n",
      "Epoch: 6708 \t|| Train Loss: 0.07873095082043305 \t|| Test Loss: 0.18405056567273662\n",
      "Epoch: 6709 \t|| Train Loss: 0.07869712022978621 \t|| Test Loss: 0.18394036567542962\n",
      "Epoch: 6710 \t|| Train Loss: 0.0786622202315699 \t|| Test Loss: 0.18383016567812266\n",
      "Epoch: 6711 \t|| Train Loss: 0.07862842982492334 \t|| Test Loss: 0.18378865568043237\n",
      "Epoch: 6712 \t|| Train Loss: 0.07859370023491516 \t|| Test Loss: 0.18367845568312538\n",
      "Epoch: 6713 \t|| Train Loss: 0.07855952882785201 \t|| Test Loss: 0.18363694568543512\n",
      "Epoch: 6714 \t|| Train Loss: 0.07852518023826043 \t|| Test Loss: 0.18352674568812816\n",
      "Epoch: 6715 \t|| Train Loss: 0.0784906278307807 \t|| Test Loss: 0.1834852356904379\n",
      "Epoch: 6716 \t|| Train Loss: 0.0784566602416057 \t|| Test Loss: 0.1833750356931309\n",
      "Epoch: 6717 \t|| Train Loss: 0.07842176024338939 \t|| Test Loss: 0.1832648356958239\n",
      "Epoch: 6718 \t|| Train Loss: 0.07838810683527099 \t|| Test Loss: 0.18322332569813365\n",
      "Epoch: 6719 \t|| Train Loss: 0.07835324024673465 \t|| Test Loss: 0.18311312570082666\n",
      "Epoch: 6720 \t|| Train Loss: 0.07831920583819967 \t|| Test Loss: 0.1830716157031364\n",
      "Epoch: 6721 \t|| Train Loss: 0.07828472025007993 \t|| Test Loss: 0.18296141570582938\n",
      "Epoch: 6722 \t|| Train Loss: 0.07825030484112835 \t|| Test Loss: 0.18291990570813915\n",
      "Epoch: 6723 \t|| Train Loss: 0.0782162002534252 \t|| Test Loss: 0.18280970571083216\n",
      "Epoch: 6724 \t|| Train Loss: 0.07818140384405703 \t|| Test Loss: 0.18276819571314187\n",
      "Epoch: 6725 \t|| Train Loss: 0.07814768025677048 \t|| Test Loss: 0.1826579957158349\n",
      "Epoch: 6726 \t|| Train Loss: 0.07811278025855414 \t|| Test Loss: 0.18254779571852792\n",
      "Epoch: 6727 \t|| Train Loss: 0.07807888284854733 \t|| Test Loss: 0.18250628572083766\n",
      "Epoch: 6728 \t|| Train Loss: 0.0780442602618994 \t|| Test Loss: 0.18239608572353067\n",
      "Epoch: 6729 \t|| Train Loss: 0.078009981851476 \t|| Test Loss: 0.1823545757258404\n",
      "Epoch: 6730 \t|| Train Loss: 0.07797574026524468 \t|| Test Loss: 0.18224437572853341\n",
      "Epoch: 6731 \t|| Train Loss: 0.0779410808544047 \t|| Test Loss: 0.18220286573084316\n",
      "Epoch: 6732 \t|| Train Loss: 0.07790722026858996 \t|| Test Loss: 0.18209266573353616\n",
      "Epoch: 6733 \t|| Train Loss: 0.07787232027037364 \t|| Test Loss: 0.18198246573622917\n",
      "Epoch: 6734 \t|| Train Loss: 0.07783855985889498 \t|| Test Loss: 0.1819409557385389\n",
      "Epoch: 6735 \t|| Train Loss: 0.0778038002737189 \t|| Test Loss: 0.1818307557412319\n",
      "Epoch: 6736 \t|| Train Loss: 0.07776965886182366 \t|| Test Loss: 0.18178924574354163\n",
      "Epoch: 6737 \t|| Train Loss: 0.07773528027706418 \t|| Test Loss: 0.18167904574623464\n",
      "Epoch: 6738 \t|| Train Loss: 0.07770075786475235 \t|| Test Loss: 0.1816375357485444\n",
      "Epoch: 6739 \t|| Train Loss: 0.07766676028040945 \t|| Test Loss: 0.18152733575123742\n",
      "Epoch: 6740 \t|| Train Loss: 0.07763186028219313 \t|| Test Loss: 0.18141713575393043\n",
      "Epoch: 6741 \t|| Train Loss: 0.07759823686924264 \t|| Test Loss: 0.1813756257562402\n",
      "Epoch: 6742 \t|| Train Loss: 0.0775633402855384 \t|| Test Loss: 0.18126542575893315\n",
      "Epoch: 6743 \t|| Train Loss: 0.07752933587217134 \t|| Test Loss: 0.18122391576124292\n",
      "Epoch: 6744 \t|| Train Loss: 0.07749482028888367 \t|| Test Loss: 0.1811137157639359\n",
      "Epoch: 6745 \t|| Train Loss: 0.0774604348751 \t|| Test Loss: 0.18107220576624566\n",
      "Epoch: 6746 \t|| Train Loss: 0.07742630029222894 \t|| Test Loss: 0.18096200576893864\n",
      "Epoch: 6747 \t|| Train Loss: 0.07739153387802868 \t|| Test Loss: 0.18092049577124839\n",
      "Epoch: 6748 \t|| Train Loss: 0.07735778029557423 \t|| Test Loss: 0.1808102957739414\n",
      "Epoch: 6749 \t|| Train Loss: 0.07732288029735788 \t|| Test Loss: 0.18070009577663443\n",
      "Epoch: 6750 \t|| Train Loss: 0.07728901288251896 \t|| Test Loss: 0.18065858577894414\n",
      "Epoch: 6751 \t|| Train Loss: 0.07725436030070316 \t|| Test Loss: 0.18054838578163718\n",
      "Epoch: 6752 \t|| Train Loss: 0.07722011188544767 \t|| Test Loss: 0.18050687578394692\n",
      "Epoch: 6753 \t|| Train Loss: 0.07718584030404843 \t|| Test Loss: 0.18039667578663993\n",
      "Epoch: 6754 \t|| Train Loss: 0.07715121088837634 \t|| Test Loss: 0.18035516578894964\n",
      "Epoch: 6755 \t|| Train Loss: 0.07711732030739371 \t|| Test Loss: 0.18024496579164268\n",
      "Epoch: 6756 \t|| Train Loss: 0.07708242030917738 \t|| Test Loss: 0.18013476579433568\n",
      "Epoch: 6757 \t|| Train Loss: 0.07704868989286663 \t|| Test Loss: 0.18009325579664542\n",
      "Epoch: 6758 \t|| Train Loss: 0.07701390031252266 \t|| Test Loss: 0.1799830557993384\n",
      "Epoch: 6759 \t|| Train Loss: 0.0769797888957953 \t|| Test Loss: 0.17994154580164815\n",
      "Epoch: 6760 \t|| Train Loss: 0.07694538031586792 \t|| Test Loss: 0.17983134580434115\n",
      "Epoch: 6761 \t|| Train Loss: 0.076910887898724 \t|| Test Loss: 0.17978983580665092\n",
      "Epoch: 6762 \t|| Train Loss: 0.07687686031921319 \t|| Test Loss: 0.1796796358093439\n",
      "Epoch: 6763 \t|| Train Loss: 0.07684198690165267 \t|| Test Loss: 0.17963812581165364\n",
      "Epoch: 6764 \t|| Train Loss: 0.07680834032255845 \t|| Test Loss: 0.17952792581434668\n",
      "Epoch: 6765 \t|| Train Loss: 0.07677344032434213 \t|| Test Loss: 0.17941772581703969\n",
      "Epoch: 6766 \t|| Train Loss: 0.07673946590614296 \t|| Test Loss: 0.17937621581934943\n",
      "Epoch: 6767 \t|| Train Loss: 0.07670492032768741 \t|| Test Loss: 0.17926601582204243\n",
      "Epoch: 6768 \t|| Train Loss: 0.07667056490907165 \t|| Test Loss: 0.17922450582435215\n",
      "Epoch: 6769 \t|| Train Loss: 0.07663640033103268 \t|| Test Loss: 0.17911430582704516\n",
      "Epoch: 6770 \t|| Train Loss: 0.07660166391200032 \t|| Test Loss: 0.1790727958293549\n",
      "Epoch: 6771 \t|| Train Loss: 0.07656788033437796 \t|| Test Loss: 0.1789625958320479\n",
      "Epoch: 6772 \t|| Train Loss: 0.07653298033616163 \t|| Test Loss: 0.1788523958347409\n",
      "Epoch: 6773 \t|| Train Loss: 0.07649914291649063 \t|| Test Loss: 0.17881088583705065\n",
      "Epoch: 6774 \t|| Train Loss: 0.0764644603395069 \t|| Test Loss: 0.17870068583974366\n",
      "Epoch: 6775 \t|| Train Loss: 0.0764302419194193 \t|| Test Loss: 0.1786591758420534\n",
      "Epoch: 6776 \t|| Train Loss: 0.07639594034285219 \t|| Test Loss: 0.17854897584474644\n",
      "Epoch: 6777 \t|| Train Loss: 0.07636134092234798 \t|| Test Loss: 0.17850746584705618\n",
      "Epoch: 6778 \t|| Train Loss: 0.07632742034619747 \t|| Test Loss: 0.17839726584974916\n",
      "Epoch: 6779 \t|| Train Loss: 0.07629252034798113 \t|| Test Loss: 0.17828706585244217\n",
      "Epoch: 6780 \t|| Train Loss: 0.07625881992683828 \t|| Test Loss: 0.1782455558547519\n",
      "Epoch: 6781 \t|| Train Loss: 0.0762240003513264 \t|| Test Loss: 0.17813535585744494\n",
      "Epoch: 6782 \t|| Train Loss: 0.07618991892976694 \t|| Test Loss: 0.17809384585975468\n",
      "Epoch: 6783 \t|| Train Loss: 0.07615548035467166 \t|| Test Loss: 0.1779836458624477\n",
      "Epoch: 6784 \t|| Train Loss: 0.07612101793269563 \t|| Test Loss: 0.1779421358647574\n",
      "Epoch: 6785 \t|| Train Loss: 0.07608696035801694 \t|| Test Loss: 0.1778319358674504\n",
      "Epoch: 6786 \t|| Train Loss: 0.07605211693562432 \t|| Test Loss: 0.1777904258697601\n",
      "Epoch: 6787 \t|| Train Loss: 0.07601844036136221 \t|| Test Loss: 0.17768022587245316\n",
      "Epoch: 6788 \t|| Train Loss: 0.07598354036314589 \t|| Test Loss: 0.17757002587514617\n",
      "Epoch: 6789 \t|| Train Loss: 0.0759495959401146 \t|| Test Loss: 0.1775285158774559\n",
      "Epoch: 6790 \t|| Train Loss: 0.07591502036649116 \t|| Test Loss: 0.17741831588014892\n",
      "Epoch: 6791 \t|| Train Loss: 0.07588069494304331 \t|| Test Loss: 0.17737680588245866\n",
      "Epoch: 6792 \t|| Train Loss: 0.07584650036983644 \t|| Test Loss: 0.1772666058851517\n",
      "Epoch: 6793 \t|| Train Loss: 0.07581179394597197 \t|| Test Loss: 0.17722509588746144\n",
      "Epoch: 6794 \t|| Train Loss: 0.0757779803731817 \t|| Test Loss: 0.17711489589015442\n",
      "Epoch: 6795 \t|| Train Loss: 0.07574308037496538 \t|| Test Loss: 0.17700469589284742\n",
      "Epoch: 6796 \t|| Train Loss: 0.07570927295046227 \t|| Test Loss: 0.17696318589515717\n",
      "Epoch: 6797 \t|| Train Loss: 0.07567456037831068 \t|| Test Loss: 0.17685298589785017\n",
      "Epoch: 6798 \t|| Train Loss: 0.07564037195339095 \t|| Test Loss: 0.17681147590015991\n",
      "Epoch: 6799 \t|| Train Loss: 0.07560604038165593 \t|| Test Loss: 0.17670127590285292\n",
      "Epoch: 6800 \t|| Train Loss: 0.07557147095631962 \t|| Test Loss: 0.1766597659051627\n",
      "Epoch: 6801 \t|| Train Loss: 0.0755375203850012 \t|| Test Loss: 0.17654956590785567\n",
      "Epoch: 6802 \t|| Train Loss: 0.07550262038678485 \t|| Test Loss: 0.1764393659105487\n",
      "Epoch: 6803 \t|| Train Loss: 0.07546894996080991 \t|| Test Loss: 0.17639785591285842\n",
      "Epoch: 6804 \t|| Train Loss: 0.07543410039013014 \t|| Test Loss: 0.17628765591555143\n",
      "Epoch: 6805 \t|| Train Loss: 0.0754000489637386 \t|| Test Loss: 0.17624614591786117\n",
      "Epoch: 6806 \t|| Train Loss: 0.07536558039347543 \t|| Test Loss: 0.17613594592055412\n",
      "Epoch: 6807 \t|| Train Loss: 0.07533114796666729 \t|| Test Loss: 0.17609443592286392\n",
      "Epoch: 6808 \t|| Train Loss: 0.0752970603968207 \t|| Test Loss: 0.17598423592555693\n",
      "Epoch: 6809 \t|| Train Loss: 0.07526224696959596 \t|| Test Loss: 0.1759427259278667\n",
      "Epoch: 6810 \t|| Train Loss: 0.07522854040016595 \t|| Test Loss: 0.1758325259305597\n",
      "Epoch: 6811 \t|| Train Loss: 0.07519364040194965 \t|| Test Loss: 0.17572232593325268\n",
      "Epoch: 6812 \t|| Train Loss: 0.07515972597408624 \t|| Test Loss: 0.17568081593556242\n",
      "Epoch: 6813 \t|| Train Loss: 0.07512512040529491 \t|| Test Loss: 0.17557061593825543\n",
      "Epoch: 6814 \t|| Train Loss: 0.07509082497701494 \t|| Test Loss: 0.1755291059405652\n",
      "Epoch: 6815 \t|| Train Loss: 0.07505660040864019 \t|| Test Loss: 0.17541890594325818\n",
      "Epoch: 6816 \t|| Train Loss: 0.07502192397994362 \t|| Test Loss: 0.17537739594556795\n",
      "Epoch: 6817 \t|| Train Loss: 0.07498808041198543 \t|| Test Loss: 0.17526719594826093\n",
      "Epoch: 6818 \t|| Train Loss: 0.07495318041376912 \t|| Test Loss: 0.17515699595095396\n",
      "Epoch: 6819 \t|| Train Loss: 0.07491940298443389 \t|| Test Loss: 0.17511548595326368\n",
      "Epoch: 6820 \t|| Train Loss: 0.07488466041711439 \t|| Test Loss: 0.17500528595595669\n",
      "Epoch: 6821 \t|| Train Loss: 0.0748505019873626 \t|| Test Loss: 0.17496377595826637\n",
      "Epoch: 6822 \t|| Train Loss: 0.07481614042045967 \t|| Test Loss: 0.17485357596095946\n",
      "Epoch: 6823 \t|| Train Loss: 0.07478160099029127 \t|| Test Loss: 0.17481206596326915\n",
      "Epoch: 6824 \t|| Train Loss: 0.07474762042380495 \t|| Test Loss: 0.1747018659659622\n",
      "Epoch: 6825 \t|| Train Loss: 0.07471272042558862 \t|| Test Loss: 0.1745916659686552\n",
      "Epoch: 6826 \t|| Train Loss: 0.07467907999478154 \t|| Test Loss: 0.17455015597096496\n",
      "Epoch: 6827 \t|| Train Loss: 0.07464420042893391 \t|| Test Loss: 0.17443995597365797\n",
      "Epoch: 6828 \t|| Train Loss: 0.07461017899771025 \t|| Test Loss: 0.17439844597596765\n",
      "Epoch: 6829 \t|| Train Loss: 0.07457568043227916 \t|| Test Loss: 0.1742882459786607\n",
      "Epoch: 6830 \t|| Train Loss: 0.07454127800063892 \t|| Test Loss: 0.17424673598097043\n",
      "Epoch: 6831 \t|| Train Loss: 0.07450716043562444 \t|| Test Loss: 0.17413653598366347\n",
      "Epoch: 6832 \t|| Train Loss: 0.0744723770035676 \t|| Test Loss: 0.17409502598597318\n",
      "Epoch: 6833 \t|| Train Loss: 0.07443864043896971 \t|| Test Loss: 0.17398482598866616\n",
      "Epoch: 6834 \t|| Train Loss: 0.07440374044075339 \t|| Test Loss: 0.1738746259913592\n",
      "Epoch: 6835 \t|| Train Loss: 0.0743698560080579 \t|| Test Loss: 0.17383311599366896\n",
      "Epoch: 6836 \t|| Train Loss: 0.07433522044409865 \t|| Test Loss: 0.17372291599636194\n",
      "Epoch: 6837 \t|| Train Loss: 0.07430095501098657 \t|| Test Loss: 0.17368140599867168\n",
      "Epoch: 6838 \t|| Train Loss: 0.07426670044744392 \t|| Test Loss: 0.1735712060013647\n",
      "Epoch: 6839 \t|| Train Loss: 0.07423205401391526 \t|| Test Loss: 0.17352969600367446\n",
      "Epoch: 6840 \t|| Train Loss: 0.0741981804507892 \t|| Test Loss: 0.17341949600636744\n",
      "Epoch: 6841 \t|| Train Loss: 0.07416328045257288 \t|| Test Loss: 0.17330929600906042\n",
      "Epoch: 6842 \t|| Train Loss: 0.07412953301840555 \t|| Test Loss: 0.1732677860113702\n",
      "Epoch: 6843 \t|| Train Loss: 0.07409476045591815 \t|| Test Loss: 0.1731575860140632\n",
      "Epoch: 6844 \t|| Train Loss: 0.07406063202133423 \t|| Test Loss: 0.17311607601637294\n",
      "Epoch: 6845 \t|| Train Loss: 0.07402624045926343 \t|| Test Loss: 0.17300587601906595\n",
      "Epoch: 6846 \t|| Train Loss: 0.0739917310242629 \t|| Test Loss: 0.1729643660213757\n",
      "Epoch: 6847 \t|| Train Loss: 0.07395772046260869 \t|| Test Loss: 0.1728541660240687\n",
      "Epoch: 6848 \t|| Train Loss: 0.0739228300271916 \t|| Test Loss: 0.17281265602637844\n",
      "Epoch: 6849 \t|| Train Loss: 0.07388920046595396 \t|| Test Loss: 0.17270245602907144\n",
      "Epoch: 6850 \t|| Train Loss: 0.07385430046773764 \t|| Test Loss: 0.17259225603176445\n",
      "Epoch: 6851 \t|| Train Loss: 0.07382030903168188 \t|| Test Loss: 0.17255074603407422\n",
      "Epoch: 6852 \t|| Train Loss: 0.07378578047108293 \t|| Test Loss: 0.1724405460367672\n",
      "Epoch: 6853 \t|| Train Loss: 0.07375140803461058 \t|| Test Loss: 0.17239903603907694\n",
      "Epoch: 6854 \t|| Train Loss: 0.0737172604744282 \t|| Test Loss: 0.17228883604176995\n",
      "Epoch: 6855 \t|| Train Loss: 0.07368250703753924 \t|| Test Loss: 0.17224732604407966\n",
      "Epoch: 6856 \t|| Train Loss: 0.07364874047777345 \t|| Test Loss: 0.1721371260467727\n",
      "Epoch: 6857 \t|| Train Loss: 0.07361384047955713 \t|| Test Loss: 0.17202692604946573\n",
      "Epoch: 6858 \t|| Train Loss: 0.07357998604202955 \t|| Test Loss: 0.17198541605177545\n",
      "Epoch: 6859 \t|| Train Loss: 0.0735453204829024 \t|| Test Loss: 0.17187521605446848\n",
      "Epoch: 6860 \t|| Train Loss: 0.07351108504495822 \t|| Test Loss: 0.17183370605677822\n",
      "Epoch: 6861 \t|| Train Loss: 0.07347680048624768 \t|| Test Loss: 0.17172350605947118\n",
      "Epoch: 6862 \t|| Train Loss: 0.07344218404788691 \t|| Test Loss: 0.17168199606178094\n",
      "Epoch: 6863 \t|| Train Loss: 0.07340828048959296 \t|| Test Loss: 0.17157179606447392\n",
      "Epoch: 6864 \t|| Train Loss: 0.07337338049137662 \t|| Test Loss: 0.17146159606716696\n",
      "Epoch: 6865 \t|| Train Loss: 0.0733396630523772 \t|| Test Loss: 0.17142008606947665\n",
      "Epoch: 6866 \t|| Train Loss: 0.07330486049472189 \t|| Test Loss: 0.1713098860721697\n",
      "Epoch: 6867 \t|| Train Loss: 0.07327076205530589 \t|| Test Loss: 0.17126837607447945\n",
      "Epoch: 6868 \t|| Train Loss: 0.07323634049806718 \t|| Test Loss: 0.1711581760771724\n",
      "Epoch: 6869 \t|| Train Loss: 0.07320186105823456 \t|| Test Loss: 0.1711166660794822\n",
      "Epoch: 6870 \t|| Train Loss: 0.07316782050141243 \t|| Test Loss: 0.1710064660821752\n",
      "Epoch: 6871 \t|| Train Loss: 0.07313296006116325 \t|| Test Loss: 0.17096495608448495\n",
      "Epoch: 6872 \t|| Train Loss: 0.0730993005047577 \t|| Test Loss: 0.17085475608717793\n",
      "Epoch: 6873 \t|| Train Loss: 0.07306440050654137 \t|| Test Loss: 0.17074455608987096\n",
      "Epoch: 6874 \t|| Train Loss: 0.07303043906565354 \t|| Test Loss: 0.1707030460921807\n",
      "Epoch: 6875 \t|| Train Loss: 0.07299588050988665 \t|| Test Loss: 0.1705928460948737\n",
      "Epoch: 6876 \t|| Train Loss: 0.07296153806858222 \t|| Test Loss: 0.1705513360971834\n",
      "Epoch: 6877 \t|| Train Loss: 0.07292736051323194 \t|| Test Loss: 0.17044113609987646\n",
      "Epoch: 6878 \t|| Train Loss: 0.0728926370715109 \t|| Test Loss: 0.1703996261021862\n",
      "Epoch: 6879 \t|| Train Loss: 0.0728588405165772 \t|| Test Loss: 0.1702894261048792\n",
      "Epoch: 6880 \t|| Train Loss: 0.07282394051836087 \t|| Test Loss: 0.17017922610757222\n",
      "Epoch: 6881 \t|| Train Loss: 0.0727901160760012 \t|| Test Loss: 0.17013771610988196\n",
      "Epoch: 6882 \t|| Train Loss: 0.07275542052170614 \t|| Test Loss: 0.17002751611257497\n",
      "Epoch: 6883 \t|| Train Loss: 0.07272121507892987 \t|| Test Loss: 0.1699860061148847\n",
      "Epoch: 6884 \t|| Train Loss: 0.07268690052505142 \t|| Test Loss: 0.16987580611757772\n",
      "Epoch: 6885 \t|| Train Loss: 0.07265231408185854 \t|| Test Loss: 0.16983429611988746\n",
      "Epoch: 6886 \t|| Train Loss: 0.07261838052839671 \t|| Test Loss: 0.1697240961225805\n",
      "Epoch: 6887 \t|| Train Loss: 0.07258348053018038 \t|| Test Loss: 0.16961389612527347\n",
      "Epoch: 6888 \t|| Train Loss: 0.07254979308634885 \t|| Test Loss: 0.1695723861275832\n",
      "Epoch: 6889 \t|| Train Loss: 0.07251496053352564 \t|| Test Loss: 0.16946218613027622\n",
      "Epoch: 6890 \t|| Train Loss: 0.07248089208927752 \t|| Test Loss: 0.16942067613258593\n",
      "Epoch: 6891 \t|| Train Loss: 0.07244644053687092 \t|| Test Loss: 0.16931047613527894\n",
      "Epoch: 6892 \t|| Train Loss: 0.07241199109220622 \t|| Test Loss: 0.16926896613758868\n",
      "Epoch: 6893 \t|| Train Loss: 0.07237792054021618 \t|| Test Loss: 0.16915876614028172\n",
      "Epoch: 6894 \t|| Train Loss: 0.07234309009513487 \t|| Test Loss: 0.1691172561425914\n",
      "Epoch: 6895 \t|| Train Loss: 0.07230940054356146 \t|| Test Loss: 0.16900705614528447\n",
      "Epoch: 6896 \t|| Train Loss: 0.07227450054534512 \t|| Test Loss: 0.16889685614797745\n",
      "Epoch: 6897 \t|| Train Loss: 0.07224056909962517 \t|| Test Loss: 0.16885534615028722\n",
      "Epoch: 6898 \t|| Train Loss: 0.0722059805486904 \t|| Test Loss: 0.16874514615298017\n",
      "Epoch: 6899 \t|| Train Loss: 0.07217166810255386 \t|| Test Loss: 0.16870363615528997\n",
      "Epoch: 6900 \t|| Train Loss: 0.07213746055203568 \t|| Test Loss: 0.16859343615798297\n",
      "Epoch: 6901 \t|| Train Loss: 0.07210276710548255 \t|| Test Loss: 0.16855192616029271\n",
      "Epoch: 6902 \t|| Train Loss: 0.07206894055538095 \t|| Test Loss: 0.16844172616298572\n",
      "Epoch: 6903 \t|| Train Loss: 0.0720340405571646 \t|| Test Loss: 0.16833152616567873\n",
      "Epoch: 6904 \t|| Train Loss: 0.07200024610997283 \t|| Test Loss: 0.16829001616798847\n",
      "Epoch: 6905 \t|| Train Loss: 0.0719655205605099 \t|| Test Loss: 0.16817981617068145\n",
      "Epoch: 6906 \t|| Train Loss: 0.07193134511290152 \t|| Test Loss: 0.1681383061729912\n",
      "Epoch: 6907 \t|| Train Loss: 0.07189700056385517 \t|| Test Loss: 0.1680281061756842\n",
      "Epoch: 6908 \t|| Train Loss: 0.0718624441158302 \t|| Test Loss: 0.16798659617799397\n",
      "Epoch: 6909 \t|| Train Loss: 0.07182848056720044 \t|| Test Loss: 0.16787639618068698\n",
      "Epoch: 6910 \t|| Train Loss: 0.0717935805689841 \t|| Test Loss: 0.16776619618337993\n",
      "Epoch: 6911 \t|| Train Loss: 0.07175992312032048 \t|| Test Loss: 0.16772468618568973\n",
      "Epoch: 6912 \t|| Train Loss: 0.07172506057232939 \t|| Test Loss: 0.16761448618838273\n",
      "Epoch: 6913 \t|| Train Loss: 0.07169102212324915 \t|| Test Loss: 0.16757297619069247\n",
      "Epoch: 6914 \t|| Train Loss: 0.07165654057567465 \t|| Test Loss: 0.16746277619338548\n",
      "Epoch: 6915 \t|| Train Loss: 0.07162212112617786 \t|| Test Loss: 0.16742126619569522\n",
      "Epoch: 6916 \t|| Train Loss: 0.07158802057901993 \t|| Test Loss: 0.16731106619838818\n",
      "Epoch: 6917 \t|| Train Loss: 0.07155322012910655 \t|| Test Loss: 0.16726955620069797\n",
      "Epoch: 6918 \t|| Train Loss: 0.0715195005823652 \t|| Test Loss: 0.16715935620339098\n",
      "Epoch: 6919 \t|| Train Loss: 0.07148460058414888 \t|| Test Loss: 0.16704915620608393\n",
      "Epoch: 6920 \t|| Train Loss: 0.07145069913359683 \t|| Test Loss: 0.16700764620839373\n",
      "Epoch: 6921 \t|| Train Loss: 0.07141608058749414 \t|| Test Loss: 0.16689744621108674\n",
      "Epoch: 6922 \t|| Train Loss: 0.07138179813652548 \t|| Test Loss: 0.16685593621339645\n",
      "Epoch: 6923 \t|| Train Loss: 0.07134756059083942 \t|| Test Loss: 0.16674573621608946\n",
      "Epoch: 6924 \t|| Train Loss: 0.07131289713945418 \t|| Test Loss: 0.16670422621839923\n",
      "Epoch: 6925 \t|| Train Loss: 0.0712790405941847 \t|| Test Loss: 0.16659402622109223\n",
      "Epoch: 6926 \t|| Train Loss: 0.07124414059596837 \t|| Test Loss: 0.16648382622378521\n",
      "Epoch: 6927 \t|| Train Loss: 0.07121037614394447 \t|| Test Loss: 0.16644231622609496\n",
      "Epoch: 6928 \t|| Train Loss: 0.07117562059931364 \t|| Test Loss: 0.16633211622878802\n",
      "Epoch: 6929 \t|| Train Loss: 0.07114147514687316 \t|| Test Loss: 0.16629060623109768\n",
      "Epoch: 6930 \t|| Train Loss: 0.07110710060265892 \t|| Test Loss: 0.16618040623379068\n",
      "Epoch: 6931 \t|| Train Loss: 0.07107257414980184 \t|| Test Loss: 0.1661388962361004\n",
      "Epoch: 6932 \t|| Train Loss: 0.07103858060600418 \t|| Test Loss: 0.16602869623879346\n",
      "Epoch: 6933 \t|| Train Loss: 0.07100368060778786 \t|| Test Loss: 0.1659184962414865\n",
      "Epoch: 6934 \t|| Train Loss: 0.07097005315429213 \t|| Test Loss: 0.16587698624379624\n",
      "Epoch: 6935 \t|| Train Loss: 0.07093516061113311 \t|| Test Loss: 0.1657667862464892\n",
      "Epoch: 6936 \t|| Train Loss: 0.0709011521572208 \t|| Test Loss: 0.16572527624879899\n",
      "Epoch: 6937 \t|| Train Loss: 0.07086664061447842 \t|| Test Loss: 0.165615076251492\n",
      "Epoch: 6938 \t|| Train Loss: 0.07083225116014949 \t|| Test Loss: 0.1655735662538017\n",
      "Epoch: 6939 \t|| Train Loss: 0.07079812061782369 \t|| Test Loss: 0.16546336625649471\n",
      "Epoch: 6940 \t|| Train Loss: 0.07076335016307818 \t|| Test Loss: 0.16542185625880448\n",
      "Epoch: 6941 \t|| Train Loss: 0.07072960062116895 \t|| Test Loss: 0.1653116562614975\n",
      "Epoch: 6942 \t|| Train Loss: 0.07069470062295262 \t|| Test Loss: 0.16520145626419044\n",
      "Epoch: 6943 \t|| Train Loss: 0.07066082916756847 \t|| Test Loss: 0.16515994626650018\n",
      "Epoch: 6944 \t|| Train Loss: 0.07062618062629791 \t|| Test Loss: 0.16504974626919325\n",
      "Epoch: 6945 \t|| Train Loss: 0.07059192817049716 \t|| Test Loss: 0.165008236271503\n",
      "Epoch: 6946 \t|| Train Loss: 0.07055766062964317 \t|| Test Loss: 0.164898036274196\n",
      "Epoch: 6947 \t|| Train Loss: 0.07052302717342583 \t|| Test Loss: 0.16485652627650574\n",
      "Epoch: 6948 \t|| Train Loss: 0.07048914063298843 \t|| Test Loss: 0.16474632627919875\n",
      "Epoch: 6949 \t|| Train Loss: 0.0704542406347721 \t|| Test Loss: 0.16463612628189175\n",
      "Epoch: 6950 \t|| Train Loss: 0.07042050617791612 \t|| Test Loss: 0.16459461628420147\n",
      "Epoch: 6951 \t|| Train Loss: 0.07038572063811739 \t|| Test Loss: 0.16448441628689445\n",
      "Epoch: 6952 \t|| Train Loss: 0.0703516051808448 \t|| Test Loss: 0.16444290628920422\n",
      "Epoch: 6953 \t|| Train Loss: 0.07031720064146267 \t|| Test Loss: 0.1643327062918972\n",
      "Epoch: 6954 \t|| Train Loss: 0.07028270418377348 \t|| Test Loss: 0.16429119629420696\n",
      "Epoch: 6955 \t|| Train Loss: 0.07024868064480794 \t|| Test Loss: 0.16418099629689994\n",
      "Epoch: 6956 \t|| Train Loss: 0.07021380318670217 \t|| Test Loss: 0.16413948629920969\n",
      "Epoch: 6957 \t|| Train Loss: 0.07018016064815322 \t|| Test Loss: 0.1640292863019027\n",
      "Epoch: 6958 \t|| Train Loss: 0.07014526064993688 \t|| Test Loss: 0.1639190863045957\n",
      "Epoch: 6959 \t|| Train Loss: 0.07011128219119248 \t|| Test Loss: 0.16387757630690547\n",
      "Epoch: 6960 \t|| Train Loss: 0.07007674065328215 \t|| Test Loss: 0.16376737630959845\n",
      "Epoch: 6961 \t|| Train Loss: 0.07004238119412114 \t|| Test Loss: 0.16372586631190827\n",
      "Epoch: 6962 \t|| Train Loss: 0.07000822065662744 \t|| Test Loss: 0.16361566631460123\n",
      "Epoch: 6963 \t|| Train Loss: 0.06997348019704983 \t|| Test Loss: 0.16357415631691094\n",
      "Epoch: 6964 \t|| Train Loss: 0.0699397006599727 \t|| Test Loss: 0.163463956319604\n",
      "Epoch: 6965 \t|| Train Loss: 0.06990480066175637 \t|| Test Loss: 0.16335375632229696\n",
      "Epoch: 6966 \t|| Train Loss: 0.0698709592015401 \t|| Test Loss: 0.1633122463246067\n",
      "Epoch: 6967 \t|| Train Loss: 0.06983628066510164 \t|| Test Loss: 0.16320204632729973\n",
      "Epoch: 6968 \t|| Train Loss: 0.0698020582044688 \t|| Test Loss: 0.16316053632960945\n",
      "Epoch: 6969 \t|| Train Loss: 0.06976776066844691 \t|| Test Loss: 0.1630503363323025\n",
      "Epoch: 6970 \t|| Train Loss: 0.06973315720739748 \t|| Test Loss: 0.16300882633461222\n",
      "Epoch: 6971 \t|| Train Loss: 0.06969924067179219 \t|| Test Loss: 0.1628986263373052\n",
      "Epoch: 6972 \t|| Train Loss: 0.06966434067357587 \t|| Test Loss: 0.16278842633999824\n",
      "Epoch: 6973 \t|| Train Loss: 0.06963063621188777 \t|| Test Loss: 0.16274691634230795\n",
      "Epoch: 6974 \t|| Train Loss: 0.06959582067692113 \t|| Test Loss: 0.16263671634500101\n",
      "Epoch: 6975 \t|| Train Loss: 0.06956173521481646 \t|| Test Loss: 0.1625952063473107\n",
      "Epoch: 6976 \t|| Train Loss: 0.0695273006802664 \t|| Test Loss: 0.16248500635000376\n",
      "Epoch: 6977 \t|| Train Loss: 0.06949283421774513 \t|| Test Loss: 0.16244349635231348\n",
      "Epoch: 6978 \t|| Train Loss: 0.06945878068361168 \t|| Test Loss: 0.16233329635500646\n",
      "Epoch: 6979 \t|| Train Loss: 0.0694239332206738 \t|| Test Loss: 0.1622917863573162\n",
      "Epoch: 6980 \t|| Train Loss: 0.06939026068695694 \t|| Test Loss: 0.1621815863600092\n",
      "Epoch: 6981 \t|| Train Loss: 0.06935536068874062 \t|| Test Loss: 0.16207138636270227\n",
      "Epoch: 6982 \t|| Train Loss: 0.06932141222516411 \t|| Test Loss: 0.16202987636501195\n",
      "Epoch: 6983 \t|| Train Loss: 0.06928684069208589 \t|| Test Loss: 0.161919676367705\n",
      "Epoch: 6984 \t|| Train Loss: 0.06925251122809277 \t|| Test Loss: 0.16187816637001473\n",
      "Epoch: 6985 \t|| Train Loss: 0.06921832069543117 \t|| Test Loss: 0.16176796637270774\n",
      "Epoch: 6986 \t|| Train Loss: 0.06918361023102146 \t|| Test Loss: 0.16172645637501748\n",
      "Epoch: 6987 \t|| Train Loss: 0.06914980069877644 \t|| Test Loss: 0.1616162563777105\n",
      "Epoch: 6988 \t|| Train Loss: 0.06911490070056012 \t|| Test Loss: 0.1615060563804035\n",
      "Epoch: 6989 \t|| Train Loss: 0.06908108923551175 \t|| Test Loss: 0.1614645463827132\n",
      "Epoch: 6990 \t|| Train Loss: 0.06904638070390538 \t|| Test Loss: 0.16135434638540622\n",
      "Epoch: 6991 \t|| Train Loss: 0.06901218823844044 \t|| Test Loss: 0.16131283638771596\n",
      "Epoch: 6992 \t|| Train Loss: 0.06897786070725065 \t|| Test Loss: 0.16120263639040897\n",
      "Epoch: 6993 \t|| Train Loss: 0.06894328724136913 \t|| Test Loss: 0.1611611263927187\n",
      "Epoch: 6994 \t|| Train Loss: 0.06890934071059593 \t|| Test Loss: 0.16105092639541171\n",
      "Epoch: 6995 \t|| Train Loss: 0.06887444071237961 \t|| Test Loss: 0.16094072639810472\n",
      "Epoch: 6996 \t|| Train Loss: 0.06884076624585941 \t|| Test Loss: 0.16089921640041455\n",
      "Epoch: 6997 \t|| Train Loss: 0.06880592071572489 \t|| Test Loss: 0.16078901640310747\n",
      "Epoch: 6998 \t|| Train Loss: 0.06877186524878809 \t|| Test Loss: 0.1607475064054172\n",
      "Epoch: 6999 \t|| Train Loss: 0.06873740071907017 \t|| Test Loss: 0.16063730640811022\n",
      "Epoch: 7000 \t|| Train Loss: 0.06870296425171679 \t|| Test Loss: 0.16059579641041996\n",
      "Epoch: 7001 \t|| Train Loss: 0.06866888072241542 \t|| Test Loss: 0.16048559641311302\n",
      "Epoch: 7002 \t|| Train Loss: 0.06863406325464547 \t|| Test Loss: 0.16044408641542274\n",
      "Epoch: 7003 \t|| Train Loss: 0.0686003607257607 \t|| Test Loss: 0.16033388641811575\n",
      "Epoch: 7004 \t|| Train Loss: 0.06856546072754437 \t|| Test Loss: 0.16022368642080873\n",
      "Epoch: 7005 \t|| Train Loss: 0.06853154225913576 \t|| Test Loss: 0.16018217642311847\n",
      "Epoch: 7006 \t|| Train Loss: 0.06849694073088963 \t|| Test Loss: 0.1600719764258115\n",
      "Epoch: 7007 \t|| Train Loss: 0.06846264126206444 \t|| Test Loss: 0.16003046642812124\n",
      "Epoch: 7008 \t|| Train Loss: 0.06842842073423491 \t|| Test Loss: 0.1599202664308142\n",
      "Epoch: 7009 \t|| Train Loss: 0.06839374026499309 \t|| Test Loss: 0.15987875643312402\n",
      "Epoch: 7010 \t|| Train Loss: 0.06835990073758019 \t|| Test Loss: 0.15976855643581703\n",
      "Epoch: 7011 \t|| Train Loss: 0.06832500073936387 \t|| Test Loss: 0.15965835643851\n",
      "Epoch: 7012 \t|| Train Loss: 0.06829121926948341 \t|| Test Loss: 0.15961684644081978\n",
      "Epoch: 7013 \t|| Train Loss: 0.06825648074270914 \t|| Test Loss: 0.15950664644351273\n",
      "Epoch: 7014 \t|| Train Loss: 0.06822231827241208 \t|| Test Loss: 0.15946513644582247\n",
      "Epoch: 7015 \t|| Train Loss: 0.0681879607460544 \t|| Test Loss: 0.15935493644851548\n",
      "Epoch: 7016 \t|| Train Loss: 0.06815341727534077 \t|| Test Loss: 0.15931342645082525\n",
      "Epoch: 7017 \t|| Train Loss: 0.0681194407493997 \t|| Test Loss: 0.15920322645351823\n",
      "Epoch: 7018 \t|| Train Loss: 0.06808454075118335 \t|| Test Loss: 0.15909302645621123\n",
      "Epoch: 7019 \t|| Train Loss: 0.06805089627983106 \t|| Test Loss: 0.159051516458521\n",
      "Epoch: 7020 \t|| Train Loss: 0.06801602075452862 \t|| Test Loss: 0.15894131646121396\n",
      "Epoch: 7021 \t|| Train Loss: 0.06798199528275975 \t|| Test Loss: 0.15889980646352375\n",
      "Epoch: 7022 \t|| Train Loss: 0.0679475007578739 \t|| Test Loss: 0.15878960646621676\n",
      "Epoch: 7023 \t|| Train Loss: 0.06791309428568842 \t|| Test Loss: 0.15874809646852653\n",
      "Epoch: 7024 \t|| Train Loss: 0.06787898076121918 \t|| Test Loss: 0.15863789647121948\n",
      "Epoch: 7025 \t|| Train Loss: 0.0678441932886171 \t|| Test Loss: 0.15859638647352922\n",
      "Epoch: 7026 \t|| Train Loss: 0.06781046076456446 \t|| Test Loss: 0.15848618647622223\n",
      "Epoch: 7027 \t|| Train Loss: 0.06777556076634814 \t|| Test Loss: 0.15837598647891524\n",
      "Epoch: 7028 \t|| Train Loss: 0.0677416722931074 \t|| Test Loss: 0.15833447648122498\n",
      "Epoch: 7029 \t|| Train Loss: 0.06770704076969339 \t|| Test Loss: 0.15822427648391799\n",
      "Epoch: 7030 \t|| Train Loss: 0.06767277129603608 \t|| Test Loss: 0.15818276648622773\n",
      "Epoch: 7031 \t|| Train Loss: 0.06763852077303867 \t|| Test Loss: 0.15807256648892073\n",
      "Epoch: 7032 \t|| Train Loss: 0.06760387029896475 \t|| Test Loss: 0.15803105649123048\n",
      "Epoch: 7033 \t|| Train Loss: 0.06757000077638393 \t|| Test Loss: 0.15792085649392348\n",
      "Epoch: 7034 \t|| Train Loss: 0.06753510077816761 \t|| Test Loss: 0.1578106564966165\n",
      "Epoch: 7035 \t|| Train Loss: 0.06750134930345504 \t|| Test Loss: 0.15776914649892626\n",
      "Epoch: 7036 \t|| Train Loss: 0.06746658078151288 \t|| Test Loss: 0.15765894650161927\n",
      "Epoch: 7037 \t|| Train Loss: 0.06743244830638372 \t|| Test Loss: 0.15761743650392898\n",
      "Epoch: 7038 \t|| Train Loss: 0.06739806078485816 \t|| Test Loss: 0.15750723650662204\n",
      "Epoch: 7039 \t|| Train Loss: 0.0673635473093124 \t|| Test Loss: 0.15746572650893173\n",
      "Epoch: 7040 \t|| Train Loss: 0.06732954078820343 \t|| Test Loss: 0.15735552651162474\n",
      "Epoch: 7041 \t|| Train Loss: 0.06729464631224111 \t|| Test Loss: 0.15731401651393445\n",
      "Epoch: 7042 \t|| Train Loss: 0.06726102079154869 \t|| Test Loss: 0.1572038165166275\n",
      "Epoch: 7043 \t|| Train Loss: 0.06722612079333237 \t|| Test Loss: 0.1570936165193205\n",
      "Epoch: 7044 \t|| Train Loss: 0.06719212531673138 \t|| Test Loss: 0.15705210652163026\n",
      "Epoch: 7045 \t|| Train Loss: 0.06715760079667764 \t|| Test Loss: 0.15694190652432324\n",
      "Epoch: 7046 \t|| Train Loss: 0.06712322431966006 \t|| Test Loss: 0.15690039652663304\n",
      "Epoch: 7047 \t|| Train Loss: 0.06708908080002293 \t|| Test Loss: 0.15679019652932602\n",
      "Epoch: 7048 \t|| Train Loss: 0.06705432332258876 \t|| Test Loss: 0.15674868653163576\n",
      "Epoch: 7049 \t|| Train Loss: 0.06702056080336818 \t|| Test Loss: 0.15663848653432874\n",
      "Epoch: 7050 \t|| Train Loss: 0.06698566080515185 \t|| Test Loss: 0.15652828653702178\n",
      "Epoch: 7051 \t|| Train Loss: 0.06695180232707904 \t|| Test Loss: 0.15648677653933152\n",
      "Epoch: 7052 \t|| Train Loss: 0.06691714080849714 \t|| Test Loss: 0.1563765765420245\n",
      "Epoch: 7053 \t|| Train Loss: 0.06688290133000771 \t|| Test Loss: 0.15633506654433424\n",
      "Epoch: 7054 \t|| Train Loss: 0.06684862081184242 \t|| Test Loss: 0.15622486654702727\n",
      "Epoch: 7055 \t|| Train Loss: 0.06681400033293641 \t|| Test Loss: 0.15618335654933702\n",
      "Epoch: 7056 \t|| Train Loss: 0.06678010081518768 \t|| Test Loss: 0.15607315655203002\n",
      "Epoch: 7057 \t|| Train Loss: 0.06674520081697136 \t|| Test Loss: 0.155962956554723\n",
      "Epoch: 7058 \t|| Train Loss: 0.06671147933742669 \t|| Test Loss: 0.15592144655703274\n",
      "Epoch: 7059 \t|| Train Loss: 0.06667668082031664 \t|| Test Loss: 0.15581124655972575\n",
      "Epoch: 7060 \t|| Train Loss: 0.06664257834035538 \t|| Test Loss: 0.1557697365620355\n",
      "Epoch: 7061 \t|| Train Loss: 0.0666081608236619 \t|| Test Loss: 0.15565953656472847\n",
      "Epoch: 7062 \t|| Train Loss: 0.06657367734328405 \t|| Test Loss: 0.15561802656703824\n",
      "Epoch: 7063 \t|| Train Loss: 0.06653964082700718 \t|| Test Loss: 0.15550782656973125\n",
      "Epoch: 7064 \t|| Train Loss: 0.06650477634621274 \t|| Test Loss: 0.155466316572041\n",
      "Epoch: 7065 \t|| Train Loss: 0.06647112083035246 \t|| Test Loss: 0.155356116574734\n",
      "Epoch: 7066 \t|| Train Loss: 0.06643622083213611 \t|| Test Loss: 0.155245916577427\n",
      "Epoch: 7067 \t|| Train Loss: 0.06640225535070302 \t|| Test Loss: 0.15520440657973675\n",
      "Epoch: 7068 \t|| Train Loss: 0.06636770083548141 \t|| Test Loss: 0.15509420658242976\n",
      "Epoch: 7069 \t|| Train Loss: 0.06633335435363172 \t|| Test Loss: 0.1550526965847395\n",
      "Epoch: 7070 \t|| Train Loss: 0.06629918083882666 \t|| Test Loss: 0.1549424965874325\n",
      "Epoch: 7071 \t|| Train Loss: 0.0662644533565604 \t|| Test Loss: 0.1549009865897423\n",
      "Epoch: 7072 \t|| Train Loss: 0.06623066084217195 \t|| Test Loss: 0.15479078659243525\n",
      "Epoch: 7073 \t|| Train Loss: 0.0661957608439556 \t|| Test Loss: 0.1546805865951283\n",
      "Epoch: 7074 \t|| Train Loss: 0.06616193236105068 \t|| Test Loss: 0.154639076597438\n",
      "Epoch: 7075 \t|| Train Loss: 0.06612724084730089 \t|| Test Loss: 0.154528876600131\n",
      "Epoch: 7076 \t|| Train Loss: 0.06609303136397934 \t|| Test Loss: 0.15448736660244072\n",
      "Epoch: 7077 \t|| Train Loss: 0.06605872085064615 \t|| Test Loss: 0.15437716660513376\n",
      "Epoch: 7078 \t|| Train Loss: 0.06602413036690805 \t|| Test Loss: 0.15433565660744347\n",
      "Epoch: 7079 \t|| Train Loss: 0.06599020085399143 \t|| Test Loss: 0.15422545661013654\n",
      "Epoch: 7080 \t|| Train Loss: 0.0659553008557751 \t|| Test Loss: 0.1541152566128295\n",
      "Epoch: 7081 \t|| Train Loss: 0.06592160937139833 \t|| Test Loss: 0.15407374661513923\n",
      "Epoch: 7082 \t|| Train Loss: 0.06588678085912036 \t|| Test Loss: 0.15396354661783226\n",
      "Epoch: 7083 \t|| Train Loss: 0.06585270837432702 \t|| Test Loss: 0.15392203662014198\n",
      "Epoch: 7084 \t|| Train Loss: 0.06581826086246564 \t|| Test Loss: 0.153811836622835\n",
      "Epoch: 7085 \t|| Train Loss: 0.06578380737725568 \t|| Test Loss: 0.15377032662514475\n",
      "Epoch: 7086 \t|| Train Loss: 0.06574974086581092 \t|| Test Loss: 0.15366012662783776\n",
      "Epoch: 7087 \t|| Train Loss: 0.06571490638018437 \t|| Test Loss: 0.15361861663014748\n",
      "Epoch: 7088 \t|| Train Loss: 0.0656812208691562 \t|| Test Loss: 0.1535084166328405\n",
      "Epoch: 7089 \t|| Train Loss: 0.06564632087093987 \t|| Test Loss: 0.15339821663553352\n",
      "Epoch: 7090 \t|| Train Loss: 0.06561238538467468 \t|| Test Loss: 0.15335670663784326\n",
      "Epoch: 7091 \t|| Train Loss: 0.06557780087428514 \t|| Test Loss: 0.15324650664053627\n",
      "Epoch: 7092 \t|| Train Loss: 0.06554348438760335 \t|| Test Loss: 0.153204996642846\n",
      "Epoch: 7093 \t|| Train Loss: 0.06550928087763042 \t|| Test Loss: 0.15309479664553902\n",
      "Epoch: 7094 \t|| Train Loss: 0.06547458339053204 \t|| Test Loss: 0.15305328664784876\n",
      "Epoch: 7095 \t|| Train Loss: 0.06544076088097568 \t|| Test Loss: 0.15294308665054177\n",
      "Epoch: 7096 \t|| Train Loss: 0.06540586088275935 \t|| Test Loss: 0.15283288665323477\n",
      "Epoch: 7097 \t|| Train Loss: 0.06537206239502233 \t|| Test Loss: 0.15279137665554451\n",
      "Epoch: 7098 \t|| Train Loss: 0.06533734088610463 \t|| Test Loss: 0.15268117665823752\n",
      "Epoch: 7099 \t|| Train Loss: 0.065303161397951 \t|| Test Loss: 0.15263966666054726\n",
      "Epoch: 7100 \t|| Train Loss: 0.0652688208894499 \t|| Test Loss: 0.15252946666324027\n",
      "Epoch: 7101 \t|| Train Loss: 0.06523426040087968 \t|| Test Loss: 0.15248795666555\n",
      "Epoch: 7102 \t|| Train Loss: 0.06520030089279519 \t|| Test Loss: 0.152377756668243\n",
      "Epoch: 7103 \t|| Train Loss: 0.06516540089457884 \t|| Test Loss: 0.15226755667093597\n",
      "Epoch: 7104 \t|| Train Loss: 0.06513173940536998 \t|| Test Loss: 0.15222604667324577\n",
      "Epoch: 7105 \t|| Train Loss: 0.06509688089792412 \t|| Test Loss: 0.15211584667593878\n",
      "Epoch: 7106 \t|| Train Loss: 0.06506283840829866 \t|| Test Loss: 0.15207433667824857\n",
      "Epoch: 7107 \t|| Train Loss: 0.06502836090126941 \t|| Test Loss: 0.15196413668094153\n",
      "Epoch: 7108 \t|| Train Loss: 0.06499393741122736 \t|| Test Loss: 0.15192262668325127\n",
      "Epoch: 7109 \t|| Train Loss: 0.06495984090461468 \t|| Test Loss: 0.15181242668594427\n",
      "Epoch: 7110 \t|| Train Loss: 0.06492503641415602 \t|| Test Loss: 0.151770916688254\n",
      "Epoch: 7111 \t|| Train Loss: 0.06489132090795995 \t|| Test Loss: 0.15166071669094705\n",
      "Epoch: 7112 \t|| Train Loss: 0.06485642090974361 \t|| Test Loss: 0.15155051669364\n",
      "Epoch: 7113 \t|| Train Loss: 0.06482251541864632 \t|| Test Loss: 0.15150900669594977\n",
      "Epoch: 7114 \t|| Train Loss: 0.06478790091308888 \t|| Test Loss: 0.15139880669864275\n",
      "Epoch: 7115 \t|| Train Loss: 0.06475361442157498 \t|| Test Loss: 0.1513572967009525\n",
      "Epoch: 7116 \t|| Train Loss: 0.06471938091643416 \t|| Test Loss: 0.15124709670364553\n",
      "Epoch: 7117 \t|| Train Loss: 0.06468471342450367 \t|| Test Loss: 0.15120558670595527\n",
      "Epoch: 7118 \t|| Train Loss: 0.06465086091977945 \t|| Test Loss: 0.15109538670864825\n",
      "Epoch: 7119 \t|| Train Loss: 0.0646159609215631 \t|| Test Loss: 0.15098518671134126\n",
      "Epoch: 7120 \t|| Train Loss: 0.06458219242899396 \t|| Test Loss: 0.15094367671365103\n",
      "Epoch: 7121 \t|| Train Loss: 0.06454744092490838 \t|| Test Loss: 0.15083347671634403\n",
      "Epoch: 7122 \t|| Train Loss: 0.06451329143192264 \t|| Test Loss: 0.15079196671865377\n",
      "Epoch: 7123 \t|| Train Loss: 0.06447892092825365 \t|| Test Loss: 0.15068176672134678\n",
      "Epoch: 7124 \t|| Train Loss: 0.06444439043485133 \t|| Test Loss: 0.1506402567236565\n",
      "Epoch: 7125 \t|| Train Loss: 0.06441040093159892 \t|| Test Loss: 0.1505300567263495\n",
      "Epoch: 7126 \t|| Train Loss: 0.0643755009333826 \t|| Test Loss: 0.15041985672904254\n",
      "Epoch: 7127 \t|| Train Loss: 0.06434186943934161 \t|| Test Loss: 0.15037834673135225\n",
      "Epoch: 7128 \t|| Train Loss: 0.06430698093672786 \t|| Test Loss: 0.1502681467340453\n",
      "Epoch: 7129 \t|| Train Loss: 0.0642729684422703 \t|| Test Loss: 0.15022663673635503\n",
      "Epoch: 7130 \t|| Train Loss: 0.06423846094007313 \t|| Test Loss: 0.150116436739048\n",
      "Epoch: 7131 \t|| Train Loss: 0.06420406744519898 \t|| Test Loss: 0.15007492674135778\n",
      "Epoch: 7132 \t|| Train Loss: 0.06416994094341841 \t|| Test Loss: 0.14996472674405079\n",
      "Epoch: 7133 \t|| Train Loss: 0.06413516644812767 \t|| Test Loss: 0.14992321674636053\n",
      "Epoch: 7134 \t|| Train Loss: 0.06410142094676369 \t|| Test Loss: 0.14981301674905353\n",
      "Epoch: 7135 \t|| Train Loss: 0.06406652094854735 \t|| Test Loss: 0.14970281675174654\n",
      "Epoch: 7136 \t|| Train Loss: 0.06403264545261796 \t|| Test Loss: 0.14966130675405626\n",
      "Epoch: 7137 \t|| Train Loss: 0.06399800095189263 \t|| Test Loss: 0.1495511067567493\n",
      "Epoch: 7138 \t|| Train Loss: 0.06396374445554666 \t|| Test Loss: 0.14950959675905903\n",
      "Epoch: 7139 \t|| Train Loss: 0.0639294809552379 \t|| Test Loss: 0.149399396761752\n",
      "Epoch: 7140 \t|| Train Loss: 0.06389484345847532 \t|| Test Loss: 0.14935788676406178\n",
      "Epoch: 7141 \t|| Train Loss: 0.06386096095858318 \t|| Test Loss: 0.1492476867667548\n",
      "Epoch: 7142 \t|| Train Loss: 0.06382606096036685 \t|| Test Loss: 0.1491374867694478\n",
      "Epoch: 7143 \t|| Train Loss: 0.06379232246296561 \t|| Test Loss: 0.14909597677175754\n",
      "Epoch: 7144 \t|| Train Loss: 0.06375754096371211 \t|| Test Loss: 0.14898577677445052\n",
      "Epoch: 7145 \t|| Train Loss: 0.0637234214658943 \t|| Test Loss: 0.1489442667767603\n",
      "Epoch: 7146 \t|| Train Loss: 0.06368902096705739 \t|| Test Loss: 0.1488340667794533\n",
      "Epoch: 7147 \t|| Train Loss: 0.06365452046882299 \t|| Test Loss: 0.148792556781763\n",
      "Epoch: 7148 \t|| Train Loss: 0.06362050097040269 \t|| Test Loss: 0.14868235678445607\n",
      "Epoch: 7149 \t|| Train Loss: 0.06358561947175165 \t|| Test Loss: 0.14864084678676576\n",
      "Epoch: 7150 \t|| Train Loss: 0.06355198097374795 \t|| Test Loss: 0.14853064678945876\n",
      "Epoch: 7151 \t|| Train Loss: 0.06351708097553162 \t|| Test Loss: 0.14842044679215177\n",
      "Epoch: 7152 \t|| Train Loss: 0.06348309847624194 \t|| Test Loss: 0.1483789367944615\n",
      "Epoch: 7153 \t|| Train Loss: 0.06344856097887688 \t|| Test Loss: 0.14826873679715452\n",
      "Epoch: 7154 \t|| Train Loss: 0.06341419747917064 \t|| Test Loss: 0.1482272267994643\n",
      "Epoch: 7155 \t|| Train Loss: 0.06338004098222216 \t|| Test Loss: 0.14811702680215727\n",
      "Epoch: 7156 \t|| Train Loss: 0.06334529648209933 \t|| Test Loss: 0.14807551680446704\n",
      "Epoch: 7157 \t|| Train Loss: 0.06331152098556743 \t|| Test Loss: 0.14796531680716005\n",
      "Epoch: 7158 \t|| Train Loss: 0.06327662098735111 \t|| Test Loss: 0.14785511680985303\n",
      "Epoch: 7159 \t|| Train Loss: 0.0632427754865896 \t|| Test Loss: 0.14781360681216277\n",
      "Epoch: 7160 \t|| Train Loss: 0.06320810099069638 \t|| Test Loss: 0.1477034068148558\n",
      "Epoch: 7161 \t|| Train Loss: 0.06317387448951828 \t|| Test Loss: 0.14766189681716554\n",
      "Epoch: 7162 \t|| Train Loss: 0.06313958099404166 \t|| Test Loss: 0.14755169681985855\n",
      "Epoch: 7163 \t|| Train Loss: 0.06310497349244697 \t|| Test Loss: 0.14751018682216827\n",
      "Epoch: 7164 \t|| Train Loss: 0.06307106099738694 \t|| Test Loss: 0.1473999868248613\n",
      "Epoch: 7165 \t|| Train Loss: 0.0630361609991706 \t|| Test Loss: 0.14728978682755428\n",
      "Epoch: 7166 \t|| Train Loss: 0.06300245249693726 \t|| Test Loss: 0.14724827682986402\n",
      "Epoch: 7167 \t|| Train Loss: 0.06296764100251587 \t|| Test Loss: 0.14713807683255703\n",
      "Epoch: 7168 \t|| Train Loss: 0.06293355149986594 \t|| Test Loss: 0.1470965668348668\n",
      "Epoch: 7169 \t|| Train Loss: 0.06289912100586115 \t|| Test Loss: 0.1469863668375598\n",
      "Epoch: 7170 \t|| Train Loss: 0.06286465050279462 \t|| Test Loss: 0.14694485683986952\n",
      "Epoch: 7171 \t|| Train Loss: 0.06283060100920641 \t|| Test Loss: 0.14683465684256253\n",
      "Epoch: 7172 \t|| Train Loss: 0.06279574950572331 \t|| Test Loss: 0.14679314684487227\n",
      "Epoch: 7173 \t|| Train Loss: 0.0627620810125517 \t|| Test Loss: 0.14668294684756528\n",
      "Epoch: 7174 \t|| Train Loss: 0.06272718101433536 \t|| Test Loss: 0.14657274685025828\n",
      "Epoch: 7175 \t|| Train Loss: 0.0626932285102136 \t|| Test Loss: 0.14653123685256803\n",
      "Epoch: 7176 \t|| Train Loss: 0.06265866101768063 \t|| Test Loss: 0.14642103685526103\n",
      "Epoch: 7177 \t|| Train Loss: 0.06262432751314227 \t|| Test Loss: 0.14637952685757077\n",
      "Epoch: 7178 \t|| Train Loss: 0.06259014102102592 \t|| Test Loss: 0.14626932686026378\n",
      "Epoch: 7179 \t|| Train Loss: 0.06255542651607097 \t|| Test Loss: 0.14622781686257352\n",
      "Epoch: 7180 \t|| Train Loss: 0.06252162102437117 \t|| Test Loss: 0.14611761686526653\n",
      "Epoch: 7181 \t|| Train Loss: 0.062486721026154844 \t|| Test Loss: 0.14600741686795954\n",
      "Epoch: 7182 \t|| Train Loss: 0.06245290552056124 \t|| Test Loss: 0.14596590687026928\n",
      "Epoch: 7183 \t|| Train Loss: 0.06241820102950013 \t|| Test Loss: 0.14585570687296234\n",
      "Epoch: 7184 \t|| Train Loss: 0.06238400452348993 \t|| Test Loss: 0.14581419687527206\n",
      "Epoch: 7185 \t|| Train Loss: 0.0623496810328454 \t|| Test Loss: 0.14570399687796504\n",
      "Epoch: 7186 \t|| Train Loss: 0.062315103526418614 \t|| Test Loss: 0.14566248688027478\n",
      "Epoch: 7187 \t|| Train Loss: 0.06228116103619067 \t|| Test Loss: 0.14555228688296779\n",
      "Epoch: 7188 \t|| Train Loss: 0.06224626103797435 \t|| Test Loss: 0.1454420868856608\n",
      "Epoch: 7189 \t|| Train Loss: 0.06221258253090889 \t|| Test Loss: 0.14540057688797056\n",
      "Epoch: 7190 \t|| Train Loss: 0.06217774104131961 \t|| Test Loss: 0.14529037689066354\n",
      "Epoch: 7191 \t|| Train Loss: 0.06214368153383759 \t|| Test Loss: 0.14524886689297328\n",
      "Epoch: 7192 \t|| Train Loss: 0.06210922104466489 \t|| Test Loss: 0.14513866689566632\n",
      "Epoch: 7193 \t|| Train Loss: 0.06207478053676627 \t|| Test Loss: 0.14509715689797603\n",
      "Epoch: 7194 \t|| Train Loss: 0.06204070104801015 \t|| Test Loss: 0.14498695690066907\n",
      "Epoch: 7195 \t|| Train Loss: 0.062005879539694955 \t|| Test Loss: 0.1449454469029788\n",
      "Epoch: 7196 \t|| Train Loss: 0.06197218105135545 \t|| Test Loss: 0.14483524690567182\n",
      "Epoch: 7197 \t|| Train Loss: 0.061937281053139094 \t|| Test Loss: 0.1447250469083648\n",
      "Epoch: 7198 \t|| Train Loss: 0.06190335854418523 \t|| Test Loss: 0.14468353691067454\n",
      "Epoch: 7199 \t|| Train Loss: 0.06186876105648438 \t|| Test Loss: 0.14457333691336754\n",
      "Epoch: 7200 \t|| Train Loss: 0.06183445754711394 \t|| Test Loss: 0.14453182691567731\n",
      "Epoch: 7201 \t|| Train Loss: 0.061800241059829654 \t|| Test Loss: 0.14442162691837032\n",
      "Epoch: 7202 \t|| Train Loss: 0.0617655565500426 \t|| Test Loss: 0.14438011692068003\n",
      "Epoch: 7203 \t|| Train Loss: 0.06173172106317493 \t|| Test Loss: 0.14426991692337304\n",
      "Epoch: 7204 \t|| Train Loss: 0.061696821064958586 \t|| Test Loss: 0.14415971692606605\n",
      "Epoch: 7205 \t|| Train Loss: 0.0616630355545329 \t|| Test Loss: 0.1441182069283758\n",
      "Epoch: 7206 \t|| Train Loss: 0.06162830106830386 \t|| Test Loss: 0.1440080069310688\n",
      "Epoch: 7207 \t|| Train Loss: 0.06159413455746158 \t|| Test Loss: 0.14396649693337854\n",
      "Epoch: 7208 \t|| Train Loss: 0.061559781071649146 \t|| Test Loss: 0.14385629693607155\n",
      "Epoch: 7209 \t|| Train Loss: 0.06152523356039026 \t|| Test Loss: 0.1438147869383813\n",
      "Epoch: 7210 \t|| Train Loss: 0.06149126107499442 \t|| Test Loss: 0.1437045869410743\n",
      "Epoch: 7211 \t|| Train Loss: 0.061456361076778085 \t|| Test Loss: 0.1435943869437673\n",
      "Epoch: 7212 \t|| Train Loss: 0.06142271256488054 \t|| Test Loss: 0.14355287694607705\n",
      "Epoch: 7213 \t|| Train Loss: 0.06138784108012335 \t|| Test Loss: 0.14344267694877\n",
      "Epoch: 7214 \t|| Train Loss: 0.06135381156780921 \t|| Test Loss: 0.1434011669510798\n",
      "Epoch: 7215 \t|| Train Loss: 0.06131932108346864 \t|| Test Loss: 0.1432909669537728\n",
      "Epoch: 7216 \t|| Train Loss: 0.06128491057073791 \t|| Test Loss: 0.1432494569560826\n",
      "Epoch: 7217 \t|| Train Loss: 0.06125080108681391 \t|| Test Loss: 0.14313925695877555\n",
      "Epoch: 7218 \t|| Train Loss: 0.061216009573666595 \t|| Test Loss: 0.14309774696108532\n",
      "Epoch: 7219 \t|| Train Loss: 0.061182281090159184 \t|| Test Loss: 0.1429875469637783\n",
      "Epoch: 7220 \t|| Train Loss: 0.06114738109194286 \t|| Test Loss: 0.14287734696647134\n",
      "Epoch: 7221 \t|| Train Loss: 0.061113488578156884 \t|| Test Loss: 0.14283583696878105\n",
      "Epoch: 7222 \t|| Train Loss: 0.06107886109528814 \t|| Test Loss: 0.14272563697147406\n",
      "Epoch: 7223 \t|| Train Loss: 0.06104458758108557 \t|| Test Loss: 0.1426841269737838\n",
      "Epoch: 7224 \t|| Train Loss: 0.06101034109863342 \t|| Test Loss: 0.1425739269764768\n",
      "Epoch: 7225 \t|| Train Loss: 0.06097568658401424 \t|| Test Loss: 0.14253241697878655\n",
      "Epoch: 7226 \t|| Train Loss: 0.060941821101978676 \t|| Test Loss: 0.14242221698147955\n",
      "Epoch: 7227 \t|| Train Loss: 0.06090692110376235 \t|| Test Loss: 0.14231201698417256\n",
      "Epoch: 7228 \t|| Train Loss: 0.060873165588504544 \t|| Test Loss: 0.14227050698648228\n",
      "Epoch: 7229 \t|| Train Loss: 0.06083840110710761 \t|| Test Loss: 0.1421603069891753\n",
      "Epoch: 7230 \t|| Train Loss: 0.06080426459143321 \t|| Test Loss: 0.14211879699148508\n",
      "Epoch: 7231 \t|| Train Loss: 0.060769881110452895 \t|| Test Loss: 0.14200859699417806\n",
      "Epoch: 7232 \t|| Train Loss: 0.0607353635943619 \t|| Test Loss: 0.1419670869964878\n",
      "Epoch: 7233 \t|| Train Loss: 0.06070136111379817 \t|| Test Loss: 0.1418568869991808\n",
      "Epoch: 7234 \t|| Train Loss: 0.060666462597290596 \t|| Test Loss: 0.14181537700149055\n",
      "Epoch: 7235 \t|| Train Loss: 0.06063284111714344 \t|| Test Loss: 0.14170517700418356\n",
      "Epoch: 7236 \t|| Train Loss: 0.060597941118927114 \t|| Test Loss: 0.14159497700687657\n",
      "Epoch: 7237 \t|| Train Loss: 0.06056394160178088 \t|| Test Loss: 0.1415534670091863\n",
      "Epoch: 7238 \t|| Train Loss: 0.06052942112227239 \t|| Test Loss: 0.14144326701187931\n",
      "Epoch: 7239 \t|| Train Loss: 0.06049504060470956 \t|| Test Loss: 0.14140175701418906\n",
      "Epoch: 7240 \t|| Train Loss: 0.06046090112561766 \t|| Test Loss: 0.14129155701688206\n",
      "Epoch: 7241 \t|| Train Loss: 0.06042613960763823 \t|| Test Loss: 0.1412500470191918\n",
      "Epoch: 7242 \t|| Train Loss: 0.060392381128962926 \t|| Test Loss: 0.1411398470218848\n",
      "Epoch: 7243 \t|| Train Loss: 0.060357481130746606 \t|| Test Loss: 0.14102964702457782\n",
      "Epoch: 7244 \t|| Train Loss: 0.06032361861212854 \t|| Test Loss: 0.14098813702688756\n",
      "Epoch: 7245 \t|| Train Loss: 0.060288961134091865 \t|| Test Loss: 0.14087793702958057\n",
      "Epoch: 7246 \t|| Train Loss: 0.060254717615057206 \t|| Test Loss: 0.1408364270318903\n",
      "Epoch: 7247 \t|| Train Loss: 0.060220441137437165 \t|| Test Loss: 0.14072622703458332\n",
      "Epoch: 7248 \t|| Train Loss: 0.0601858166179859 \t|| Test Loss: 0.14068471703689306\n",
      "Epoch: 7249 \t|| Train Loss: 0.06015192114078242 \t|| Test Loss: 0.14057451703958607\n",
      "Epoch: 7250 \t|| Train Loss: 0.060117021142566084 \t|| Test Loss: 0.14046431704227907\n",
      "Epoch: 7251 \t|| Train Loss: 0.060083295622476184 \t|| Test Loss: 0.14042280704458884\n",
      "Epoch: 7252 \t|| Train Loss: 0.06004850114591137 \t|| Test Loss: 0.14031260704728182\n",
      "Epoch: 7253 \t|| Train Loss: 0.060014394625404865 \t|| Test Loss: 0.14027109704959156\n",
      "Epoch: 7254 \t|| Train Loss: 0.05997998114925664 \t|| Test Loss: 0.14016089705228457\n",
      "Epoch: 7255 \t|| Train Loss: 0.05994549362833355 \t|| Test Loss: 0.1401193870545943\n",
      "Epoch: 7256 \t|| Train Loss: 0.05991146115260191 \t|| Test Loss: 0.14000918705728732\n",
      "Epoch: 7257 \t|| Train Loss: 0.059876592631262236 \t|| Test Loss: 0.13996767705959706\n",
      "Epoch: 7258 \t|| Train Loss: 0.059842941155947196 \t|| Test Loss: 0.1398574770622901\n",
      "Epoch: 7259 \t|| Train Loss: 0.05980804115773084 \t|| Test Loss: 0.13974727706498308\n",
      "Epoch: 7260 \t|| Train Loss: 0.059774071635752525 \t|| Test Loss: 0.13970576706729282\n",
      "Epoch: 7261 \t|| Train Loss: 0.05973952116107613 \t|| Test Loss: 0.13959556706998583\n",
      "Epoch: 7262 \t|| Train Loss: 0.05970517063868121 \t|| Test Loss: 0.13955405707229557\n",
      "Epoch: 7263 \t|| Train Loss: 0.0596710011644214 \t|| Test Loss: 0.13944385707498858\n",
      "Epoch: 7264 \t|| Train Loss: 0.05963626964160988 \t|| Test Loss: 0.13940234707729832\n",
      "Epoch: 7265 \t|| Train Loss: 0.059602481167766674 \t|| Test Loss: 0.13929214707999132\n",
      "Epoch: 7266 \t|| Train Loss: 0.05956758116955034 \t|| Test Loss: 0.13918194708268433\n",
      "Epoch: 7267 \t|| Train Loss: 0.05953374864610017 \t|| Test Loss: 0.13914043708499407\n",
      "Epoch: 7268 \t|| Train Loss: 0.05949906117289562 \t|| Test Loss: 0.13903023708768708\n",
      "Epoch: 7269 \t|| Train Loss: 0.05946484764902886 \t|| Test Loss: 0.13898872708999682\n",
      "Epoch: 7270 \t|| Train Loss: 0.05943054117624087 \t|| Test Loss: 0.13887852709268983\n",
      "Epoch: 7271 \t|| Train Loss: 0.059395946651957535 \t|| Test Loss: 0.13883701709499957\n",
      "Epoch: 7272 \t|| Train Loss: 0.05936202117958618 \t|| Test Loss: 0.13872681709769258\n",
      "Epoch: 7273 \t|| Train Loss: 0.05932712118136983 \t|| Test Loss: 0.1386166171003856\n",
      "Epoch: 7274 \t|| Train Loss: 0.05929342565644782 \t|| Test Loss: 0.13857510710269533\n",
      "Epoch: 7275 \t|| Train Loss: 0.05925860118471511 \t|| Test Loss: 0.13846490710538834\n",
      "Epoch: 7276 \t|| Train Loss: 0.05922452465937651 \t|| Test Loss: 0.13842339710769808\n",
      "Epoch: 7277 \t|| Train Loss: 0.05919008118806037 \t|| Test Loss: 0.13831319711039108\n",
      "Epoch: 7278 \t|| Train Loss: 0.0591556236623052 \t|| Test Loss: 0.13827168711270083\n",
      "Epoch: 7279 \t|| Train Loss: 0.05912156119140566 \t|| Test Loss: 0.13816148711539383\n",
      "Epoch: 7280 \t|| Train Loss: 0.05908672266523387 \t|| Test Loss: 0.13811997711770357\n",
      "Epoch: 7281 \t|| Train Loss: 0.059053041194750924 \t|| Test Loss: 0.13800977712039655\n",
      "Epoch: 7282 \t|| Train Loss: 0.059018141196534604 \t|| Test Loss: 0.1378995771230896\n",
      "Epoch: 7283 \t|| Train Loss: 0.058984201669724165 \t|| Test Loss: 0.13785806712539933\n",
      "Epoch: 7284 \t|| Train Loss: 0.05894962119987988 \t|| Test Loss: 0.13774786712809234\n",
      "Epoch: 7285 \t|| Train Loss: 0.05891530067265285 \t|| Test Loss: 0.13770635713040208\n",
      "Epoch: 7286 \t|| Train Loss: 0.05888110120322515 \t|| Test Loss: 0.1375961571330951\n",
      "Epoch: 7287 \t|| Train Loss: 0.05884639967558153 \t|| Test Loss: 0.13755464713540483\n",
      "Epoch: 7288 \t|| Train Loss: 0.05881258120657042 \t|| Test Loss: 0.13744444713809784\n",
      "Epoch: 7289 \t|| Train Loss: 0.0587776812083541 \t|| Test Loss: 0.13733424714079082\n",
      "Epoch: 7290 \t|| Train Loss: 0.058743878680071825 \t|| Test Loss: 0.13729273714310058\n",
      "Epoch: 7291 \t|| Train Loss: 0.058709161211699355 \t|| Test Loss: 0.1371825371457936\n",
      "Epoch: 7292 \t|| Train Loss: 0.05867497768300051 \t|| Test Loss: 0.13714102714810333\n",
      "Epoch: 7293 \t|| Train Loss: 0.05864064121504464 \t|| Test Loss: 0.13703082715079637\n",
      "Epoch: 7294 \t|| Train Loss: 0.05860607668592919 \t|| Test Loss: 0.13698931715310608\n",
      "Epoch: 7295 \t|| Train Loss: 0.05857212121838991 \t|| Test Loss: 0.1368791171557991\n",
      "Epoch: 7296 \t|| Train Loss: 0.058537221220173574 \t|| Test Loss: 0.1367689171584921\n",
      "Epoch: 7297 \t|| Train Loss: 0.05850355569041947 \t|| Test Loss: 0.13672740716080184\n",
      "Epoch: 7298 \t|| Train Loss: 0.05846870122351886 \t|| Test Loss: 0.13661720716349482\n",
      "Epoch: 7299 \t|| Train Loss: 0.05843465469334815 \t|| Test Loss: 0.1365756971658046\n",
      "Epoch: 7300 \t|| Train Loss: 0.058400181226864134 \t|| Test Loss: 0.1364654971684976\n",
      "Epoch: 7301 \t|| Train Loss: 0.058365753696276834 \t|| Test Loss: 0.1364239871708073\n",
      "Epoch: 7302 \t|| Train Loss: 0.0583316612302094 \t|| Test Loss: 0.13631378717350034\n",
      "Epoch: 7303 \t|| Train Loss: 0.05829685269920552 \t|| Test Loss: 0.13627227717581006\n",
      "Epoch: 7304 \t|| Train Loss: 0.05826314123355467 \t|| Test Loss: 0.1361620771785031\n",
      "Epoch: 7305 \t|| Train Loss: 0.05822824123533835 \t|| Test Loss: 0.1360518771811961\n",
      "Epoch: 7306 \t|| Train Loss: 0.05819433170369581 \t|| Test Loss: 0.13601036718350584\n",
      "Epoch: 7307 \t|| Train Loss: 0.058159721238683626 \t|| Test Loss: 0.13590016718619885\n",
      "Epoch: 7308 \t|| Train Loss: 0.058125430706624494 \t|| Test Loss: 0.1358586571885086\n",
      "Epoch: 7309 \t|| Train Loss: 0.05809120124202889 \t|| Test Loss: 0.1357484571912016\n",
      "Epoch: 7310 \t|| Train Loss: 0.05805652970955318 \t|| Test Loss: 0.13570694719351134\n",
      "Epoch: 7311 \t|| Train Loss: 0.05802268124537417 \t|| Test Loss: 0.13559674719620435\n",
      "Epoch: 7312 \t|| Train Loss: 0.057987781247157845 \t|| Test Loss: 0.13548654719889736\n",
      "Epoch: 7313 \t|| Train Loss: 0.05795400871404346 \t|| Test Loss: 0.1354450372012071\n",
      "Epoch: 7314 \t|| Train Loss: 0.057919261250503104 \t|| Test Loss: 0.13533483720390008\n",
      "Epoch: 7315 \t|| Train Loss: 0.05788510771697214 \t|| Test Loss: 0.13529332720620985\n",
      "Epoch: 7316 \t|| Train Loss: 0.05785074125384837 \t|| Test Loss: 0.13518312720890285\n",
      "Epoch: 7317 \t|| Train Loss: 0.05781620671990083 \t|| Test Loss: 0.1351416172112126\n",
      "Epoch: 7318 \t|| Train Loss: 0.05778222125719366 \t|| Test Loss: 0.1350314172139056\n",
      "Epoch: 7319 \t|| Train Loss: 0.05774732125897732 \t|| Test Loss: 0.1349212172165986\n",
      "Epoch: 7320 \t|| Train Loss: 0.05771368572439113 \t|| Test Loss: 0.13487970721890835\n",
      "Epoch: 7321 \t|| Train Loss: 0.057678801262322596 \t|| Test Loss: 0.13476950722160136\n",
      "Epoch: 7322 \t|| Train Loss: 0.0576447847273198 \t|| Test Loss: 0.1347279972239111\n",
      "Epoch: 7323 \t|| Train Loss: 0.057610281265667876 \t|| Test Loss: 0.13461779722660402\n",
      "Epoch: 7324 \t|| Train Loss: 0.05757588373024848 \t|| Test Loss: 0.13457628722891385\n",
      "Epoch: 7325 \t|| Train Loss: 0.057541761269013156 \t|| Test Loss: 0.13446608723160686\n",
      "Epoch: 7326 \t|| Train Loss: 0.057506982733177156 \t|| Test Loss: 0.13442457723391663\n",
      "Epoch: 7327 \t|| Train Loss: 0.05747324127235844 \t|| Test Loss: 0.1343143772366096\n",
      "Epoch: 7328 \t|| Train Loss: 0.05743834127414209 \t|| Test Loss: 0.1342041772393026\n",
      "Epoch: 7329 \t|| Train Loss: 0.057404461737667466 \t|| Test Loss: 0.13416266724161235\n",
      "Epoch: 7330 \t|| Train Loss: 0.05736982127748737 \t|| Test Loss: 0.1340524672443054\n",
      "Epoch: 7331 \t|| Train Loss: 0.057335560740596134 \t|| Test Loss: 0.1340109572466151\n",
      "Epoch: 7332 \t|| Train Loss: 0.05730130128083266 \t|| Test Loss: 0.13390075724930808\n",
      "Epoch: 7333 \t|| Train Loss: 0.05726665974352483 \t|| Test Loss: 0.13385924725161785\n",
      "Epoch: 7334 \t|| Train Loss: 0.057232781284177914 \t|| Test Loss: 0.13374904725431083\n",
      "Epoch: 7335 \t|| Train Loss: 0.05719788128596158 \t|| Test Loss: 0.13363884725700387\n",
      "Epoch: 7336 \t|| Train Loss: 0.057164138748015104 \t|| Test Loss: 0.1335973372593136\n",
      "Epoch: 7337 \t|| Train Loss: 0.05712936128930686 \t|| Test Loss: 0.13348713726200662\n",
      "Epoch: 7338 \t|| Train Loss: 0.05709523775094379 \t|| Test Loss: 0.1334456272643163\n",
      "Epoch: 7339 \t|| Train Loss: 0.05706084129265213 \t|| Test Loss: 0.13333542726700934\n",
      "Epoch: 7340 \t|| Train Loss: 0.057026336753872475 \t|| Test Loss: 0.1332939172693191\n",
      "Epoch: 7341 \t|| Train Loss: 0.056992321295997427 \t|| Test Loss: 0.13318371727201211\n",
      "Epoch: 7342 \t|| Train Loss: 0.05695743575680114 \t|| Test Loss: 0.13314220727432186\n",
      "Epoch: 7343 \t|| Train Loss: 0.05692380129934268 \t|| Test Loss: 0.13303200727701486\n",
      "Epoch: 7344 \t|| Train Loss: 0.05688890130112635 \t|| Test Loss: 0.13292180727970787\n",
      "Epoch: 7345 \t|| Train Loss: 0.05685491476129144 \t|| Test Loss: 0.1328802972820176\n",
      "Epoch: 7346 \t|| Train Loss: 0.056820381304471625 \t|| Test Loss: 0.13277009728471062\n",
      "Epoch: 7347 \t|| Train Loss: 0.05678601376422012 \t|| Test Loss: 0.13272858728702033\n",
      "Epoch: 7348 \t|| Train Loss: 0.05675186130781691 \t|| Test Loss: 0.13261838728971337\n",
      "Epoch: 7349 \t|| Train Loss: 0.056717112767148824 \t|| Test Loss: 0.1325768772920231\n",
      "Epoch: 7350 \t|| Train Loss: 0.05668334131116217 \t|| Test Loss: 0.1324666772947161\n",
      "Epoch: 7351 \t|| Train Loss: 0.056648441312945844 \t|| Test Loss: 0.13235647729740913\n",
      "Epoch: 7352 \t|| Train Loss: 0.05661459177163909 \t|| Test Loss: 0.13231496729971887\n",
      "Epoch: 7353 \t|| Train Loss: 0.05657992131629112 \t|| Test Loss: 0.13220476730241187\n",
      "Epoch: 7354 \t|| Train Loss: 0.056545690774567794 \t|| Test Loss: 0.13216325730472162\n",
      "Epoch: 7355 \t|| Train Loss: 0.05651140131963639 \t|| Test Loss: 0.13205305730741462\n",
      "Epoch: 7356 \t|| Train Loss: 0.05647678977749646 \t|| Test Loss: 0.13201154730972434\n",
      "Epoch: 7357 \t|| Train Loss: 0.05644288132298166 \t|| Test Loss: 0.13190134731241737\n",
      "Epoch: 7358 \t|| Train Loss: 0.056407981324765336 \t|| Test Loss: 0.13179114731511032\n",
      "Epoch: 7359 \t|| Train Loss: 0.05637426878198676 \t|| Test Loss: 0.1317496373174201\n",
      "Epoch: 7360 \t|| Train Loss: 0.056339461328110595 \t|| Test Loss: 0.1316394373201131\n",
      "Epoch: 7361 \t|| Train Loss: 0.05630536778491544 \t|| Test Loss: 0.13159792732242287\n",
      "Epoch: 7362 \t|| Train Loss: 0.05627094133145587 \t|| Test Loss: 0.13148772732511588\n",
      "Epoch: 7363 \t|| Train Loss: 0.05623646678784411 \t|| Test Loss: 0.13144621732742562\n",
      "Epoch: 7364 \t|| Train Loss: 0.056202421334801154 \t|| Test Loss: 0.1313360173301186\n",
      "Epoch: 7365 \t|| Train Loss: 0.0561675657907728 \t|| Test Loss: 0.13129450733242837\n",
      "Epoch: 7366 \t|| Train Loss: 0.05613390133814643 \t|| Test Loss: 0.13118430733512135\n",
      "Epoch: 7367 \t|| Train Loss: 0.0560990013399301 \t|| Test Loss: 0.13107410733781438\n",
      "Epoch: 7368 \t|| Train Loss: 0.056065044795263086 \t|| Test Loss: 0.13103259734012412\n",
      "Epoch: 7369 \t|| Train Loss: 0.05603048134327537 \t|| Test Loss: 0.1309223973428171\n",
      "Epoch: 7370 \t|| Train Loss: 0.055996143798191775 \t|| Test Loss: 0.13088088734512687\n",
      "Epoch: 7371 \t|| Train Loss: 0.055961961346620646 \t|| Test Loss: 0.13077068734781988\n",
      "Epoch: 7372 \t|| Train Loss: 0.05592724280112046 \t|| Test Loss: 0.13072917735012962\n",
      "Epoch: 7373 \t|| Train Loss: 0.05589344134996591 \t|| Test Loss: 0.1306189773528226\n",
      "Epoch: 7374 \t|| Train Loss: 0.05585854135174959 \t|| Test Loss: 0.1305087773555156\n",
      "Epoch: 7375 \t|| Train Loss: 0.055824721805610746 \t|| Test Loss: 0.13046726735782535\n",
      "Epoch: 7376 \t|| Train Loss: 0.05579002135509485 \t|| Test Loss: 0.13035706736051839\n",
      "Epoch: 7377 \t|| Train Loss: 0.05575582080853943 \t|| Test Loss: 0.13031555736282813\n",
      "Epoch: 7378 \t|| Train Loss: 0.05572150135844014 \t|| Test Loss: 0.1302053573655211\n",
      "Epoch: 7379 \t|| Train Loss: 0.0556869198114681 \t|| Test Loss: 0.13016384736783085\n",
      "Epoch: 7380 \t|| Train Loss: 0.055652981361785404 \t|| Test Loss: 0.13005364737052388\n",
      "Epoch: 7381 \t|| Train Loss: 0.05561808136356907 \t|| Test Loss: 0.1299434473732169\n",
      "Epoch: 7382 \t|| Train Loss: 0.055584398815958405 \t|| Test Loss: 0.12990193737552663\n",
      "Epoch: 7383 \t|| Train Loss: 0.05554956136691436 \t|| Test Loss: 0.12979173737821964\n",
      "Epoch: 7384 \t|| Train Loss: 0.05551549781888708 \t|| Test Loss: 0.12975022738052938\n",
      "Epoch: 7385 \t|| Train Loss: 0.05548104137025963 \t|| Test Loss: 0.12964002738322236\n",
      "Epoch: 7386 \t|| Train Loss: 0.05544659682181576 \t|| Test Loss: 0.1295985173855321\n",
      "Epoch: 7387 \t|| Train Loss: 0.0554125213736049 \t|| Test Loss: 0.12948831738822514\n",
      "Epoch: 7388 \t|| Train Loss: 0.05537769582474446 \t|| Test Loss: 0.12944680739053488\n",
      "Epoch: 7389 \t|| Train Loss: 0.05534400137695017 \t|| Test Loss: 0.1293366073932279\n",
      "Epoch: 7390 \t|| Train Loss: 0.055309101378733835 \t|| Test Loss: 0.1292264073959209\n",
      "Epoch: 7391 \t|| Train Loss: 0.05527517482923473 \t|| Test Loss: 0.12918489739823058\n",
      "Epoch: 7392 \t|| Train Loss: 0.05524058138207912 \t|| Test Loss: 0.12907469740092362\n",
      "Epoch: 7393 \t|| Train Loss: 0.05520627383216342 \t|| Test Loss: 0.12903318740323336\n",
      "Epoch: 7394 \t|| Train Loss: 0.055172061385424374 \t|| Test Loss: 0.12892298740592636\n",
      "Epoch: 7395 \t|| Train Loss: 0.055137372835092104 \t|| Test Loss: 0.1288814774082361\n",
      "Epoch: 7396 \t|| Train Loss: 0.055103541388769675 \t|| Test Loss: 0.12877127741092909\n",
      "Epoch: 7397 \t|| Train Loss: 0.05506864139055334 \t|| Test Loss: 0.1286610774136221\n",
      "Epoch: 7398 \t|| Train Loss: 0.0550348518395824 \t|| Test Loss: 0.12861956741593183\n",
      "Epoch: 7399 \t|| Train Loss: 0.055000121393898614 \t|| Test Loss: 0.12850936741862484\n",
      "Epoch: 7400 \t|| Train Loss: 0.05496595084251108 \t|| Test Loss: 0.1284678574209346\n",
      "Epoch: 7401 \t|| Train Loss: 0.05493160139724389 \t|| Test Loss: 0.12835765742362765\n",
      "Epoch: 7402 \t|| Train Loss: 0.05489704984543975 \t|| Test Loss: 0.12831614742593736\n",
      "Epoch: 7403 \t|| Train Loss: 0.05486308140058915 \t|| Test Loss: 0.1282059474286304\n",
      "Epoch: 7404 \t|| Train Loss: 0.05482818140237282 \t|| Test Loss: 0.12809574743132335\n",
      "Epoch: 7405 \t|| Train Loss: 0.05479452884993004 \t|| Test Loss: 0.12805423743363314\n",
      "Epoch: 7406 \t|| Train Loss: 0.054759661405718085 \t|| Test Loss: 0.1279440374363261\n",
      "Epoch: 7407 \t|| Train Loss: 0.05472562785285871 \t|| Test Loss: 0.12790252743863587\n",
      "Epoch: 7408 \t|| Train Loss: 0.05469114140906337 \t|| Test Loss: 0.12779232744132885\n",
      "Epoch: 7409 \t|| Train Loss: 0.054656726855787395 \t|| Test Loss: 0.12775081744363864\n",
      "Epoch: 7410 \t|| Train Loss: 0.054622621412408645 \t|| Test Loss: 0.12764061744633162\n",
      "Epoch: 7411 \t|| Train Loss: 0.05458782585871609 \t|| Test Loss: 0.12759910744864134\n",
      "Epoch: 7412 \t|| Train Loss: 0.05455410141575392 \t|| Test Loss: 0.1274889074513344\n",
      "Epoch: 7413 \t|| Train Loss: 0.05451920141753759 \t|| Test Loss: 0.12737870745402738\n",
      "Epoch: 7414 \t|| Train Loss: 0.05448530486320637 \t|| Test Loss: 0.12733719745633715\n",
      "Epoch: 7415 \t|| Train Loss: 0.054450681420882864 \t|| Test Loss: 0.12722699745903016\n",
      "Epoch: 7416 \t|| Train Loss: 0.054416403866135055 \t|| Test Loss: 0.1271854874613399\n",
      "Epoch: 7417 \t|| Train Loss: 0.05438216142422814 \t|| Test Loss: 0.12707528746403285\n",
      "Epoch: 7418 \t|| Train Loss: 0.05434750286906374 \t|| Test Loss: 0.12703377746634262\n",
      "Epoch: 7419 \t|| Train Loss: 0.0543136414275734 \t|| Test Loss: 0.1269235774690356\n",
      "Epoch: 7420 \t|| Train Loss: 0.05427874142935709 \t|| Test Loss: 0.12681337747172866\n",
      "Epoch: 7421 \t|| Train Loss: 0.05424498187355402 \t|| Test Loss: 0.12677186747403835\n",
      "Epoch: 7422 \t|| Train Loss: 0.054210221432702356 \t|| Test Loss: 0.12666166747673135\n",
      "Epoch: 7423 \t|| Train Loss: 0.054176080876482714 \t|| Test Loss: 0.12662015747904112\n",
      "Epoch: 7424 \t|| Train Loss: 0.054141701436047615 \t|| Test Loss: 0.12650995748173413\n",
      "Epoch: 7425 \t|| Train Loss: 0.054107179879411396 \t|| Test Loss: 0.12646844748404387\n",
      "Epoch: 7426 \t|| Train Loss: 0.054073181439392895 \t|| Test Loss: 0.12635824748673688\n",
      "Epoch: 7427 \t|| Train Loss: 0.05403828144117656 \t|| Test Loss: 0.1262480474894299\n",
      "Epoch: 7428 \t|| Train Loss: 0.05400465888390169 \t|| Test Loss: 0.12620653749173963\n",
      "Epoch: 7429 \t|| Train Loss: 0.053969761444521855 \t|| Test Loss: 0.12609633749443264\n",
      "Epoch: 7430 \t|| Train Loss: 0.05393575788683038 \t|| Test Loss: 0.1260548274967424\n",
      "Epoch: 7431 \t|| Train Loss: 0.05390124144786711 \t|| Test Loss: 0.12594462749943536\n",
      "Epoch: 7432 \t|| Train Loss: 0.05386685688975905 \t|| Test Loss: 0.1259031175017451\n",
      "Epoch: 7433 \t|| Train Loss: 0.05383272145121239 \t|| Test Loss: 0.12579291750443805\n",
      "Epoch: 7434 \t|| Train Loss: 0.053797955892687724 \t|| Test Loss: 0.1257514075067479\n",
      "Epoch: 7435 \t|| Train Loss: 0.05376420145455766 \t|| Test Loss: 0.1256412075094409\n",
      "Epoch: 7436 \t|| Train Loss: 0.053729301456341326 \t|| Test Loss: 0.12553100751213386\n",
      "Epoch: 7437 \t|| Train Loss: 0.05369543489717802 \t|| Test Loss: 0.12548949751444363\n",
      "Epoch: 7438 \t|| Train Loss: 0.05366078145968661 \t|| Test Loss: 0.12537929751713664\n",
      "Epoch: 7439 \t|| Train Loss: 0.05362653390010672 \t|| Test Loss: 0.1253377875194464\n",
      "Epoch: 7440 \t|| Train Loss: 0.053592261463031886 \t|| Test Loss: 0.12522758752213942\n",
      "Epoch: 7441 \t|| Train Loss: 0.05355763290303539 \t|| Test Loss: 0.12518607752444913\n",
      "Epoch: 7442 \t|| Train Loss: 0.05352374146637717 \t|| Test Loss: 0.1250758775271421\n",
      "Epoch: 7443 \t|| Train Loss: 0.05348884146816083 \t|| Test Loss: 0.12496567752983512\n",
      "Epoch: 7444 \t|| Train Loss: 0.05345511190752568 \t|| Test Loss: 0.12492416753214489\n",
      "Epoch: 7445 \t|| Train Loss: 0.05342032147150609 \t|| Test Loss: 0.1248139675348379\n",
      "Epoch: 7446 \t|| Train Loss: 0.05338621091045437 \t|| Test Loss: 0.12477245753714764\n",
      "Epoch: 7447 \t|| Train Loss: 0.05335180147485138 \t|| Test Loss: 0.12466225753984066\n",
      "Epoch: 7448 \t|| Train Loss: 0.05331730991338305 \t|| Test Loss: 0.12462074754215036\n",
      "Epoch: 7449 \t|| Train Loss: 0.05328328147819665 \t|| Test Loss: 0.12451054754484336\n",
      "Epoch: 7450 \t|| Train Loss: 0.053248408916311704 \t|| Test Loss: 0.12446903754715315\n",
      "Epoch: 7451 \t|| Train Loss: 0.05321476148154193 \t|| Test Loss: 0.12435883754984615\n",
      "Epoch: 7452 \t|| Train Loss: 0.05317986148332558 \t|| Test Loss: 0.12424863755253912\n",
      "Epoch: 7453 \t|| Train Loss: 0.053145887920802014 \t|| Test Loss: 0.12420712755484886\n",
      "Epoch: 7454 \t|| Train Loss: 0.05311134148667087 \t|| Test Loss: 0.1240969275575419\n",
      "Epoch: 7455 \t|| Train Loss: 0.053076986923730696 \t|| Test Loss: 0.12405541755985164\n",
      "Epoch: 7456 \t|| Train Loss: 0.053042821490016136 \t|| Test Loss: 0.12394521756254462\n",
      "Epoch: 7457 \t|| Train Loss: 0.05300808592665938 \t|| Test Loss: 0.12390370756485436\n",
      "Epoch: 7458 \t|| Train Loss: 0.05297430149336142 \t|| Test Loss: 0.1237935075675474\n",
      "Epoch: 7459 \t|| Train Loss: 0.05293940149514509 \t|| Test Loss: 0.12368330757024038\n",
      "Epoch: 7460 \t|| Train Loss: 0.05290556493114966 \t|| Test Loss: 0.12364179757255012\n",
      "Epoch: 7461 \t|| Train Loss: 0.052870881498490355 \t|| Test Loss: 0.12353159757524315\n",
      "Epoch: 7462 \t|| Train Loss: 0.052836663934078355 \t|| Test Loss: 0.12349008757755289\n",
      "Epoch: 7463 \t|| Train Loss: 0.052802361501835635 \t|| Test Loss: 0.12337988758024591\n",
      "Epoch: 7464 \t|| Train Loss: 0.052767762937007044 \t|| Test Loss: 0.12333837758255564\n",
      "Epoch: 7465 \t|| Train Loss: 0.0527338415051809 \t|| Test Loss: 0.12322817758524862\n",
      "Epoch: 7466 \t|| Train Loss: 0.05269894150696457 \t|| Test Loss: 0.12311797758794166\n",
      "Epoch: 7467 \t|| Train Loss: 0.05266524194149732 \t|| Test Loss: 0.12307646759025137\n",
      "Epoch: 7468 \t|| Train Loss: 0.052630421510309854 \t|| Test Loss: 0.12296626759294435\n",
      "Epoch: 7469 \t|| Train Loss: 0.052596340944426 \t|| Test Loss: 0.12292475759525412\n",
      "Epoch: 7470 \t|| Train Loss: 0.05256190151365512 \t|| Test Loss: 0.12281455759794713\n",
      "Epoch: 7471 \t|| Train Loss: 0.05252743994735469 \t|| Test Loss: 0.12277304760025691\n",
      "Epoch: 7472 \t|| Train Loss: 0.05249338151700039 \t|| Test Loss: 0.12266284760294988\n",
      "Epoch: 7473 \t|| Train Loss: 0.05245853895028337 \t|| Test Loss: 0.12262133760525964\n",
      "Epoch: 7474 \t|| Train Loss: 0.052424861520345666 \t|| Test Loss: 0.12251113760795262\n",
      "Epoch: 7475 \t|| Train Loss: 0.052389961522129345 \t|| Test Loss: 0.12240093761064563\n",
      "Epoch: 7476 \t|| Train Loss: 0.05235601795477365 \t|| Test Loss: 0.1223594276129554\n",
      "Epoch: 7477 \t|| Train Loss: 0.05232144152547462 \t|| Test Loss: 0.12224922761564841\n",
      "Epoch: 7478 \t|| Train Loss: 0.05228711695770234 \t|| Test Loss: 0.12220771761795816\n",
      "Epoch: 7479 \t|| Train Loss: 0.052252921528819885 \t|| Test Loss: 0.12209751762065113\n",
      "Epoch: 7480 \t|| Train Loss: 0.05221821596063102 \t|| Test Loss: 0.12205600762296087\n",
      "Epoch: 7481 \t|| Train Loss: 0.05218440153216516 \t|| Test Loss: 0.12194580762565388\n",
      "Epoch: 7482 \t|| Train Loss: 0.05214950153394882 \t|| Test Loss: 0.12183560762834689\n",
      "Epoch: 7483 \t|| Train Loss: 0.05211569496512129 \t|| Test Loss: 0.12179409763065663\n",
      "Epoch: 7484 \t|| Train Loss: 0.0520809815372941 \t|| Test Loss: 0.12168389763334964\n",
      "Epoch: 7485 \t|| Train Loss: 0.052046793968049995 \t|| Test Loss: 0.12164238763565938\n",
      "Epoch: 7486 \t|| Train Loss: 0.05201246154063936 \t|| Test Loss: 0.12153218763835238\n",
      "Epoch: 7487 \t|| Train Loss: 0.05197789297097868 \t|| Test Loss: 0.12149067764066215\n",
      "Epoch: 7488 \t|| Train Loss: 0.05194394154398465 \t|| Test Loss: 0.12138047764335513\n",
      "Epoch: 7489 \t|| Train Loss: 0.05190904154576832 \t|| Test Loss: 0.12127027764604814\n",
      "Epoch: 7490 \t|| Train Loss: 0.051875371975468966 \t|| Test Loss: 0.12122876764835791\n",
      "Epoch: 7491 \t|| Train Loss: 0.051840521549113595 \t|| Test Loss: 0.12111856765105089\n",
      "Epoch: 7492 \t|| Train Loss: 0.051806470978397655 \t|| Test Loss: 0.12107705765336063\n",
      "Epoch: 7493 \t|| Train Loss: 0.05177200155245887 \t|| Test Loss: 0.12096685765605364\n",
      "Epoch: 7494 \t|| Train Loss: 0.05173756998132634 \t|| Test Loss: 0.12092534765836338\n",
      "Epoch: 7495 \t|| Train Loss: 0.05170348155580414 \t|| Test Loss: 0.12081514766105639\n",
      "Epoch: 7496 \t|| Train Loss: 0.05166866898425502 \t|| Test Loss: 0.12077363766336613\n",
      "Epoch: 7497 \t|| Train Loss: 0.051634961559149414 \t|| Test Loss: 0.12066343766605914\n",
      "Epoch: 7498 \t|| Train Loss: 0.05160006156093309 \t|| Test Loss: 0.12055323766875219\n",
      "Epoch: 7499 \t|| Train Loss: 0.0515661479887453 \t|| Test Loss: 0.12051172767106189\n",
      "Epoch: 7500 \t|| Train Loss: 0.05153154156427836 \t|| Test Loss: 0.1204015276737549\n",
      "Epoch: 7501 \t|| Train Loss: 0.05149724699167398 \t|| Test Loss: 0.12036001767606463\n",
      "Epoch: 7502 \t|| Train Loss: 0.051463021567623626 \t|| Test Loss: 0.12024981767875764\n",
      "Epoch: 7503 \t|| Train Loss: 0.051428345994602664 \t|| Test Loss: 0.12020830768106738\n",
      "Epoch: 7504 \t|| Train Loss: 0.051394501570968906 \t|| Test Loss: 0.12009810768376039\n",
      "Epoch: 7505 \t|| Train Loss: 0.05135960157275258 \t|| Test Loss: 0.1199879076864534\n",
      "Epoch: 7506 \t|| Train Loss: 0.05132582499909296 \t|| Test Loss: 0.11994639768876314\n",
      "Epoch: 7507 \t|| Train Loss: 0.05129108157609785 \t|| Test Loss: 0.11983619769145612\n",
      "Epoch: 7508 \t|| Train Loss: 0.05125692400202164 \t|| Test Loss: 0.11979468769376589\n",
      "Epoch: 7509 \t|| Train Loss: 0.05122256157944314 \t|| Test Loss: 0.1196844876964589\n",
      "Epoch: 7510 \t|| Train Loss: 0.051188023004950324 \t|| Test Loss: 0.11964297769876864\n",
      "Epoch: 7511 \t|| Train Loss: 0.05115404158278839 \t|| Test Loss: 0.11953277770146165\n",
      "Epoch: 7512 \t|| Train Loss: 0.05111914158457206 \t|| Test Loss: 0.11942257770415465\n",
      "Epoch: 7513 \t|| Train Loss: 0.051085502009440606 \t|| Test Loss: 0.11938106770646444\n",
      "Epoch: 7514 \t|| Train Loss: 0.05105062158791733 \t|| Test Loss: 0.1192708677091574\n",
      "Epoch: 7515 \t|| Train Loss: 0.0510166010123693 \t|| Test Loss: 0.11922935771146714\n",
      "Epoch: 7516 \t|| Train Loss: 0.05098210159126261 \t|| Test Loss: 0.11911915771416015\n",
      "Epoch: 7517 \t|| Train Loss: 0.05094770001529797 \t|| Test Loss: 0.11907764771646989\n",
      "Epoch: 7518 \t|| Train Loss: 0.05091358159460788 \t|| Test Loss: 0.1189674477191629\n",
      "Epoch: 7519 \t|| Train Loss: 0.05087879901822665 \t|| Test Loss: 0.11892593772147267\n",
      "Epoch: 7520 \t|| Train Loss: 0.050845061597953156 \t|| Test Loss: 0.11881573772416565\n",
      "Epoch: 7521 \t|| Train Loss: 0.05081016159973682 \t|| Test Loss: 0.11870553772685866\n",
      "Epoch: 7522 \t|| Train Loss: 0.05077627802271696 \t|| Test Loss: 0.1186640277291684\n",
      "Epoch: 7523 \t|| Train Loss: 0.0507416416030821 \t|| Test Loss: 0.1185538277318614\n",
      "Epoch: 7524 \t|| Train Loss: 0.05070737702564563 \t|| Test Loss: 0.11851231773417115\n",
      "Epoch: 7525 \t|| Train Loss: 0.05067312160642738 \t|| Test Loss: 0.11840211773686415\n",
      "Epoch: 7526 \t|| Train Loss: 0.05063847602857432 \t|| Test Loss: 0.11836060773917392\n",
      "Epoch: 7527 \t|| Train Loss: 0.05060460160977265 \t|| Test Loss: 0.1182504077418669\n",
      "Epoch: 7528 \t|| Train Loss: 0.050569701611556314 \t|| Test Loss: 0.11814020774455991\n",
      "Epoch: 7529 \t|| Train Loss: 0.05053595503306461 \t|| Test Loss: 0.11809869774686965\n",
      "Epoch: 7530 \t|| Train Loss: 0.0505011816149016 \t|| Test Loss: 0.11798849774956266\n",
      "Epoch: 7531 \t|| Train Loss: 0.05046705403599329 \t|| Test Loss: 0.1179469877518724\n",
      "Epoch: 7532 \t|| Train Loss: 0.05043266161824687 \t|| Test Loss: 0.11783678775456541\n",
      "Epoch: 7533 \t|| Train Loss: 0.050398153038921964 \t|| Test Loss: 0.11779527775687515\n",
      "Epoch: 7534 \t|| Train Loss: 0.05036414162159214 \t|| Test Loss: 0.11768507775956816\n",
      "Epoch: 7535 \t|| Train Loss: 0.05032925204185064 \t|| Test Loss: 0.1176435677618779\n",
      "Epoch: 7536 \t|| Train Loss: 0.05029562162493741 \t|| Test Loss: 0.1175333677645709\n",
      "Epoch: 7537 \t|| Train Loss: 0.050260721626721086 \t|| Test Loss: 0.11742316776726391\n",
      "Epoch: 7538 \t|| Train Loss: 0.05022673104634094 \t|| Test Loss: 0.11738165776957366\n",
      "Epoch: 7539 \t|| Train Loss: 0.050192201630066366 \t|| Test Loss: 0.11727145777226666\n",
      "Epoch: 7540 \t|| Train Loss: 0.050157830049269624 \t|| Test Loss: 0.1172299477745764\n",
      "Epoch: 7541 \t|| Train Loss: 0.05012368163341163 \t|| Test Loss: 0.11711974777726938\n",
      "Epoch: 7542 \t|| Train Loss: 0.050088929052198305 \t|| Test Loss: 0.11707823777957915\n",
      "Epoch: 7543 \t|| Train Loss: 0.050055161636756905 \t|| Test Loss: 0.11696803778227209\n",
      "Epoch: 7544 \t|| Train Loss: 0.05002026163854055 \t|| Test Loss: 0.11685783778496517\n",
      "Epoch: 7545 \t|| Train Loss: 0.049986408056688594 \t|| Test Loss: 0.11681632778727491\n",
      "Epoch: 7546 \t|| Train Loss: 0.04995174164188585 \t|| Test Loss: 0.11670612778996792\n",
      "Epoch: 7547 \t|| Train Loss: 0.04991750705961727 \t|| Test Loss: 0.11666461779227766\n",
      "Epoch: 7548 \t|| Train Loss: 0.04988322164523114 \t|| Test Loss: 0.11655441779497067\n",
      "Epoch: 7549 \t|| Train Loss: 0.04984860606254597 \t|| Test Loss: 0.11651290779728044\n",
      "Epoch: 7550 \t|| Train Loss: 0.04981470164857639 \t|| Test Loss: 0.11640270779997344\n",
      "Epoch: 7551 \t|| Train Loss: 0.04977980165036007 \t|| Test Loss: 0.1162925078026664\n",
      "Epoch: 7552 \t|| Train Loss: 0.04974608506703625 \t|| Test Loss: 0.11625099780497616\n",
      "Epoch: 7553 \t|| Train Loss: 0.049711281653705336 \t|| Test Loss: 0.11614079780766917\n",
      "Epoch: 7554 \t|| Train Loss: 0.04967718406996493 \t|| Test Loss: 0.11609928780997891\n",
      "Epoch: 7555 \t|| Train Loss: 0.04964276165705061 \t|| Test Loss: 0.11598908781267192\n",
      "Epoch: 7556 \t|| Train Loss: 0.04960828307289361 \t|| Test Loss: 0.11594757781498166\n",
      "Epoch: 7557 \t|| Train Loss: 0.04957424166039589 \t|| Test Loss: 0.1158373778176747\n",
      "Epoch: 7558 \t|| Train Loss: 0.0495393820758223 \t|| Test Loss: 0.11579586781998438\n",
      "Epoch: 7559 \t|| Train Loss: 0.049505721663741176 \t|| Test Loss: 0.11568566782267742\n",
      "Epoch: 7560 \t|| Train Loss: 0.04947082166552482 \t|| Test Loss: 0.11557546782537043\n",
      "Epoch: 7561 \t|| Train Loss: 0.049436861080312575 \t|| Test Loss: 0.11553395782768017\n",
      "Epoch: 7562 \t|| Train Loss: 0.0494023016688701 \t|| Test Loss: 0.11542375783037317\n",
      "Epoch: 7563 \t|| Train Loss: 0.04936796008324126 \t|| Test Loss: 0.11538224783268292\n",
      "Epoch: 7564 \t|| Train Loss: 0.049333781672215374 \t|| Test Loss: 0.11527204783537592\n",
      "Epoch: 7565 \t|| Train Loss: 0.049299059086169945 \t|| Test Loss: 0.11523053783768566\n",
      "Epoch: 7566 \t|| Train Loss: 0.049265261675560654 \t|| Test Loss: 0.11512033784037864\n",
      "Epoch: 7567 \t|| Train Loss: 0.04923036167734432 \t|| Test Loss: 0.11501013784307168\n",
      "Epoch: 7568 \t|| Train Loss: 0.049196538090660234 \t|| Test Loss: 0.11496862784538142\n",
      "Epoch: 7569 \t|| Train Loss: 0.0491618416806896 \t|| Test Loss: 0.11485842784807443\n",
      "Epoch: 7570 \t|| Train Loss: 0.04912763709358893 \t|| Test Loss: 0.11481691785038414\n",
      "Epoch: 7571 \t|| Train Loss: 0.049093321684034866 \t|| Test Loss: 0.11470671785307718\n",
      "Epoch: 7572 \t|| Train Loss: 0.0490587360965176 \t|| Test Loss: 0.11466520785538692\n",
      "Epoch: 7573 \t|| Train Loss: 0.04902480168738014 \t|| Test Loss: 0.11455500785807993\n",
      "Epoch: 7574 \t|| Train Loss: 0.04898990168916382 \t|| Test Loss: 0.11444480786077293\n",
      "Epoch: 7575 \t|| Train Loss: 0.04895621510100788 \t|| Test Loss: 0.11440329786308268\n",
      "Epoch: 7576 \t|| Train Loss: 0.04892138169250908 \t|| Test Loss: 0.11429309786577568\n",
      "Epoch: 7577 \t|| Train Loss: 0.048887314103936576 \t|| Test Loss: 0.11425158786808542\n",
      "Epoch: 7578 \t|| Train Loss: 0.048852861695854365 \t|| Test Loss: 0.11414138787077839\n",
      "Epoch: 7579 \t|| Train Loss: 0.04881841310686526 \t|| Test Loss: 0.11409987787308817\n",
      "Epoch: 7580 \t|| Train Loss: 0.04878434169919964 \t|| Test Loss: 0.11398967787578115\n",
      "Epoch: 7581 \t|| Train Loss: 0.04874951210979395 \t|| Test Loss: 0.11394816787809096\n",
      "Epoch: 7582 \t|| Train Loss: 0.048715821702544904 \t|| Test Loss: 0.11383796788078393\n",
      "Epoch: 7583 \t|| Train Loss: 0.048680921704328584 \t|| Test Loss: 0.11372776788347691\n",
      "Epoch: 7584 \t|| Train Loss: 0.04864699111428422 \t|| Test Loss: 0.11368625788578668\n",
      "Epoch: 7585 \t|| Train Loss: 0.04861240170767385 \t|| Test Loss: 0.11357605788847969\n",
      "Epoch: 7586 \t|| Train Loss: 0.0485780901172129 \t|| Test Loss: 0.11353454789078943\n",
      "Epoch: 7587 \t|| Train Loss: 0.04854388171101913 \t|| Test Loss: 0.11342434789348244\n",
      "Epoch: 7588 \t|| Train Loss: 0.04850918912014159 \t|| Test Loss: 0.1133828378957922\n",
      "Epoch: 7589 \t|| Train Loss: 0.048475361714364396 \t|| Test Loss: 0.11327263789848518\n",
      "Epoch: 7590 \t|| Train Loss: 0.048440461716148075 \t|| Test Loss: 0.11316243790117819\n",
      "Epoch: 7591 \t|| Train Loss: 0.04840666812463189 \t|| Test Loss: 0.11312092790348793\n",
      "Epoch: 7592 \t|| Train Loss: 0.04837194171949335 \t|| Test Loss: 0.11301072790618094\n",
      "Epoch: 7593 \t|| Train Loss: 0.04833776712756056 \t|| Test Loss: 0.11296921790849065\n",
      "Epoch: 7594 \t|| Train Loss: 0.048303421722838615 \t|| Test Loss: 0.11285901791118366\n",
      "Epoch: 7595 \t|| Train Loss: 0.04826886613048925 \t|| Test Loss: 0.1128175079134934\n",
      "Epoch: 7596 \t|| Train Loss: 0.048234901726183894 \t|| Test Loss: 0.11270730791618644\n",
      "Epoch: 7597 \t|| Train Loss: 0.04820000172796756 \t|| Test Loss: 0.11259710791887942\n",
      "Epoch: 7598 \t|| Train Loss: 0.04816634513497953 \t|| Test Loss: 0.11255559792118916\n",
      "Epoch: 7599 \t|| Train Loss: 0.048131481731312833 \t|| Test Loss: 0.1124453979238822\n",
      "Epoch: 7600 \t|| Train Loss: 0.04809744413790822 \t|| Test Loss: 0.11240388792619194\n",
      "Epoch: 7601 \t|| Train Loss: 0.04806296173465811 \t|| Test Loss: 0.11229368792888494\n",
      "Epoch: 7602 \t|| Train Loss: 0.0480285431408369 \t|| Test Loss: 0.11225217793119466\n",
      "Epoch: 7603 \t|| Train Loss: 0.04799444173800339 \t|| Test Loss: 0.11214197793388767\n",
      "Epoch: 7604 \t|| Train Loss: 0.04795964214376559 \t|| Test Loss: 0.11210046793619743\n",
      "Epoch: 7605 \t|| Train Loss: 0.04792592174134865 \t|| Test Loss: 0.11199026793889041\n",
      "Epoch: 7606 \t|| Train Loss: 0.047891021743132325 \t|| Test Loss: 0.11188006794158345\n",
      "Epoch: 7607 \t|| Train Loss: 0.04785712114825587 \t|| Test Loss: 0.11183855794389319\n",
      "Epoch: 7608 \t|| Train Loss: 0.047822501746477605 \t|| Test Loss: 0.11172835794658623\n",
      "Epoch: 7609 \t|| Train Loss: 0.04778822015118456 \t|| Test Loss: 0.11168684794889594\n",
      "Epoch: 7610 \t|| Train Loss: 0.047753981749822864 \t|| Test Loss: 0.11157664795158895\n",
      "Epoch: 7611 \t|| Train Loss: 0.047719319154113246 \t|| Test Loss: 0.11153513795389866\n",
      "Epoch: 7612 \t|| Train Loss: 0.04768546175316814 \t|| Test Loss: 0.11142493795659167\n",
      "Epoch: 7613 \t|| Train Loss: 0.047650561754951824 \t|| Test Loss: 0.11131473795928468\n",
      "Epoch: 7614 \t|| Train Loss: 0.047616798158603535 \t|| Test Loss: 0.11127322796159442\n",
      "Epoch: 7615 \t|| Train Loss: 0.0475820417582971 \t|| Test Loss: 0.11116302796428743\n",
      "Epoch: 7616 \t|| Train Loss: 0.04754789716153221 \t|| Test Loss: 0.11112151796659717\n",
      "Epoch: 7617 \t|| Train Loss: 0.04751352176164236 \t|| Test Loss: 0.11101131796929016\n",
      "Epoch: 7618 \t|| Train Loss: 0.047478996164460906 \t|| Test Loss: 0.11096980797159992\n",
      "Epoch: 7619 \t|| Train Loss: 0.04744500176498765 \t|| Test Loss: 0.11085960797429292\n",
      "Epoch: 7620 \t|| Train Loss: 0.04741010176677131 \t|| Test Loss: 0.11074940797698592\n",
      "Epoch: 7621 \t|| Train Loss: 0.0473764751689512 \t|| Test Loss: 0.11070789797929567\n",
      "Epoch: 7622 \t|| Train Loss: 0.047341581770116575 \t|| Test Loss: 0.11059769798198871\n",
      "Epoch: 7623 \t|| Train Loss: 0.04730757417187987 \t|| Test Loss: 0.11055618798429848\n",
      "Epoch: 7624 \t|| Train Loss: 0.04727306177346187 \t|| Test Loss: 0.11044598798699141\n",
      "Epoch: 7625 \t|| Train Loss: 0.047238673174808544 \t|| Test Loss: 0.1104044779893012\n",
      "Epoch: 7626 \t|| Train Loss: 0.04720454177680712 \t|| Test Loss: 0.11029427799199416\n",
      "Epoch: 7627 \t|| Train Loss: 0.047169772177737226 \t|| Test Loss: 0.11025276799430392\n",
      "Epoch: 7628 \t|| Train Loss: 0.0471360217801524 \t|| Test Loss: 0.11014256799699693\n",
      "Epoch: 7629 \t|| Train Loss: 0.04710112178193606 \t|| Test Loss: 0.11003236799968992\n",
      "Epoch: 7630 \t|| Train Loss: 0.04706725118222752 \t|| Test Loss: 0.1099908580019997\n",
      "Epoch: 7631 \t|| Train Loss: 0.04703260178528135 \t|| Test Loss: 0.10988065800469271\n",
      "Epoch: 7632 \t|| Train Loss: 0.0469983501851562 \t|| Test Loss: 0.10983914800700242\n",
      "Epoch: 7633 \t|| Train Loss: 0.04696408178862662 \t|| Test Loss: 0.10972894800969543\n",
      "Epoch: 7634 \t|| Train Loss: 0.04692944918808488 \t|| Test Loss: 0.1096874380120052\n",
      "Epoch: 7635 \t|| Train Loss: 0.0468955617919719 \t|| Test Loss: 0.10957723801469821\n",
      "Epoch: 7636 \t|| Train Loss: 0.04686066179375556 \t|| Test Loss: 0.10946703801739119\n",
      "Epoch: 7637 \t|| Train Loss: 0.04682692819257516 \t|| Test Loss: 0.10942552801970093\n",
      "Epoch: 7638 \t|| Train Loss: 0.046792141797100825 \t|| Test Loss: 0.10931532802239392\n",
      "Epoch: 7639 \t|| Train Loss: 0.04675802719550386 \t|| Test Loss: 0.10927381802470368\n",
      "Epoch: 7640 \t|| Train Loss: 0.046723621800446105 \t|| Test Loss: 0.10916361802739669\n",
      "Epoch: 7641 \t|| Train Loss: 0.04668912619843253 \t|| Test Loss: 0.10912210802970641\n",
      "Epoch: 7642 \t|| Train Loss: 0.046655101803791385 \t|| Test Loss: 0.10901190803239942\n",
      "Epoch: 7643 \t|| Train Loss: 0.04662022520136122 \t|| Test Loss: 0.1089703980347092\n",
      "Epoch: 7644 \t|| Train Loss: 0.046586581807136644 \t|| Test Loss: 0.10886019803740218\n",
      "Epoch: 7645 \t|| Train Loss: 0.046551681808920324 \t|| Test Loss: 0.10874999804009519\n",
      "Epoch: 7646 \t|| Train Loss: 0.04651770420585151 \t|| Test Loss: 0.10870848804240493\n",
      "Epoch: 7647 \t|| Train Loss: 0.0464831618122656 \t|| Test Loss: 0.10859828804509794\n",
      "Epoch: 7648 \t|| Train Loss: 0.0464488032087802 \t|| Test Loss: 0.10855677804740768\n",
      "Epoch: 7649 \t|| Train Loss: 0.04641464181561088 \t|| Test Loss: 0.10844657805010069\n",
      "Epoch: 7650 \t|| Train Loss: 0.04637990221170888 \t|| Test Loss: 0.10840506805241046\n",
      "Epoch: 7651 \t|| Train Loss: 0.04634612181895614 \t|| Test Loss: 0.10829486805510342\n",
      "Epoch: 7652 \t|| Train Loss: 0.046311221820739816 \t|| Test Loss: 0.10818466805779643\n",
      "Epoch: 7653 \t|| Train Loss: 0.04627738121619916 \t|| Test Loss: 0.10814315806010613\n",
      "Epoch: 7654 \t|| Train Loss: 0.04624270182408507 \t|| Test Loss: 0.1080329580627992\n",
      "Epoch: 7655 \t|| Train Loss: 0.046208480219127844 \t|| Test Loss: 0.10799144806510896\n",
      "Epoch: 7656 \t|| Train Loss: 0.04617418182743036 \t|| Test Loss: 0.10788124806780194\n",
      "Epoch: 7657 \t|| Train Loss: 0.046139579222056526 \t|| Test Loss: 0.10783973807011171\n",
      "Epoch: 7658 \t|| Train Loss: 0.04610566183077564 \t|| Test Loss: 0.10772953807280472\n",
      "Epoch: 7659 \t|| Train Loss: 0.04607076183255931 \t|| Test Loss: 0.10761933807549773\n",
      "Epoch: 7660 \t|| Train Loss: 0.046037058226546815 \t|| Test Loss: 0.10757782807780747\n",
      "Epoch: 7661 \t|| Train Loss: 0.04600224183590458 \t|| Test Loss: 0.10746762808050044\n",
      "Epoch: 7662 \t|| Train Loss: 0.04596815722947549 \t|| Test Loss: 0.10742611808281019\n",
      "Epoch: 7663 \t|| Train Loss: 0.04593372183924986 \t|| Test Loss: 0.1073159180855032\n",
      "Epoch: 7664 \t|| Train Loss: 0.045899256232404186 \t|| Test Loss: 0.10727440808781297\n",
      "Epoch: 7665 \t|| Train Loss: 0.04586520184259513 \t|| Test Loss: 0.10716420809050595\n",
      "Epoch: 7666 \t|| Train Loss: 0.04583035523533287 \t|| Test Loss: 0.10712269809281567\n",
      "Epoch: 7667 \t|| Train Loss: 0.0457966818459404 \t|| Test Loss: 0.10701249809550872\n",
      "Epoch: 7668 \t|| Train Loss: 0.04576178184772407 \t|| Test Loss: 0.10690229809820169\n",
      "Epoch: 7669 \t|| Train Loss: 0.045727834239823156 \t|| Test Loss: 0.10686078810051143\n",
      "Epoch: 7670 \t|| Train Loss: 0.04569326185106934 \t|| Test Loss: 0.10675058810320448\n",
      "Epoch: 7671 \t|| Train Loss: 0.04565893324275183 \t|| Test Loss: 0.10670907810551422\n",
      "Epoch: 7672 \t|| Train Loss: 0.04562474185441462 \t|| Test Loss: 0.10659887810820719\n",
      "Epoch: 7673 \t|| Train Loss: 0.04559003224568052 \t|| Test Loss: 0.10655736811051694\n",
      "Epoch: 7674 \t|| Train Loss: 0.04555622185775989 \t|| Test Loss: 0.10644716811320998\n",
      "Epoch: 7675 \t|| Train Loss: 0.04552132185954356 \t|| Test Loss: 0.10633696811590294\n",
      "Epoch: 7676 \t|| Train Loss: 0.0454875112501708 \t|| Test Loss: 0.10629545811821269\n",
      "Epoch: 7677 \t|| Train Loss: 0.04545280186288883 \t|| Test Loss: 0.1061852581209057\n",
      "Epoch: 7678 \t|| Train Loss: 0.0454186102530995 \t|| Test Loss: 0.10614374812321545\n",
      "Epoch: 7679 \t|| Train Loss: 0.04538428186623412 \t|| Test Loss: 0.10603354812590844\n",
      "Epoch: 7680 \t|| Train Loss: 0.04534970925602817 \t|| Test Loss: 0.10599203812821818\n",
      "Epoch: 7681 \t|| Train Loss: 0.045315761869579384 \t|| Test Loss: 0.1058818381309112\n",
      "Epoch: 7682 \t|| Train Loss: 0.04528086187136306 \t|| Test Loss: 0.10577163813360421\n",
      "Epoch: 7683 \t|| Train Loss: 0.04524718826051846 \t|| Test Loss: 0.10573012813591398\n",
      "Epoch: 7684 \t|| Train Loss: 0.04521234187470834 \t|| Test Loss: 0.10561992813860695\n",
      "Epoch: 7685 \t|| Train Loss: 0.04517828726344714 \t|| Test Loss: 0.1055784181409167\n",
      "Epoch: 7686 \t|| Train Loss: 0.0451438218780536 \t|| Test Loss: 0.10546821814360971\n",
      "Epoch: 7687 \t|| Train Loss: 0.045109386266375825 \t|| Test Loss: 0.10542670814591945\n",
      "Epoch: 7688 \t|| Train Loss: 0.04507530188139888 \t|| Test Loss: 0.10531650814861242\n",
      "Epoch: 7689 \t|| Train Loss: 0.04504048526930451 \t|| Test Loss: 0.10527499815092219\n",
      "Epoch: 7690 \t|| Train Loss: 0.045006781884744156 \t|| Test Loss: 0.1051647981536152\n",
      "Epoch: 7691 \t|| Train Loss: 0.04497188188652782 \t|| Test Loss: 0.10505459815630822\n",
      "Epoch: 7692 \t|| Train Loss: 0.044937964273794796 \t|| Test Loss: 0.10501308815861798\n",
      "Epoch: 7693 \t|| Train Loss: 0.044903361889873095 \t|| Test Loss: 0.10490288816131095\n",
      "Epoch: 7694 \t|| Train Loss: 0.04486906327672348 \t|| Test Loss: 0.10486137816362069\n",
      "Epoch: 7695 \t|| Train Loss: 0.044834841893218375 \t|| Test Loss: 0.1047511781663137\n",
      "Epoch: 7696 \t|| Train Loss: 0.04480016227965216 \t|| Test Loss: 0.10470966816862345\n",
      "Epoch: 7697 \t|| Train Loss: 0.04476632189656364 \t|| Test Loss: 0.10459946817131646\n",
      "Epoch: 7698 \t|| Train Loss: 0.044731421898347314 \t|| Test Loss: 0.10448926817400946\n",
      "Epoch: 7699 \t|| Train Loss: 0.044697641284142456 \t|| Test Loss: 0.1044477581763192\n",
      "Epoch: 7700 \t|| Train Loss: 0.04466290190169259 \t|| Test Loss: 0.1043375581790122\n",
      "Epoch: 7701 \t|| Train Loss: 0.04462874028707113 \t|| Test Loss: 0.10429604818132195\n",
      "Epoch: 7702 \t|| Train Loss: 0.04459438190503786 \t|| Test Loss: 0.10418584818401495\n",
      "Epoch: 7703 \t|| Train Loss: 0.04455983928999981 \t|| Test Loss: 0.1041443381863247\n",
      "Epoch: 7704 \t|| Train Loss: 0.04452586190838313 \t|| Test Loss: 0.1040341381890177\n",
      "Epoch: 7705 \t|| Train Loss: 0.044490961910166806 \t|| Test Loss: 0.10392393819171072\n",
      "Epoch: 7706 \t|| Train Loss: 0.044457318294490095 \t|| Test Loss: 0.10388242819402047\n",
      "Epoch: 7707 \t|| Train Loss: 0.04442244191351207 \t|| Test Loss: 0.10377222819671346\n",
      "Epoch: 7708 \t|| Train Loss: 0.044388417297418784 \t|| Test Loss: 0.1037307181990232\n",
      "Epoch: 7709 \t|| Train Loss: 0.044353921916857345 \t|| Test Loss: 0.10362051820171622\n",
      "Epoch: 7710 \t|| Train Loss: 0.04431951630034747 \t|| Test Loss: 0.10357900820402596\n",
      "Epoch: 7711 \t|| Train Loss: 0.044285401920202624 \t|| Test Loss: 0.10346880820671896\n",
      "Epoch: 7712 \t|| Train Loss: 0.044250615303276154 \t|| Test Loss: 0.1034272982090287\n",
      "Epoch: 7713 \t|| Train Loss: 0.04421688192354789 \t|| Test Loss: 0.1033170982117217\n",
      "Epoch: 7714 \t|| Train Loss: 0.04418198192533157 \t|| Test Loss: 0.10320689821441471\n",
      "Epoch: 7715 \t|| Train Loss: 0.044148094307766436 \t|| Test Loss: 0.10316538821672445\n",
      "Epoch: 7716 \t|| Train Loss: 0.04411346192867684 \t|| Test Loss: 0.10305518821941746\n",
      "Epoch: 7717 \t|| Train Loss: 0.04407919331069511 \t|| Test Loss: 0.1030136782217272\n",
      "Epoch: 7718 \t|| Train Loss: 0.044044941932022116 \t|| Test Loss: 0.10290347822442025\n",
      "Epoch: 7719 \t|| Train Loss: 0.04401029231362379 \t|| Test Loss: 0.10286196822672997\n",
      "Epoch: 7720 \t|| Train Loss: 0.04397642193536739 \t|| Test Loss: 0.10275176822942296\n",
      "Epoch: 7721 \t|| Train Loss: 0.04394152193715105 \t|| Test Loss: 0.10264156823211598\n",
      "Epoch: 7722 \t|| Train Loss: 0.0439077713181141 \t|| Test Loss: 0.10260005823442571\n",
      "Epoch: 7723 \t|| Train Loss: 0.04387300194049633 \t|| Test Loss: 0.10248985823711872\n",
      "Epoch: 7724 \t|| Train Loss: 0.04383887032104279 \t|| Test Loss: 0.10244834823942846\n",
      "Epoch: 7725 \t|| Train Loss: 0.04380448194384161 \t|| Test Loss: 0.10233814824212147\n",
      "Epoch: 7726 \t|| Train Loss: 0.04376996932397147 \t|| Test Loss: 0.1022966382444312\n",
      "Epoch: 7727 \t|| Train Loss: 0.043735961947186874 \t|| Test Loss: 0.10218643824712419\n",
      "Epoch: 7728 \t|| Train Loss: 0.04370106832690014 \t|| Test Loss: 0.10214492824943396\n",
      "Epoch: 7729 \t|| Train Loss: 0.04366744195053216 \t|| Test Loss: 0.10203472825212696\n",
      "Epoch: 7730 \t|| Train Loss: 0.04363254195231583 \t|| Test Loss: 0.10192452825481994\n",
      "Epoch: 7731 \t|| Train Loss: 0.043598547331390444 \t|| Test Loss: 0.10188301825712971\n",
      "Epoch: 7732 \t|| Train Loss: 0.04356402195566109 \t|| Test Loss: 0.10177281825982272\n",
      "Epoch: 7733 \t|| Train Loss: 0.04352964633431912 \t|| Test Loss: 0.10173130826213253\n",
      "Epoch: 7734 \t|| Train Loss: 0.04349550195900637 \t|| Test Loss: 0.10162110826482547\n",
      "Epoch: 7735 \t|| Train Loss: 0.043460745337247794 \t|| Test Loss: 0.10157959826713522\n",
      "Epoch: 7736 \t|| Train Loss: 0.04342698196235163 \t|| Test Loss: 0.10146939826982822\n",
      "Epoch: 7737 \t|| Train Loss: 0.04339208196413531 \t|| Test Loss: 0.10135919827252123\n",
      "Epoch: 7738 \t|| Train Loss: 0.04335822434173808 \t|| Test Loss: 0.10131768827483097\n",
      "Epoch: 7739 \t|| Train Loss: 0.04332356196748058 \t|| Test Loss: 0.10120748827752397\n",
      "Epoch: 7740 \t|| Train Loss: 0.04328932334466677 \t|| Test Loss: 0.10116597827983372\n",
      "Epoch: 7741 \t|| Train Loss: 0.04325504197082586 \t|| Test Loss: 0.10105577828252672\n",
      "Epoch: 7742 \t|| Train Loss: 0.04322042234759545 \t|| Test Loss: 0.10101426828483646\n",
      "Epoch: 7743 \t|| Train Loss: 0.04318652197417113 \t|| Test Loss: 0.10090406828752947\n",
      "Epoch: 7744 \t|| Train Loss: 0.043151621975954804 \t|| Test Loss: 0.10079386829022248\n",
      "Epoch: 7745 \t|| Train Loss: 0.04311790135208574 \t|| Test Loss: 0.10075235829253222\n",
      "Epoch: 7746 \t|| Train Loss: 0.04308310197930008 \t|| Test Loss: 0.10064215829522523\n",
      "Epoch: 7747 \t|| Train Loss: 0.04304900035501442 \t|| Test Loss: 0.10060064829753497\n",
      "Epoch: 7748 \t|| Train Loss: 0.04301458198264535 \t|| Test Loss: 0.10049044830022798\n",
      "Epoch: 7749 \t|| Train Loss: 0.04298009935794311 \t|| Test Loss: 0.10044893830253772\n",
      "Epoch: 7750 \t|| Train Loss: 0.04294606198599062 \t|| Test Loss: 0.10033873830523073\n",
      "Epoch: 7751 \t|| Train Loss: 0.04291119836087179 \t|| Test Loss: 0.10029722830754047\n",
      "Epoch: 7752 \t|| Train Loss: 0.042877541989335896 \t|| Test Loss: 0.10018702831023348\n",
      "Epoch: 7753 \t|| Train Loss: 0.04284264199111957 \t|| Test Loss: 0.10007682831292652\n",
      "Epoch: 7754 \t|| Train Loss: 0.04280867736536208 \t|| Test Loss: 0.10003531831523622\n",
      "Epoch: 7755 \t|| Train Loss: 0.042774121994464835 \t|| Test Loss: 0.09992511831792923\n",
      "Epoch: 7756 \t|| Train Loss: 0.04273977636829076 \t|| Test Loss: 0.09988360832023897\n",
      "Epoch: 7757 \t|| Train Loss: 0.042705601997810115 \t|| Test Loss: 0.09977340832293197\n",
      "Epoch: 7758 \t|| Train Loss: 0.04267087537121945 \t|| Test Loss: 0.09973189832524172\n",
      "Epoch: 7759 \t|| Train Loss: 0.04263708200115539 \t|| Test Loss: 0.09962169832793473\n",
      "Epoch: 7760 \t|| Train Loss: 0.04260218200293906 \t|| Test Loss: 0.09951149833062775\n",
      "Epoch: 7761 \t|| Train Loss: 0.04256835437570972 \t|| Test Loss: 0.09946998833293746\n",
      "Epoch: 7762 \t|| Train Loss: 0.042533662006284334 \t|| Test Loss: 0.09935978833563047\n",
      "Epoch: 7763 \t|| Train Loss: 0.04249945337863842 \t|| Test Loss: 0.09931827833794019\n",
      "Epoch: 7764 \t|| Train Loss: 0.042465142009629586 \t|| Test Loss: 0.09920807834063324\n",
      "Epoch: 7765 \t|| Train Loss: 0.0424305523815671 \t|| Test Loss: 0.09916656834294299\n",
      "Epoch: 7766 \t|| Train Loss: 0.04239662201297488 \t|| Test Loss: 0.09905636834563598\n",
      "Epoch: 7767 \t|| Train Loss: 0.042361722014758546 \t|| Test Loss: 0.09894616834832896\n",
      "Epoch: 7768 \t|| Train Loss: 0.04232803138605738 \t|| Test Loss: 0.09890465835063875\n",
      "Epoch: 7769 \t|| Train Loss: 0.04229320201810384 \t|| Test Loss: 0.09879445835333175\n",
      "Epoch: 7770 \t|| Train Loss: 0.042259130388986064 \t|| Test Loss: 0.0987529483556415\n",
      "Epoch: 7771 \t|| Train Loss: 0.04222468202144909 \t|| Test Loss: 0.09864274835833446\n",
      "Epoch: 7772 \t|| Train Loss: 0.04219022939191475 \t|| Test Loss: 0.09860123836064423\n",
      "Epoch: 7773 \t|| Train Loss: 0.04215616202479438 \t|| Test Loss: 0.09849103836333725\n",
      "Epoch: 7774 \t|| Train Loss: 0.042121328394843435 \t|| Test Loss: 0.098449528365647\n",
      "Epoch: 7775 \t|| Train Loss: 0.04208764202813964 \t|| Test Loss: 0.09833932836833999\n",
      "Epoch: 7776 \t|| Train Loss: 0.04205274202992332 \t|| Test Loss: 0.09822912837103298\n",
      "Epoch: 7777 \t|| Train Loss: 0.042018807399333724 \t|| Test Loss: 0.09818761837334275\n",
      "Epoch: 7778 \t|| Train Loss: 0.04198422203326859 \t|| Test Loss: 0.09807741837603574\n",
      "Epoch: 7779 \t|| Train Loss: 0.041949906402262406 \t|| Test Loss: 0.09803590837834547\n",
      "Epoch: 7780 \t|| Train Loss: 0.04191570203661386 \t|| Test Loss: 0.0979257083810385\n",
      "Epoch: 7781 \t|| Train Loss: 0.04188100540519109 \t|| Test Loss: 0.09788419838334825\n",
      "Epoch: 7782 \t|| Train Loss: 0.04184718203995913 \t|| Test Loss: 0.09777399838604124\n",
      "Epoch: 7783 \t|| Train Loss: 0.0418122820417428 \t|| Test Loss: 0.09766379838873424\n",
      "Epoch: 7784 \t|| Train Loss: 0.04177848440968136 \t|| Test Loss: 0.097622288391044\n",
      "Epoch: 7785 \t|| Train Loss: 0.041743762045088076 \t|| Test Loss: 0.097512088393737\n",
      "Epoch: 7786 \t|| Train Loss: 0.041709583412610066 \t|| Test Loss: 0.09747057839604673\n",
      "Epoch: 7787 \t|| Train Loss: 0.04167524204843335 \t|| Test Loss: 0.09736037839873975\n",
      "Epoch: 7788 \t|| Train Loss: 0.04164068241553874 \t|| Test Loss: 0.09731886840104949\n",
      "Epoch: 7789 \t|| Train Loss: 0.041606722051778636 \t|| Test Loss: 0.09720866840374248\n",
      "Epoch: 7790 \t|| Train Loss: 0.041571822053562295 \t|| Test Loss: 0.09709846840643549\n",
      "Epoch: 7791 \t|| Train Loss: 0.04153816142002904 \t|| Test Loss: 0.09705695840874524\n",
      "Epoch: 7792 \t|| Train Loss: 0.041503302056907554 \t|| Test Loss: 0.09694675841143825\n",
      "Epoch: 7793 \t|| Train Loss: 0.04146926042295771 \t|| Test Loss: 0.09690524841374801\n",
      "Epoch: 7794 \t|| Train Loss: 0.04143478206025285 \t|| Test Loss: 0.09679504841644099\n",
      "Epoch: 7795 \t|| Train Loss: 0.04140035942588639 \t|| Test Loss: 0.09675353841875074\n",
      "Epoch: 7796 \t|| Train Loss: 0.041366262063598114 \t|| Test Loss: 0.09664333842144375\n",
      "Epoch: 7797 \t|| Train Loss: 0.04133145842881509 \t|| Test Loss: 0.09660182842375349\n",
      "Epoch: 7798 \t|| Train Loss: 0.041297742066943394 \t|| Test Loss: 0.09649162842644646\n",
      "Epoch: 7799 \t|| Train Loss: 0.04126284206872706 \t|| Test Loss: 0.09638142842913949\n",
      "Epoch: 7800 \t|| Train Loss: 0.041228937433305364 \t|| Test Loss: 0.09633991843144922\n",
      "Epoch: 7801 \t|| Train Loss: 0.04119432207207234 \t|| Test Loss: 0.09622971843414224\n",
      "Epoch: 7802 \t|| Train Loss: 0.04116003643623405 \t|| Test Loss: 0.09618820843645201\n",
      "Epoch: 7803 \t|| Train Loss: 0.04112580207541762 \t|| Test Loss: 0.09607800843914498\n",
      "Epoch: 7804 \t|| Train Loss: 0.04109113543916273 \t|| Test Loss: 0.09603649844145473\n",
      "Epoch: 7805 \t|| Train Loss: 0.041057282078762886 \t|| Test Loss: 0.09592629844414775\n",
      "Epoch: 7806 \t|| Train Loss: 0.04102238208054655 \t|| Test Loss: 0.09581609844684076\n",
      "Epoch: 7807 \t|| Train Loss: 0.040988614443653024 \t|| Test Loss: 0.0957745884491505\n",
      "Epoch: 7808 \t|| Train Loss: 0.040953862083891825 \t|| Test Loss: 0.0956643884518435\n",
      "Epoch: 7809 \t|| Train Loss: 0.040919713446581706 \t|| Test Loss: 0.09562287845415325\n",
      "Epoch: 7810 \t|| Train Loss: 0.0408853420872371 \t|| Test Loss: 0.09551267845684627\n",
      "Epoch: 7811 \t|| Train Loss: 0.04085081244951039 \t|| Test Loss: 0.09547116845915599\n",
      "Epoch: 7812 \t|| Train Loss: 0.04081682209058238 \t|| Test Loss: 0.09536096846184898\n",
      "Epoch: 7813 \t|| Train Loss: 0.04078192209236603 \t|| Test Loss: 0.09525076846454199\n",
      "Epoch: 7814 \t|| Train Loss: 0.040748291454000676 \t|| Test Loss: 0.09520925846685174\n",
      "Epoch: 7815 \t|| Train Loss: 0.04071340209571132 \t|| Test Loss: 0.09509905846954476\n",
      "Epoch: 7816 \t|| Train Loss: 0.04067939045692935 \t|| Test Loss: 0.09505754847185452\n",
      "Epoch: 7817 \t|| Train Loss: 0.04064488209905658 \t|| Test Loss: 0.0949473484745475\n",
      "Epoch: 7818 \t|| Train Loss: 0.04061048945985803 \t|| Test Loss: 0.09490583847685723\n",
      "Epoch: 7819 \t|| Train Loss: 0.040576362102401856 \t|| Test Loss: 0.09479563847955026\n",
      "Epoch: 7820 \t|| Train Loss: 0.04054158846278673 \t|| Test Loss: 0.09475412848186\n",
      "Epoch: 7821 \t|| Train Loss: 0.040507842105747136 \t|| Test Loss: 0.094643928484553\n",
      "Epoch: 7822 \t|| Train Loss: 0.04047294210753081 \t|| Test Loss: 0.094533728487246\n",
      "Epoch: 7823 \t|| Train Loss: 0.040439067467277004 \t|| Test Loss: 0.09449221848955573\n",
      "Epoch: 7824 \t|| Train Loss: 0.040404422110876095 \t|| Test Loss: 0.09438201849224875\n",
      "Epoch: 7825 \t|| Train Loss: 0.0403701664702057 \t|| Test Loss: 0.09434050849455848\n",
      "Epoch: 7826 \t|| Train Loss: 0.04033590211422135 \t|| Test Loss: 0.0942303084972515\n",
      "Epoch: 7827 \t|| Train Loss: 0.040301265473134375 \t|| Test Loss: 0.09418879849956124\n",
      "Epoch: 7828 \t|| Train Loss: 0.04026738211756663 \t|| Test Loss: 0.09407859850225428\n",
      "Epoch: 7829 \t|| Train Loss: 0.04023248211935029 \t|| Test Loss: 0.0939683985049473\n",
      "Epoch: 7830 \t|| Train Loss: 0.04019874447762467 \t|| Test Loss: 0.09392688850725704\n",
      "Epoch: 7831 \t|| Train Loss: 0.04016396212269559 \t|| Test Loss: 0.09381668850995008\n",
      "Epoch: 7832 \t|| Train Loss: 0.04012984348055335 \t|| Test Loss: 0.0937751785122598\n",
      "Epoch: 7833 \t|| Train Loss: 0.04009544212604086 \t|| Test Loss: 0.09366497851495278\n",
      "Epoch: 7834 \t|| Train Loss: 0.04006094248348205 \t|| Test Loss: 0.09362346851726253\n",
      "Epoch: 7835 \t|| Train Loss: 0.040026922129386126 \t|| Test Loss: 0.09351326851995552\n",
      "Epoch: 7836 \t|| Train Loss: 0.03999204148641072 \t|| Test Loss: 0.09347175852226533\n",
      "Epoch: 7837 \t|| Train Loss: 0.0399584021327314 \t|| Test Loss: 0.09336155852495831\n",
      "Epoch: 7838 \t|| Train Loss: 0.03992350213451508 \t|| Test Loss: 0.0932513585276513\n",
      "Epoch: 7839 \t|| Train Loss: 0.03988952049090101 \t|| Test Loss: 0.09320984852996105\n",
      "Epoch: 7840 \t|| Train Loss: 0.03985498213786035 \t|| Test Loss: 0.0930996485326541\n",
      "Epoch: 7841 \t|| Train Loss: 0.0398206194938297 \t|| Test Loss: 0.0930581385349638\n",
      "Epoch: 7842 \t|| Train Loss: 0.03978646214120561 \t|| Test Loss: 0.0929479385376568\n",
      "Epoch: 7843 \t|| Train Loss: 0.03975171849675837 \t|| Test Loss: 0.09290642853996656\n",
      "Epoch: 7844 \t|| Train Loss: 0.03971794214455089 \t|| Test Loss: 0.09279622854265956\n",
      "Epoch: 7845 \t|| Train Loss: 0.039683042146334564 \t|| Test Loss: 0.09268602854535257\n",
      "Epoch: 7846 \t|| Train Loss: 0.03964919750124866 \t|| Test Loss: 0.0926445185476623\n",
      "Epoch: 7847 \t|| Train Loss: 0.03961452214967983 \t|| Test Loss: 0.09253431855035532\n",
      "Epoch: 7848 \t|| Train Loss: 0.03958029650417735 \t|| Test Loss: 0.09249280855266505\n",
      "Epoch: 7849 \t|| Train Loss: 0.039546002153025096 \t|| Test Loss: 0.09238260855535806\n",
      "Epoch: 7850 \t|| Train Loss: 0.03951139550710604 \t|| Test Loss: 0.09234109855766778\n",
      "Epoch: 7851 \t|| Train Loss: 0.03947748215637038 \t|| Test Loss: 0.09223089856036082\n",
      "Epoch: 7852 \t|| Train Loss: 0.03944258215815405 \t|| Test Loss: 0.09212069856305381\n",
      "Epoch: 7853 \t|| Train Loss: 0.03940887451159632 \t|| Test Loss: 0.09207918856536357\n",
      "Epoch: 7854 \t|| Train Loss: 0.03937406216149932 \t|| Test Loss: 0.09196898856805658\n",
      "Epoch: 7855 \t|| Train Loss: 0.039339973514525 \t|| Test Loss: 0.0919274785703663\n",
      "Epoch: 7856 \t|| Train Loss: 0.039305542164844595 \t|| Test Loss: 0.09181727857305931\n",
      "Epoch: 7857 \t|| Train Loss: 0.03927107251745369 \t|| Test Loss: 0.09177576857536905\n",
      "Epoch: 7858 \t|| Train Loss: 0.039237022168189875 \t|| Test Loss: 0.09166556857806206\n",
      "Epoch: 7859 \t|| Train Loss: 0.03920217152038237 \t|| Test Loss: 0.09162405858037179\n",
      "Epoch: 7860 \t|| Train Loss: 0.03916850217153515 \t|| Test Loss: 0.09151385858306481\n",
      "Epoch: 7861 \t|| Train Loss: 0.039133602173318814 \t|| Test Loss: 0.0914036585857578\n",
      "Epoch: 7862 \t|| Train Loss: 0.03909965052487266 \t|| Test Loss: 0.09136214858806754\n",
      "Epoch: 7863 \t|| Train Loss: 0.03906508217666408 \t|| Test Loss: 0.09125194859076059\n",
      "Epoch: 7864 \t|| Train Loss: 0.039030749527801334 \t|| Test Loss: 0.0912104385930703\n",
      "Epoch: 7865 \t|| Train Loss: 0.03899656218000937 \t|| Test Loss: 0.09110023859576331\n",
      "Epoch: 7866 \t|| Train Loss: 0.03896184853073002 \t|| Test Loss: 0.09105872859807304\n",
      "Epoch: 7867 \t|| Train Loss: 0.03892804218335464 \t|| Test Loss: 0.09094852860076606\n",
      "Epoch: 7868 \t|| Train Loss: 0.038893142185138306 \t|| Test Loss: 0.09083832860345906\n",
      "Epoch: 7869 \t|| Train Loss: 0.03885932753522031 \t|| Test Loss: 0.0907968186057688\n",
      "Epoch: 7870 \t|| Train Loss: 0.03882462218848358 \t|| Test Loss: 0.09068661860846179\n",
      "Epoch: 7871 \t|| Train Loss: 0.03879042653814898 \t|| Test Loss: 0.09064510861077156\n",
      "Epoch: 7872 \t|| Train Loss: 0.038756102191828866 \t|| Test Loss: 0.09053490861346455\n",
      "Epoch: 7873 \t|| Train Loss: 0.038721525541077675 \t|| Test Loss: 0.09049339861577432\n",
      "Epoch: 7874 \t|| Train Loss: 0.038687582195174125 \t|| Test Loss: 0.09038319861846732\n",
      "Epoch: 7875 \t|| Train Loss: 0.03865268219695779 \t|| Test Loss: 0.09027299862116031\n",
      "Epoch: 7876 \t|| Train Loss: 0.03861900454556797 \t|| Test Loss: 0.09023148862347005\n",
      "Epoch: 7877 \t|| Train Loss: 0.03858416220030307 \t|| Test Loss: 0.09012128862616306\n",
      "Epoch: 7878 \t|| Train Loss: 0.038550103548496646 \t|| Test Loss: 0.09007977862847286\n",
      "Epoch: 7879 \t|| Train Loss: 0.03851564220364835 \t|| Test Loss: 0.08996957863116584\n",
      "Epoch: 7880 \t|| Train Loss: 0.03848120255142532 \t|| Test Loss: 0.08992806863347558\n",
      "Epoch: 7881 \t|| Train Loss: 0.038447122206993624 \t|| Test Loss: 0.08981786863616856\n",
      "Epoch: 7882 \t|| Train Loss: 0.03841230155435401 \t|| Test Loss: 0.0897763586384783\n",
      "Epoch: 7883 \t|| Train Loss: 0.03837860221033889 \t|| Test Loss: 0.08966615864117132\n",
      "Epoch: 7884 \t|| Train Loss: 0.03834370221212256 \t|| Test Loss: 0.08955595864386431\n",
      "Epoch: 7885 \t|| Train Loss: 0.038309780558844306 \t|| Test Loss: 0.08951444864617406\n",
      "Epoch: 7886 \t|| Train Loss: 0.03827518221546784 \t|| Test Loss: 0.08940424864886708\n",
      "Epoch: 7887 \t|| Train Loss: 0.03824087956177298 \t|| Test Loss: 0.0893627386511768\n",
      "Epoch: 7888 \t|| Train Loss: 0.038206662218813116 \t|| Test Loss: 0.08925253865386984\n",
      "Epoch: 7889 \t|| Train Loss: 0.03817197856470168 \t|| Test Loss: 0.08921102865617957\n",
      "Epoch: 7890 \t|| Train Loss: 0.03813814222215839 \t|| Test Loss: 0.08910082865887256\n",
      "Epoch: 7891 \t|| Train Loss: 0.038103242223942055 \t|| Test Loss: 0.08899062866156557\n",
      "Epoch: 7892 \t|| Train Loss: 0.03806945756919196 \t|| Test Loss: 0.08894911866387531\n",
      "Epoch: 7893 \t|| Train Loss: 0.03803472222728733 \t|| Test Loss: 0.08883891866656832\n",
      "Epoch: 7894 \t|| Train Loss: 0.03800055657212063 \t|| Test Loss: 0.08879740866887806\n",
      "Epoch: 7895 \t|| Train Loss: 0.03796620223063261 \t|| Test Loss: 0.08868720867157108\n",
      "Epoch: 7896 \t|| Train Loss: 0.03793165557504932 \t|| Test Loss: 0.08864569867388081\n",
      "Epoch: 7897 \t|| Train Loss: 0.03789768223397788 \t|| Test Loss: 0.08853549867657382\n",
      "Epoch: 7898 \t|| Train Loss: 0.03786278223576154 \t|| Test Loss: 0.08842529867926685\n",
      "Epoch: 7899 \t|| Train Loss: 0.03782913457953961 \t|| Test Loss: 0.08838378868157658\n",
      "Epoch: 7900 \t|| Train Loss: 0.03779426223910681 \t|| Test Loss: 0.08827358868426957\n",
      "Epoch: 7901 \t|| Train Loss: 0.037760233582468314 \t|| Test Loss: 0.08823207868657931\n",
      "Epoch: 7902 \t|| Train Loss: 0.03772574224245209 \t|| Test Loss: 0.08812187868927232\n",
      "Epoch: 7903 \t|| Train Loss: 0.03769133258539698 \t|| Test Loss: 0.08808036869158206\n",
      "Epoch: 7904 \t|| Train Loss: 0.03765722224579738 \t|| Test Loss: 0.08797016869427507\n",
      "Epoch: 7905 \t|| Train Loss: 0.03762243158832566 \t|| Test Loss: 0.08792865869658481\n",
      "Epoch: 7906 \t|| Train Loss: 0.03758870224914264 \t|| Test Loss: 0.08781845869927782\n",
      "Epoch: 7907 \t|| Train Loss: 0.03755380225092632 \t|| Test Loss: 0.08770825870197081\n",
      "Epoch: 7908 \t|| Train Loss: 0.037519910592815946 \t|| Test Loss: 0.08766674870428057\n",
      "Epoch: 7909 \t|| Train Loss: 0.03748528225427157 \t|| Test Loss: 0.08755654870697357\n",
      "Epoch: 7910 \t|| Train Loss: 0.03745100959574462 \t|| Test Loss: 0.08751503870928332\n",
      "Epoch: 7911 \t|| Train Loss: 0.03741676225761685 \t|| Test Loss: 0.08740483871197632\n",
      "Epoch: 7912 \t|| Train Loss: 0.037382108598673316 \t|| Test Loss: 0.08736332871428608\n",
      "Epoch: 7913 \t|| Train Loss: 0.03734824226096214 \t|| Test Loss: 0.0872531287169791\n",
      "Epoch: 7914 \t|| Train Loss: 0.0373133422627458 \t|| Test Loss: 0.08714292871967208\n",
      "Epoch: 7915 \t|| Train Loss: 0.03727958760316359 \t|| Test Loss: 0.08710141872198186\n",
      "Epoch: 7916 \t|| Train Loss: 0.03724482226609106 \t|| Test Loss: 0.08699121872467483\n",
      "Epoch: 7917 \t|| Train Loss: 0.03721068660609228 \t|| Test Loss: 0.08694970872698457\n",
      "Epoch: 7918 \t|| Train Loss: 0.03717630226943635 \t|| Test Loss: 0.08683950872967758\n",
      "Epoch: 7919 \t|| Train Loss: 0.037141785609020955 \t|| Test Loss: 0.08679799873198735\n",
      "Epoch: 7920 \t|| Train Loss: 0.03710778227278162 \t|| Test Loss: 0.08668779873468033\n",
      "Epoch: 7921 \t|| Train Loss: 0.037072884611949644 \t|| Test Loss: 0.08664628873699007\n",
      "Epoch: 7922 \t|| Train Loss: 0.0370392622761269 \t|| Test Loss: 0.08653608873968309\n",
      "Epoch: 7923 \t|| Train Loss: 0.037004362277910575 \t|| Test Loss: 0.0864258887423761\n",
      "Epoch: 7924 \t|| Train Loss: 0.03697036361643994 \t|| Test Loss: 0.08638437874468582\n",
      "Epoch: 7925 \t|| Train Loss: 0.036935842281255835 \t|| Test Loss: 0.08627417874737883\n",
      "Epoch: 7926 \t|| Train Loss: 0.036901462619368615 \t|| Test Loss: 0.08623266874968859\n",
      "Epoch: 7927 \t|| Train Loss: 0.036867322284601114 \t|| Test Loss: 0.08612246875238158\n",
      "Epoch: 7928 \t|| Train Loss: 0.0368325616222973 \t|| Test Loss: 0.08608095875469132\n",
      "Epoch: 7929 \t|| Train Loss: 0.036798802287946394 \t|| Test Loss: 0.08597075875738433\n",
      "Epoch: 7930 \t|| Train Loss: 0.036763902289730054 \t|| Test Loss: 0.08586055876007735\n",
      "Epoch: 7931 \t|| Train Loss: 0.0367300406267876 \t|| Test Loss: 0.08581904876238708\n",
      "Epoch: 7932 \t|| Train Loss: 0.03669538229307534 \t|| Test Loss: 0.08570884876508009\n",
      "Epoch: 7933 \t|| Train Loss: 0.036661139629716274 \t|| Test Loss: 0.08566733876738983\n",
      "Epoch: 7934 \t|| Train Loss: 0.0366268622964206 \t|| Test Loss: 0.08555713877008285\n",
      "Epoch: 7935 \t|| Train Loss: 0.03659223863264495 \t|| Test Loss: 0.08551562877239256\n",
      "Epoch: 7936 \t|| Train Loss: 0.03655834229976587 \t|| Test Loss: 0.08540542877508558\n",
      "Epoch: 7937 \t|| Train Loss: 0.03652344230154954 \t|| Test Loss: 0.0852952287777786\n",
      "Epoch: 7938 \t|| Train Loss: 0.03648971763713524 \t|| Test Loss: 0.08525371878008833\n",
      "Epoch: 7939 \t|| Train Loss: 0.03645492230489482 \t|| Test Loss: 0.08514351878278134\n",
      "Epoch: 7940 \t|| Train Loss: 0.03642081664006393 \t|| Test Loss: 0.08510200878509108\n",
      "Epoch: 7941 \t|| Train Loss: 0.0363864023082401 \t|| Test Loss: 0.08499180878778409\n",
      "Epoch: 7942 \t|| Train Loss: 0.0363519156429926 \t|| Test Loss: 0.08495029879009383\n",
      "Epoch: 7943 \t|| Train Loss: 0.03631788231158537 \t|| Test Loss: 0.08484009879278681\n",
      "Epoch: 7944 \t|| Train Loss: 0.036283014645921305 \t|| Test Loss: 0.08479858879509658\n",
      "Epoch: 7945 \t|| Train Loss: 0.036249362314930644 \t|| Test Loss: 0.08468838879778956\n",
      "Epoch: 7946 \t|| Train Loss: 0.036214462316714324 \t|| Test Loss: 0.0845781888004826\n",
      "Epoch: 7947 \t|| Train Loss: 0.03618049365041158 \t|| Test Loss: 0.08453667880279234\n",
      "Epoch: 7948 \t|| Train Loss: 0.03614594232005959 \t|| Test Loss: 0.08442647880548533\n",
      "Epoch: 7949 \t|| Train Loss: 0.03611159265334026 \t|| Test Loss: 0.08438496880779509\n",
      "Epoch: 7950 \t|| Train Loss: 0.03607742232340487 \t|| Test Loss: 0.08427476881048812\n",
      "Epoch: 7951 \t|| Train Loss: 0.03604269165626896 \t|| Test Loss: 0.08423325881279783\n",
      "Epoch: 7952 \t|| Train Loss: 0.036008902326750136 \t|| Test Loss: 0.08412305881549084\n",
      "Epoch: 7953 \t|| Train Loss: 0.0359740023285338 \t|| Test Loss: 0.08401285881818385\n",
      "Epoch: 7954 \t|| Train Loss: 0.03594017066075923 \t|| Test Loss: 0.08397134882049359\n",
      "Epoch: 7955 \t|| Train Loss: 0.035905482331879075 \t|| Test Loss: 0.0838611488231866\n",
      "Epoch: 7956 \t|| Train Loss: 0.03587126966368792 \t|| Test Loss: 0.08381963882549634\n",
      "Epoch: 7957 \t|| Train Loss: 0.035836962335224355 \t|| Test Loss: 0.08370943882818935\n",
      "Epoch: 7958 \t|| Train Loss: 0.03580236866661661 \t|| Test Loss: 0.08366792883049907\n",
      "Epoch: 7959 \t|| Train Loss: 0.035768442338569614 \t|| Test Loss: 0.0835577288331921\n",
      "Epoch: 7960 \t|| Train Loss: 0.035733542340353294 \t|| Test Loss: 0.0834475288358851\n",
      "Epoch: 7961 \t|| Train Loss: 0.035699847671106885 \t|| Test Loss: 0.08340601883819485\n",
      "Epoch: 7962 \t|| Train Loss: 0.035665022343698574 \t|| Test Loss: 0.08329581884088784\n",
      "Epoch: 7963 \t|| Train Loss: 0.03563094667403556 \t|| Test Loss: 0.0832543088431976\n",
      "Epoch: 7964 \t|| Train Loss: 0.03559650234704383 \t|| Test Loss: 0.0831441088458906\n",
      "Epoch: 7965 \t|| Train Loss: 0.03556204567696426 \t|| Test Loss: 0.08310259884820034\n",
      "Epoch: 7966 \t|| Train Loss: 0.03552798235038911 \t|| Test Loss: 0.08299239885089335\n",
      "Epoch: 7967 \t|| Train Loss: 0.03549314467989294 \t|| Test Loss: 0.08295088885320309\n",
      "Epoch: 7968 \t|| Train Loss: 0.03545946235373439 \t|| Test Loss: 0.0828406888558961\n",
      "Epoch: 7969 \t|| Train Loss: 0.035424562355518066 \t|| Test Loss: 0.0827304888585891\n",
      "Epoch: 7970 \t|| Train Loss: 0.035390623684383234 \t|| Test Loss: 0.08268897886089883\n",
      "Epoch: 7971 \t|| Train Loss: 0.035356042358863325 \t|| Test Loss: 0.08257877886359186\n",
      "Epoch: 7972 \t|| Train Loss: 0.03532172268731191 \t|| Test Loss: 0.08253726886590157\n",
      "Epoch: 7973 \t|| Train Loss: 0.0352875223622086 \t|| Test Loss: 0.08242706886859462\n",
      "Epoch: 7974 \t|| Train Loss: 0.03525282169024059 \t|| Test Loss: 0.08238555887090435\n",
      "Epoch: 7975 \t|| Train Loss: 0.03521900236555388 \t|| Test Loss: 0.08227535887359735\n",
      "Epoch: 7976 \t|| Train Loss: 0.035184102367337544 \t|| Test Loss: 0.08216515887629035\n",
      "Epoch: 7977 \t|| Train Loss: 0.03515030069473087 \t|| Test Loss: 0.0821236488786001\n",
      "Epoch: 7978 \t|| Train Loss: 0.03511558237068283 \t|| Test Loss: 0.0820134488812931\n",
      "Epoch: 7979 \t|| Train Loss: 0.03508139969765956 \t|| Test Loss: 0.08197193888360285\n",
      "Epoch: 7980 \t|| Train Loss: 0.035047062374028104 \t|| Test Loss: 0.08186173888629582\n",
      "Epoch: 7981 \t|| Train Loss: 0.03501249870058823 \t|| Test Loss: 0.0818202288886056\n",
      "Epoch: 7982 \t|| Train Loss: 0.03497854237737337 \t|| Test Loss: 0.08171002889129858\n",
      "Epoch: 7983 \t|| Train Loss: 0.03494364237915703 \t|| Test Loss: 0.0815998288939916\n",
      "Epoch: 7984 \t|| Train Loss: 0.03490997770507853 \t|| Test Loss: 0.08155831889630136\n",
      "Epoch: 7985 \t|| Train Loss: 0.03487512238250231 \t|| Test Loss: 0.08144811889899434\n",
      "Epoch: 7986 \t|| Train Loss: 0.03484107670800722 \t|| Test Loss: 0.08140660890130409\n",
      "Epoch: 7987 \t|| Train Loss: 0.03480660238584759 \t|| Test Loss: 0.08129640890399709\n",
      "Epoch: 7988 \t|| Train Loss: 0.034772175710935896 \t|| Test Loss: 0.08125489890630688\n",
      "Epoch: 7989 \t|| Train Loss: 0.034738082389192876 \t|| Test Loss: 0.08114469890899986\n",
      "Epoch: 7990 \t|| Train Loss: 0.03470327471386458 \t|| Test Loss: 0.0811031889113096\n",
      "Epoch: 7991 \t|| Train Loss: 0.03466956239253814 \t|| Test Loss: 0.0809929889140026\n",
      "Epoch: 7992 \t|| Train Loss: 0.034634662394321794 \t|| Test Loss: 0.08088278891669562\n",
      "Epoch: 7993 \t|| Train Loss: 0.03460075371835486 \t|| Test Loss: 0.08084127891900536\n",
      "Epoch: 7994 \t|| Train Loss: 0.034566142397667074 \t|| Test Loss: 0.08073107892169835\n",
      "Epoch: 7995 \t|| Train Loss: 0.03453185272128356 \t|| Test Loss: 0.0806895689240081\n",
      "Epoch: 7996 \t|| Train Loss: 0.03449762240101236 \t|| Test Loss: 0.08057936892670112\n",
      "Epoch: 7997 \t|| Train Loss: 0.03446295172421223 \t|| Test Loss: 0.08053785892901084\n",
      "Epoch: 7998 \t|| Train Loss: 0.03442910240435763 \t|| Test Loss: 0.08042765893170387\n",
      "Epoch: 7999 \t|| Train Loss: 0.03439420240614129 \t|| Test Loss: 0.08031745893439687\n",
      "Epoch: 8000 \t|| Train Loss: 0.034360430728702526 \t|| Test Loss: 0.0802759489367066\n",
      "Epoch: 8001 \t|| Train Loss: 0.034325682409486566 \t|| Test Loss: 0.08016574893939961\n",
      "Epoch: 8002 \t|| Train Loss: 0.03429152973163121 \t|| Test Loss: 0.08012423894170935\n",
      "Epoch: 8003 \t|| Train Loss: 0.034257162412831846 \t|| Test Loss: 0.08001403894440234\n",
      "Epoch: 8004 \t|| Train Loss: 0.03422262873455989 \t|| Test Loss: 0.0799725289467121\n",
      "Epoch: 8005 \t|| Train Loss: 0.03418864241617712 \t|| Test Loss: 0.0798623289494051\n",
      "Epoch: 8006 \t|| Train Loss: 0.03415374241796079 \t|| Test Loss: 0.07975212895209813\n",
      "Epoch: 8007 \t|| Train Loss: 0.03412010773905018 \t|| Test Loss: 0.07971061895440786\n",
      "Epoch: 8008 \t|| Train Loss: 0.03408522242130606 \t|| Test Loss: 0.07960041895710089\n",
      "Epoch: 8009 \t|| Train Loss: 0.034051206741978854 \t|| Test Loss: 0.0795589089594106\n",
      "Epoch: 8010 \t|| Train Loss: 0.03401670242465134 \t|| Test Loss: 0.07944870896210363\n",
      "Epoch: 8011 \t|| Train Loss: 0.03398230574490756 \t|| Test Loss: 0.07940719896441334\n",
      "Epoch: 8012 \t|| Train Loss: 0.033948182427996604 \t|| Test Loss: 0.07929699896710636\n",
      "Epoch: 8013 \t|| Train Loss: 0.03391340474783623 \t|| Test Loss: 0.07925548896941609\n",
      "Epoch: 8014 \t|| Train Loss: 0.033879662431341884 \t|| Test Loss: 0.07914528897210911\n",
      "Epoch: 8015 \t|| Train Loss: 0.03384476243312555 \t|| Test Loss: 0.07903508897480212\n",
      "Epoch: 8016 \t|| Train Loss: 0.033810883752326513 \t|| Test Loss: 0.07899357897711184\n",
      "Epoch: 8017 \t|| Train Loss: 0.03377624243647083 \t|| Test Loss: 0.07888337897980484\n",
      "Epoch: 8018 \t|| Train Loss: 0.033741982755255195 \t|| Test Loss: 0.0788418689821146\n",
      "Epoch: 8019 \t|| Train Loss: 0.03370772243981609 \t|| Test Loss: 0.07873166898480762\n",
      "Epoch: 8020 \t|| Train Loss: 0.03367308175818387 \t|| Test Loss: 0.07869015898711736\n",
      "Epoch: 8021 \t|| Train Loss: 0.033639202443161376 \t|| Test Loss: 0.07857995898981036\n",
      "Epoch: 8022 \t|| Train Loss: 0.033604302444945035 \t|| Test Loss: 0.07846975899250333\n",
      "Epoch: 8023 \t|| Train Loss: 0.03357056076267417 \t|| Test Loss: 0.07842824899481313\n",
      "Epoch: 8024 \t|| Train Loss: 0.03353578244829031 \t|| Test Loss: 0.0783180489975061\n",
      "Epoch: 8025 \t|| Train Loss: 0.033501659765602855 \t|| Test Loss: 0.07827653899981589\n",
      "Epoch: 8026 \t|| Train Loss: 0.033467262451635574 \t|| Test Loss: 0.07816633900250886\n",
      "Epoch: 8027 \t|| Train Loss: 0.03343275876853154 \t|| Test Loss: 0.07812482900481861\n",
      "Epoch: 8028 \t|| Train Loss: 0.03339874245498086 \t|| Test Loss: 0.07801462900751163\n",
      "Epoch: 8029 \t|| Train Loss: 0.03336385777146021 \t|| Test Loss: 0.07797311900982137\n",
      "Epoch: 8030 \t|| Train Loss: 0.033330222458326134 \t|| Test Loss: 0.07786291901251438\n",
      "Epoch: 8031 \t|| Train Loss: 0.03329532246010981 \t|| Test Loss: 0.07775271901520736\n",
      "Epoch: 8032 \t|| Train Loss: 0.03326133677595051 \t|| Test Loss: 0.07771120901751713\n",
      "Epoch: 8033 \t|| Train Loss: 0.033226802463455093 \t|| Test Loss: 0.07760100902021014\n",
      "Epoch: 8034 \t|| Train Loss: 0.03319243577887919 \t|| Test Loss: 0.07755949902251986\n",
      "Epoch: 8035 \t|| Train Loss: 0.03315828246680036 \t|| Test Loss: 0.07744929902521287\n",
      "Epoch: 8036 \t|| Train Loss: 0.033123534781807865 \t|| Test Loss: 0.07740778902752263\n",
      "Epoch: 8037 \t|| Train Loss: 0.03308976247014563 \t|| Test Loss: 0.07729758903021562\n",
      "Epoch: 8038 \t|| Train Loss: 0.0330548624719293 \t|| Test Loss: 0.07718738903290862\n",
      "Epoch: 8039 \t|| Train Loss: 0.03302101378629817 \t|| Test Loss: 0.07714587903521837\n",
      "Epoch: 8040 \t|| Train Loss: 0.03298634247527457 \t|| Test Loss: 0.07703567903791139\n",
      "Epoch: 8041 \t|| Train Loss: 0.03295211278922684 \t|| Test Loss: 0.0769941690402211\n",
      "Epoch: 8042 \t|| Train Loss: 0.03291782247861985 \t|| Test Loss: 0.0768839690429141\n",
      "Epoch: 8043 \t|| Train Loss: 0.032883211792155524 \t|| Test Loss: 0.07684245904522387\n",
      "Epoch: 8044 \t|| Train Loss: 0.032849302481965124 \t|| Test Loss: 0.07673225904791688\n",
      "Epoch: 8045 \t|| Train Loss: 0.032814402483748784 \t|| Test Loss: 0.0766220590506099\n",
      "Epoch: 8046 \t|| Train Loss: 0.0327806907966458 \t|| Test Loss: 0.07658054905291961\n",
      "Epoch: 8047 \t|| Train Loss: 0.032745882487094057 \t|| Test Loss: 0.07647034905561263\n",
      "Epoch: 8048 \t|| Train Loss: 0.032711789799574495 \t|| Test Loss: 0.07642883905792236\n",
      "Epoch: 8049 \t|| Train Loss: 0.03267736249043934 \t|| Test Loss: 0.07631863906061537\n",
      "Epoch: 8050 \t|| Train Loss: 0.03264288880250318 \t|| Test Loss: 0.07627712906292512\n",
      "Epoch: 8051 \t|| Train Loss: 0.03260884249378462 \t|| Test Loss: 0.07616692906561813\n",
      "Epoch: 8052 \t|| Train Loss: 0.03257398780543186 \t|| Test Loss: 0.07612541906792789\n",
      "Epoch: 8053 \t|| Train Loss: 0.03254032249712988 \t|| Test Loss: 0.07601521907062084\n",
      "Epoch: 8054 \t|| Train Loss: 0.03250542249891356 \t|| Test Loss: 0.07590501907331389\n",
      "Epoch: 8055 \t|| Train Loss: 0.03247146680992215 \t|| Test Loss: 0.07586350907562359\n",
      "Epoch: 8056 \t|| Train Loss: 0.03243690250225883 \t|| Test Loss: 0.07575330907831662\n",
      "Epoch: 8057 \t|| Train Loss: 0.03240256581285083 \t|| Test Loss: 0.07571179908062638\n",
      "Epoch: 8058 \t|| Train Loss: 0.03236838250560411 \t|| Test Loss: 0.07560159908331936\n",
      "Epoch: 8059 \t|| Train Loss: 0.032333664815779525 \t|| Test Loss: 0.07556008908562913\n",
      "Epoch: 8060 \t|| Train Loss: 0.03229986250894938 \t|| Test Loss: 0.07544988908832215\n",
      "Epoch: 8061 \t|| Train Loss: 0.032264962510733054 \t|| Test Loss: 0.07533968909101514\n",
      "Epoch: 8062 \t|| Train Loss: 0.03223114382026979 \t|| Test Loss: 0.07529817909332488\n",
      "Epoch: 8063 \t|| Train Loss: 0.03219644251407831 \t|| Test Loss: 0.07518797909601789\n",
      "Epoch: 8064 \t|| Train Loss: 0.03216224282319848 \t|| Test Loss: 0.07514646909832762\n",
      "Epoch: 8065 \t|| Train Loss: 0.03212792251742359 \t|| Test Loss: 0.07503626910102064\n",
      "Epoch: 8066 \t|| Train Loss: 0.032093341826127164 \t|| Test Loss: 0.07499475910333035\n",
      "Epoch: 8067 \t|| Train Loss: 0.03205940252076887 \t|| Test Loss: 0.07488455910602339\n",
      "Epoch: 8068 \t|| Train Loss: 0.03202450252255254 \t|| Test Loss: 0.0747743591087164\n",
      "Epoch: 8069 \t|| Train Loss: 0.03199082083061746 \t|| Test Loss: 0.07473284911102612\n",
      "Epoch: 8070 \t|| Train Loss: 0.03195598252589782 \t|| Test Loss: 0.07462264911371913\n",
      "Epoch: 8071 \t|| Train Loss: 0.03192191983354614 \t|| Test Loss: 0.07458113911602889\n",
      "Epoch: 8072 \t|| Train Loss: 0.031887462529243085 \t|| Test Loss: 0.0744709391187219\n",
      "Epoch: 8073 \t|| Train Loss: 0.031853018836474824 \t|| Test Loss: 0.07442942912103163\n",
      "Epoch: 8074 \t|| Train Loss: 0.03181894253258835 \t|| Test Loss: 0.07431922912372464\n",
      "Epoch: 8075 \t|| Train Loss: 0.031784117839403506 \t|| Test Loss: 0.07427771912603437\n",
      "Epoch: 8076 \t|| Train Loss: 0.03175042253593363 \t|| Test Loss: 0.07416751912872736\n",
      "Epoch: 8077 \t|| Train Loss: 0.0317155225377173 \t|| Test Loss: 0.07405731913142037\n",
      "Epoch: 8078 \t|| Train Loss: 0.031681596843893794 \t|| Test Loss: 0.07401580913373013\n",
      "Epoch: 8079 \t|| Train Loss: 0.03164700254106257 \t|| Test Loss: 0.07390560913642312\n",
      "Epoch: 8080 \t|| Train Loss: 0.031612695846822476 \t|| Test Loss: 0.07386409913873286\n",
      "Epoch: 8081 \t|| Train Loss: 0.03157848254440784 \t|| Test Loss: 0.07375389914142588\n",
      "Epoch: 8082 \t|| Train Loss: 0.03154379484975116 \t|| Test Loss: 0.07371238914373561\n",
      "Epoch: 8083 \t|| Train Loss: 0.03150996254775311 \t|| Test Loss: 0.07360218914642866\n",
      "Epoch: 8084 \t|| Train Loss: 0.03147506254953679 \t|| Test Loss: 0.07349198914912165\n",
      "Epoch: 8085 \t|| Train Loss: 0.03144127385424145 \t|| Test Loss: 0.0734504791514314\n",
      "Epoch: 8086 \t|| Train Loss: 0.03140654255288207 \t|| Test Loss: 0.07334027915412439\n",
      "Epoch: 8087 \t|| Train Loss: 0.03137237285717013 \t|| Test Loss: 0.07329876915643414\n",
      "Epoch: 8088 \t|| Train Loss: 0.031338022556227335 \t|| Test Loss: 0.07318856915912714\n",
      "Epoch: 8089 \t|| Train Loss: 0.03130347186009882 \t|| Test Loss: 0.07314705916143686\n",
      "Epoch: 8090 \t|| Train Loss: 0.031269502559572615 \t|| Test Loss: 0.07303685916412986\n",
      "Epoch: 8091 \t|| Train Loss: 0.031234602561356274 \t|| Test Loss: 0.0729266591668229\n",
      "Epoch: 8092 \t|| Train Loss: 0.0312009508645891 \t|| Test Loss: 0.07288514916913262\n",
      "Epoch: 8093 \t|| Train Loss: 0.031166082564701554 \t|| Test Loss: 0.07277494917182563\n",
      "Epoch: 8094 \t|| Train Loss: 0.031132049867517785 \t|| Test Loss: 0.07273343917413538\n",
      "Epoch: 8095 \t|| Train Loss: 0.031097562568046834 \t|| Test Loss: 0.07262323917682838\n",
      "Epoch: 8096 \t|| Train Loss: 0.031063148870446478 \t|| Test Loss: 0.07258172917913812\n",
      "Epoch: 8097 \t|| Train Loss: 0.0310290425713921 \t|| Test Loss: 0.07247152918183115\n",
      "Epoch: 8098 \t|| Train Loss: 0.030994247873375142 \t|| Test Loss: 0.07243001918414091\n",
      "Epoch: 8099 \t|| Train Loss: 0.03096052257473738 \t|| Test Loss: 0.07231981918683389\n",
      "Epoch: 8100 \t|| Train Loss: 0.03092562257652104 \t|| Test Loss: 0.07220961918952687\n",
      "Epoch: 8101 \t|| Train Loss: 0.030891726877865427 \t|| Test Loss: 0.07216810919183662\n",
      "Epoch: 8102 \t|| Train Loss: 0.030857102579866326 \t|| Test Loss: 0.07205790919452965\n",
      "Epoch: 8103 \t|| Train Loss: 0.030822825880794123 \t|| Test Loss: 0.07201639919683937\n",
      "Epoch: 8104 \t|| Train Loss: 0.030788582583211592 \t|| Test Loss: 0.07190619919953238\n",
      "Epoch: 8105 \t|| Train Loss: 0.030753924883722812 \t|| Test Loss: 0.07186468920184212\n",
      "Epoch: 8106 \t|| Train Loss: 0.030720062586556872 \t|| Test Loss: 0.07175448920453516\n",
      "Epoch: 8107 \t|| Train Loss: 0.03068516258834054 \t|| Test Loss: 0.07164428920722812\n",
      "Epoch: 8108 \t|| Train Loss: 0.03065140388821309 \t|| Test Loss: 0.07160277920953792\n",
      "Epoch: 8109 \t|| Train Loss: 0.030616642591685818 \t|| Test Loss: 0.07149257921223089\n",
      "Epoch: 8110 \t|| Train Loss: 0.030582502891141783 \t|| Test Loss: 0.07145106921454064\n",
      "Epoch: 8111 \t|| Train Loss: 0.030548122595031084 \t|| Test Loss: 0.07134086921723362\n",
      "Epoch: 8112 \t|| Train Loss: 0.030513601894070458 \t|| Test Loss: 0.0712993592195434\n",
      "Epoch: 8113 \t|| Train Loss: 0.030479602598376353 \t|| Test Loss: 0.07118915922223638\n",
      "Epoch: 8114 \t|| Train Loss: 0.03044470260016003 \t|| Test Loss: 0.07107895922492939\n",
      "Epoch: 8115 \t|| Train Loss: 0.030411080898560743 \t|| Test Loss: 0.07103744922723913\n",
      "Epoch: 8116 \t|| Train Loss: 0.03037618260350531 \t|| Test Loss: 0.07092724922993215\n",
      "Epoch: 8117 \t|| Train Loss: 0.03034217990148943 \t|| Test Loss: 0.0708857392322419\n",
      "Epoch: 8118 \t|| Train Loss: 0.030307662606850572 \t|| Test Loss: 0.07077553923493493\n",
      "Epoch: 8119 \t|| Train Loss: 0.030273278904418117 \t|| Test Loss: 0.07073402923724464\n",
      "Epoch: 8120 \t|| Train Loss: 0.030239142610195845 \t|| Test Loss: 0.07062382923993765\n",
      "Epoch: 8121 \t|| Train Loss: 0.030204377907346806 \t|| Test Loss: 0.07058231924224738\n",
      "Epoch: 8122 \t|| Train Loss: 0.030170622613541125 \t|| Test Loss: 0.0704721192449404\n",
      "Epoch: 8123 \t|| Train Loss: 0.030135722615324788 \t|| Test Loss: 0.07036191924763344\n",
      "Epoch: 8124 \t|| Train Loss: 0.03010185691183708 \t|| Test Loss: 0.07032040924994314\n",
      "Epoch: 8125 \t|| Train Loss: 0.030067202618670064 \t|| Test Loss: 0.07021020925263614\n",
      "Epoch: 8126 \t|| Train Loss: 0.03003295591476577 \t|| Test Loss: 0.07016869925494588\n",
      "Epoch: 8127 \t|| Train Loss: 0.029998682622015337 \t|| Test Loss: 0.07005849925763888\n",
      "Epoch: 8128 \t|| Train Loss: 0.029964054917694456 \t|| Test Loss: 0.07001698925994862\n",
      "Epoch: 8129 \t|| Train Loss: 0.0299301626253606 \t|| Test Loss: 0.06990678926264164\n",
      "Epoch: 8130 \t|| Train Loss: 0.02989526262714428 \t|| Test Loss: 0.06979658926533465\n",
      "Epoch: 8131 \t|| Train Loss: 0.029861533922184724 \t|| Test Loss: 0.06975507926764439\n",
      "Epoch: 8132 \t|| Train Loss: 0.029826742630489556 \t|| Test Loss: 0.06964487927033738\n",
      "Epoch: 8133 \t|| Train Loss: 0.029792632925113416 \t|| Test Loss: 0.06960336927264715\n",
      "Epoch: 8134 \t|| Train Loss: 0.02975822263383483 \t|| Test Loss: 0.06949316927534013\n",
      "Epoch: 8135 \t|| Train Loss: 0.029723731928042098 \t|| Test Loss: 0.06945165927764992\n",
      "Epoch: 8136 \t|| Train Loss: 0.029689702637180092 \t|| Test Loss: 0.06934145928034288\n",
      "Epoch: 8137 \t|| Train Loss: 0.029654830930970787 \t|| Test Loss: 0.06929994928265264\n",
      "Epoch: 8138 \t|| Train Loss: 0.02962118264052538 \t|| Test Loss: 0.06918974928534566\n",
      "Epoch: 8139 \t|| Train Loss: 0.02958628264230904 \t|| Test Loss: 0.06907954928803867\n",
      "Epoch: 8140 \t|| Train Loss: 0.029552309935461075 \t|| Test Loss: 0.06903803929034841\n",
      "Epoch: 8141 \t|| Train Loss: 0.029517762645654318 \t|| Test Loss: 0.0689278392930414\n",
      "Epoch: 8142 \t|| Train Loss: 0.029483408938389754 \t|| Test Loss: 0.06888632929535116\n",
      "Epoch: 8143 \t|| Train Loss: 0.029449242648999598 \t|| Test Loss: 0.06877612929804416\n",
      "Epoch: 8144 \t|| Train Loss: 0.02941450794131844 \t|| Test Loss: 0.06873461930035389\n",
      "Epoch: 8145 \t|| Train Loss: 0.02938072265234487 \t|| Test Loss: 0.0686244193030469\n",
      "Epoch: 8146 \t|| Train Loss: 0.02934582265412853 \t|| Test Loss: 0.0685142193057399\n",
      "Epoch: 8147 \t|| Train Loss: 0.02931198694580872 \t|| Test Loss: 0.06847270930804965\n",
      "Epoch: 8148 \t|| Train Loss: 0.029277302657473813 \t|| Test Loss: 0.06836250931074266\n",
      "Epoch: 8149 \t|| Train Loss: 0.029243085948737414 \t|| Test Loss: 0.0683209993130524\n",
      "Epoch: 8150 \t|| Train Loss: 0.02920878266081909 \t|| Test Loss: 0.06821079931574542\n",
      "Epoch: 8151 \t|| Train Loss: 0.029174184951666106 \t|| Test Loss: 0.06816928931805515\n",
      "Epoch: 8152 \t|| Train Loss: 0.029140262664164363 \t|| Test Loss: 0.06805908932074814\n",
      "Epoch: 8153 \t|| Train Loss: 0.029105362665948025 \t|| Test Loss: 0.06794888932344116\n",
      "Epoch: 8154 \t|| Train Loss: 0.02907166395615638 \t|| Test Loss: 0.0679073793257509\n",
      "Epoch: 8155 \t|| Train Loss: 0.0290368426692933 \t|| Test Loss: 0.06779717932844391\n",
      "Epoch: 8156 \t|| Train Loss: 0.02900276295908505 \t|| Test Loss: 0.06775566933075364\n",
      "Epoch: 8157 \t|| Train Loss: 0.028968322672638568 \t|| Test Loss: 0.06764546933344666\n",
      "Epoch: 8158 \t|| Train Loss: 0.028933861962013745 \t|| Test Loss: 0.0676039593357564\n",
      "Epoch: 8159 \t|| Train Loss: 0.028899802675983854 \t|| Test Loss: 0.0674937593384494\n",
      "Epoch: 8160 \t|| Train Loss: 0.028864960964942434 \t|| Test Loss: 0.06745224934075914\n",
      "Epoch: 8161 \t|| Train Loss: 0.028831282679329127 \t|| Test Loss: 0.06734204934345216\n",
      "Epoch: 8162 \t|| Train Loss: 0.028796382681112797 \t|| Test Loss: 0.06723184934614515\n",
      "Epoch: 8163 \t|| Train Loss: 0.02876243996943272 \t|| Test Loss: 0.06719033934845489\n",
      "Epoch: 8164 \t|| Train Loss: 0.028727862684458087 \t|| Test Loss: 0.0670801393511479\n",
      "Epoch: 8165 \t|| Train Loss: 0.028693538972361397 \t|| Test Loss: 0.06703862935345764\n",
      "Epoch: 8166 \t|| Train Loss: 0.028659342687803353 \t|| Test Loss: 0.06692842935615065\n",
      "Epoch: 8167 \t|| Train Loss: 0.02862463797529008 \t|| Test Loss: 0.0668869193584604\n",
      "Epoch: 8168 \t|| Train Loss: 0.02859082269114862 \t|| Test Loss: 0.06677671936115341\n",
      "Epoch: 8169 \t|| Train Loss: 0.028555922692932285 \t|| Test Loss: 0.0666665193638464\n",
      "Epoch: 8170 \t|| Train Loss: 0.02852211697978037 \t|| Test Loss: 0.06662500936615617\n",
      "Epoch: 8171 \t|| Train Loss: 0.028487402696277565 \t|| Test Loss: 0.06651480936884917\n",
      "Epoch: 8172 \t|| Train Loss: 0.02845321598270905 \t|| Test Loss: 0.06647329937115891\n",
      "Epoch: 8173 \t|| Train Loss: 0.028418882699622828 \t|| Test Loss: 0.06636309937385193\n",
      "Epoch: 8174 \t|| Train Loss: 0.02838431498563774 \t|| Test Loss: 0.06632158937616164\n",
      "Epoch: 8175 \t|| Train Loss: 0.028350362702968108 \t|| Test Loss: 0.06621138937885467\n",
      "Epoch: 8176 \t|| Train Loss: 0.02831546270475177 \t|| Test Loss: 0.06610118938154766\n",
      "Epoch: 8177 \t|| Train Loss: 0.028281793990128035 \t|| Test Loss: 0.0660596793838574\n",
      "Epoch: 8178 \t|| Train Loss: 0.028246942708097057 \t|| Test Loss: 0.06594947938655042\n",
      "Epoch: 8179 \t|| Train Loss: 0.02821289299305671 \t|| Test Loss: 0.06590796938886015\n",
      "Epoch: 8180 \t|| Train Loss: 0.02817842271144232 \t|| Test Loss: 0.06579776939155316\n",
      "Epoch: 8181 \t|| Train Loss: 0.02814399199598539 \t|| Test Loss: 0.0657562593938629\n",
      "Epoch: 8182 \t|| Train Loss: 0.028109902714787603 \t|| Test Loss: 0.0656460593965559\n",
      "Epoch: 8183 \t|| Train Loss: 0.028075090998914077 \t|| Test Loss: 0.06560454939886566\n",
      "Epoch: 8184 \t|| Train Loss: 0.02804138271813287 \t|| Test Loss: 0.06549434940155867\n",
      "Epoch: 8185 \t|| Train Loss: 0.028006482719916542 \t|| Test Loss: 0.06538414940425166\n",
      "Epoch: 8186 \t|| Train Loss: 0.027972570003404373 \t|| Test Loss: 0.0653426394065614\n",
      "Epoch: 8187 \t|| Train Loss: 0.027937962723261812 \t|| Test Loss: 0.06523243940925441\n",
      "Epoch: 8188 \t|| Train Loss: 0.027903669006333044 \t|| Test Loss: 0.06519092941156415\n",
      "Epoch: 8189 \t|| Train Loss: 0.02786944272660709 \t|| Test Loss: 0.06508072941425716\n",
      "Epoch: 8190 \t|| Train Loss: 0.027834768009261733 \t|| Test Loss: 0.06503921941656689\n",
      "Epoch: 8191 \t|| Train Loss: 0.02780092272995236 \t|| Test Loss: 0.06492901941925991\n",
      "Epoch: 8192 \t|| Train Loss: 0.027766022731736027 \t|| Test Loss: 0.06481881942195292\n",
      "Epoch: 8193 \t|| Train Loss: 0.027732247013752008 \t|| Test Loss: 0.06477730942426267\n",
      "Epoch: 8194 \t|| Train Loss: 0.027697502735081297 \t|| Test Loss: 0.06466710942695568\n",
      "Epoch: 8195 \t|| Train Loss: 0.027663346016680707 \t|| Test Loss: 0.06462559942926542\n",
      "Epoch: 8196 \t|| Train Loss: 0.027628982738426577 \t|| Test Loss: 0.06451539943195841\n",
      "Epoch: 8197 \t|| Train Loss: 0.027594445019609382 \t|| Test Loss: 0.06447388943426818\n",
      "Epoch: 8198 \t|| Train Loss: 0.027560462741771864 \t|| Test Loss: 0.06436368943696116\n",
      "Epoch: 8199 \t|| Train Loss: 0.027525562743555533 \t|| Test Loss: 0.06425348943965417\n",
      "Epoch: 8200 \t|| Train Loss: 0.027491924024099678 \t|| Test Loss: 0.0642119794419639\n",
      "Epoch: 8201 \t|| Train Loss: 0.027457042746900785 \t|| Test Loss: 0.06410177944465692\n",
      "Epoch: 8202 \t|| Train Loss: 0.027423023027028353 \t|| Test Loss: 0.06406026944696668\n",
      "Epoch: 8203 \t|| Train Loss: 0.02738852275024607 \t|| Test Loss: 0.06395006944965966\n",
      "Epoch: 8204 \t|| Train Loss: 0.02735412202995704 \t|| Test Loss: 0.06390855945196941\n",
      "Epoch: 8205 \t|| Train Loss: 0.027320002753591345 \t|| Test Loss: 0.06379835945466242\n",
      "Epoch: 8206 \t|| Train Loss: 0.02728522103288573 \t|| Test Loss: 0.06375684945697216\n",
      "Epoch: 8207 \t|| Train Loss: 0.027251482756936618 \t|| Test Loss: 0.06364664945966518\n",
      "Epoch: 8208 \t|| Train Loss: 0.02721658275872028 \t|| Test Loss: 0.06353644946235817\n",
      "Epoch: 8209 \t|| Train Loss: 0.027182700037376006 \t|| Test Loss: 0.06349493946466793\n",
      "Epoch: 8210 \t|| Train Loss: 0.02714806276206556 \t|| Test Loss: 0.06338473946736092\n",
      "Epoch: 8211 \t|| Train Loss: 0.027113799040304698 \t|| Test Loss: 0.06334322946967065\n",
      "Epoch: 8212 \t|| Train Loss: 0.02707954276541083 \t|| Test Loss: 0.06323302947236369\n",
      "Epoch: 8213 \t|| Train Loss: 0.027044898043233366 \t|| Test Loss: 0.06319151947467341\n",
      "Epoch: 8214 \t|| Train Loss: 0.02701102276875611 \t|| Test Loss: 0.06308131947736642\n",
      "Epoch: 8215 \t|| Train Loss: 0.026976122770539783 \t|| Test Loss: 0.06297111948005943\n",
      "Epoch: 8216 \t|| Train Loss: 0.026942377047723665 \t|| Test Loss: 0.06292960948236917\n",
      "Epoch: 8217 \t|| Train Loss: 0.02690760277388506 \t|| Test Loss: 0.06281940948506218\n",
      "Epoch: 8218 \t|| Train Loss: 0.026873476050652344 \t|| Test Loss: 0.06277789948737195\n",
      "Epoch: 8219 \t|| Train Loss: 0.02683908277723033 \t|| Test Loss: 0.06266769949006493\n",
      "Epoch: 8220 \t|| Train Loss: 0.026804575053581033 \t|| Test Loss: 0.06262618949237468\n",
      "Epoch: 8221 \t|| Train Loss: 0.026770562780575595 \t|| Test Loss: 0.06251598949506765\n",
      "Epoch: 8222 \t|| Train Loss: 0.026735674056509708 \t|| Test Loss: 0.06247447949737743\n",
      "Epoch: 8223 \t|| Train Loss: 0.026702042783920875 \t|| Test Loss: 0.06236427950007043\n",
      "Epoch: 8224 \t|| Train Loss: 0.02666714278570454 \t|| Test Loss: 0.062254079502763425\n",
      "Epoch: 8225 \t|| Train Loss: 0.026633153061000003 \t|| Test Loss: 0.062212569505073166\n",
      "Epoch: 8226 \t|| Train Loss: 0.02659862278904982 \t|| Test Loss: 0.062102369507766174\n",
      "Epoch: 8227 \t|| Train Loss: 0.026564252063928685 \t|| Test Loss: 0.06206085951007593\n",
      "Epoch: 8228 \t|| Train Loss: 0.026530102792395083 \t|| Test Loss: 0.06195065951276896\n",
      "Epoch: 8229 \t|| Train Loss: 0.02649535106685736 \t|| Test Loss: 0.061909149515078685\n",
      "Epoch: 8230 \t|| Train Loss: 0.026461582795740367 \t|| Test Loss: 0.061798949517771685\n",
      "Epoch: 8231 \t|| Train Loss: 0.026426682797524047 \t|| Test Loss: 0.06168874952046468\n",
      "Epoch: 8232 \t|| Train Loss: 0.026392830071347663 \t|| Test Loss: 0.061647239522774434\n",
      "Epoch: 8233 \t|| Train Loss: 0.026358162800869306 \t|| Test Loss: 0.06153703952546746\n",
      "Epoch: 8234 \t|| Train Loss: 0.02632392907427633 \t|| Test Loss: 0.06149552952777718\n",
      "Epoch: 8235 \t|| Train Loss: 0.02628964280421458 \t|| Test Loss: 0.06138532953047018\n",
      "Epoch: 8236 \t|| Train Loss: 0.026255028077205016 \t|| Test Loss: 0.06134381953277992\n",
      "Epoch: 8237 \t|| Train Loss: 0.02622112280755986 \t|| Test Loss: 0.06123361953547292\n",
      "Epoch: 8238 \t|| Train Loss: 0.026186222809343525 \t|| Test Loss: 0.061123419538165934\n",
      "Epoch: 8239 \t|| Train Loss: 0.026152507081695302 \t|| Test Loss: 0.06108190954047569\n",
      "Epoch: 8240 \t|| Train Loss: 0.02611770281268879 \t|| Test Loss: 0.06097170954316868\n",
      "Epoch: 8241 \t|| Train Loss: 0.026083606084623977 \t|| Test Loss: 0.060930199545478424\n",
      "Epoch: 8242 \t|| Train Loss: 0.026049182816034067 \t|| Test Loss: 0.060819999548171425\n",
      "Epoch: 8243 \t|| Train Loss: 0.02601470508755268 \t|| Test Loss: 0.06077848955048119\n",
      "Epoch: 8244 \t|| Train Loss: 0.02598066281937934 \t|| Test Loss: 0.060668289553174194\n",
      "Epoch: 8245 \t|| Train Loss: 0.02594580409048135 \t|| Test Loss: 0.06062677955548392\n",
      "Epoch: 8246 \t|| Train Loss: 0.02591214282272461 \t|| Test Loss: 0.06051657955817693\n",
      "Epoch: 8247 \t|| Train Loss: 0.02587724282450829 \t|| Test Loss: 0.06040637956086995\n",
      "Epoch: 8248 \t|| Train Loss: 0.025843283094971643 \t|| Test Loss: 0.06036486956317968\n",
      "Epoch: 8249 \t|| Train Loss: 0.02580872282785356 \t|| Test Loss: 0.0602546695658727\n",
      "Epoch: 8250 \t|| Train Loss: 0.025774382097900318 \t|| Test Loss: 0.06021315956818243\n",
      "Epoch: 8251 \t|| Train Loss: 0.02574020283119884 \t|| Test Loss: 0.06010295957087543\n",
      "Epoch: 8252 \t|| Train Loss: 0.025705481100829007 \t|| Test Loss: 0.06006144957318519\n",
      "Epoch: 8253 \t|| Train Loss: 0.025671682834544123 \t|| Test Loss: 0.059951249575878184\n",
      "Epoch: 8254 \t|| Train Loss: 0.025636782836327775 \t|| Test Loss: 0.05984104957857119\n",
      "Epoch: 8255 \t|| Train Loss: 0.025602960105319296 \t|| Test Loss: 0.05979953958088093\n",
      "Epoch: 8256 \t|| Train Loss: 0.02556826283967305 \t|| Test Loss: 0.05968933958357394\n",
      "Epoch: 8257 \t|| Train Loss: 0.025534059108247974 \t|| Test Loss: 0.059647829585883695\n",
      "Epoch: 8258 \t|| Train Loss: 0.025499742843018324 \t|| Test Loss: 0.05953762958857668\n",
      "Epoch: 8259 \t|| Train Loss: 0.025465158111176674 \t|| Test Loss: 0.05949611959088643\n",
      "Epoch: 8260 \t|| Train Loss: 0.025431222846363604 \t|| Test Loss: 0.05938591959357945\n",
      "Epoch: 8261 \t|| Train Loss: 0.025396322848147267 \t|| Test Loss: 0.059275719596272446\n",
      "Epoch: 8262 \t|| Train Loss: 0.025362637115666952 \t|| Test Loss: 0.05923420959858218\n",
      "Epoch: 8263 \t|| Train Loss: 0.025327802851492547 \t|| Test Loss: 0.05912400960127519\n",
      "Epoch: 8264 \t|| Train Loss: 0.025293736118595644 \t|| Test Loss: 0.05908249960358495\n",
      "Epoch: 8265 \t|| Train Loss: 0.025259282854837816 \t|| Test Loss: 0.05897229960627796\n",
      "Epoch: 8266 \t|| Train Loss: 0.025224835121524302 \t|| Test Loss: 0.05893078960858768\n",
      "Epoch: 8267 \t|| Train Loss: 0.025190762858183093 \t|| Test Loss: 0.05882058961128069\n",
      "Epoch: 8268 \t|| Train Loss: 0.025155934124453 \t|| Test Loss: 0.05877907961359043\n",
      "Epoch: 8269 \t|| Train Loss: 0.025122242861528365 \t|| Test Loss: 0.05866887961628344\n",
      "Epoch: 8270 \t|| Train Loss: 0.02508734286331204 \t|| Test Loss: 0.05855867961897647\n",
      "Epoch: 8271 \t|| Train Loss: 0.025053413128943297 \t|| Test Loss: 0.058517169621286204\n",
      "Epoch: 8272 \t|| Train Loss: 0.025018822866657308 \t|| Test Loss: 0.05840696962397919\n",
      "Epoch: 8273 \t|| Train Loss: 0.024984512131871965 \t|| Test Loss: 0.05836545962628894\n",
      "Epoch: 8274 \t|| Train Loss: 0.0249503028700026 \t|| Test Loss: 0.05825525962898195\n",
      "Epoch: 8275 \t|| Train Loss: 0.024915611134800657 \t|| Test Loss: 0.05821374963129168\n",
      "Epoch: 8276 \t|| Train Loss: 0.024881782873347864 \t|| Test Loss: 0.05810354963398466\n",
      "Epoch: 8277 \t|| Train Loss: 0.024846882875131527 \t|| Test Loss: 0.05799334963667768\n",
      "Epoch: 8278 \t|| Train Loss: 0.024813090139290943 \t|| Test Loss: 0.05795183963898744\n",
      "Epoch: 8279 \t|| Train Loss: 0.0247783628784768 \t|| Test Loss: 0.05784163964168043\n",
      "Epoch: 8280 \t|| Train Loss: 0.024744189142219625 \t|| Test Loss: 0.05780012964399019\n",
      "Epoch: 8281 \t|| Train Loss: 0.024709842881822087 \t|| Test Loss: 0.0576899296466832\n",
      "Epoch: 8282 \t|| Train Loss: 0.024675288145148307 \t|| Test Loss: 0.05764841964899294\n",
      "Epoch: 8283 \t|| Train Loss: 0.024641322885167342 \t|| Test Loss: 0.057538219651685964\n",
      "Epoch: 8284 \t|| Train Loss: 0.024606422886951015 \t|| Test Loss: 0.05742801965437896\n",
      "Epoch: 8285 \t|| Train Loss: 0.024572767149638592 \t|| Test Loss: 0.05738650965668871\n",
      "Epoch: 8286 \t|| Train Loss: 0.02453790289029628 \t|| Test Loss: 0.0572763096593817\n",
      "Epoch: 8287 \t|| Train Loss: 0.024503866152567277 \t|| Test Loss: 0.05723479966169144\n",
      "Epoch: 8288 \t|| Train Loss: 0.024469382893641565 \t|| Test Loss: 0.057124599664384455\n",
      "Epoch: 8289 \t|| Train Loss: 0.02443496515549596 \t|| Test Loss: 0.05708308966669419\n",
      "Epoch: 8290 \t|| Train Loss: 0.02440086289698684 \t|| Test Loss: 0.0569728896693872\n",
      "Epoch: 8291 \t|| Train Loss: 0.024366064158424645 \t|| Test Loss: 0.056931379671696945\n",
      "Epoch: 8292 \t|| Train Loss: 0.024332342900332114 \t|| Test Loss: 0.056821179674389946\n",
      "Epoch: 8293 \t|| Train Loss: 0.02429744290211578 \t|| Test Loss: 0.056710979677082954\n",
      "Epoch: 8294 \t|| Train Loss: 0.024263543162914923 \t|| Test Loss: 0.0566694696793927\n",
      "Epoch: 8295 \t|| Train Loss: 0.02422892290546105 \t|| Test Loss: 0.0565592696820857\n",
      "Epoch: 8296 \t|| Train Loss: 0.024194642165843615 \t|| Test Loss: 0.05651775968439545\n",
      "Epoch: 8297 \t|| Train Loss: 0.024160402908806326 \t|| Test Loss: 0.05640755968708844\n",
      "Epoch: 8298 \t|| Train Loss: 0.024125741168772297 \t|| Test Loss: 0.0563660496893982\n",
      "Epoch: 8299 \t|| Train Loss: 0.024091882912151603 \t|| Test Loss: 0.05625584969209119\n",
      "Epoch: 8300 \t|| Train Loss: 0.024056982913935272 \t|| Test Loss: 0.05614564969478421\n",
      "Epoch: 8301 \t|| Train Loss: 0.024023220173262583 \t|| Test Loss: 0.056104139697093956\n",
      "Epoch: 8302 \t|| Train Loss: 0.023988462917280545 \t|| Test Loss: 0.05599393969978698\n",
      "Epoch: 8303 \t|| Train Loss: 0.023954319176191268 \t|| Test Loss: 0.0559524297020967\n",
      "Epoch: 8304 \t|| Train Loss: 0.023919942920625818 \t|| Test Loss: 0.055842229704789706\n",
      "Epoch: 8305 \t|| Train Loss: 0.023885418179119947 \t|| Test Loss: 0.05580071970709945\n",
      "Epoch: 8306 \t|| Train Loss: 0.023851422923971088 \t|| Test Loss: 0.055690519709792455\n",
      "Epoch: 8307 \t|| Train Loss: 0.023816522925754768 \t|| Test Loss: 0.05558031971248546\n",
      "Epoch: 8308 \t|| Train Loss: 0.023782897183610225 \t|| Test Loss: 0.055538809714795204\n",
      "Epoch: 8309 \t|| Train Loss: 0.023748002929100037 \t|| Test Loss: 0.0554286097174882\n",
      "Epoch: 8310 \t|| Train Loss: 0.02371399618653892 \t|| Test Loss: 0.055387099719797925\n",
      "Epoch: 8311 \t|| Train Loss: 0.0236794829324453 \t|| Test Loss: 0.055276899722490946\n",
      "Epoch: 8312 \t|| Train Loss: 0.023645095189467606 \t|| Test Loss: 0.05523538972480072\n",
      "Epoch: 8313 \t|| Train Loss: 0.023610962935790587 \t|| Test Loss: 0.055125189727493695\n",
      "Epoch: 8314 \t|| Train Loss: 0.02357619419239629 \t|| Test Loss: 0.05508367972980345\n",
      "Epoch: 8315 \t|| Train Loss: 0.02354244293913586 \t|| Test Loss: 0.05497347973249646\n",
      "Epoch: 8316 \t|| Train Loss: 0.023507542940919536 \t|| Test Loss: 0.054863279735189466\n",
      "Epoch: 8317 \t|| Train Loss: 0.02347367319688658 \t|| Test Loss: 0.054821769737499214\n",
      "Epoch: 8318 \t|| Train Loss: 0.023439022944264795 \t|| Test Loss: 0.054711569740192215\n",
      "Epoch: 8319 \t|| Train Loss: 0.02340477219981526 \t|| Test Loss: 0.054670059742501956\n",
      "Epoch: 8320 \t|| Train Loss: 0.02337050294761007 \t|| Test Loss: 0.05455985974519495\n",
      "Epoch: 8321 \t|| Train Loss: 0.023335871202743937 \t|| Test Loss: 0.05451834974750469\n",
      "Epoch: 8322 \t|| Train Loss: 0.02330198295095535 \t|| Test Loss: 0.05440814975019772\n",
      "Epoch: 8323 \t|| Train Loss: 0.02326708295273902 \t|| Test Loss: 0.054297949752890706\n",
      "Epoch: 8324 \t|| Train Loss: 0.023233350207234233 \t|| Test Loss: 0.05425643975520045\n",
      "Epoch: 8325 \t|| Train Loss: 0.023198562956084294 \t|| Test Loss: 0.05414623975789347\n",
      "Epoch: 8326 \t|| Train Loss: 0.023164449210162925 \t|| Test Loss: 0.05410472976020322\n",
      "Epoch: 8327 \t|| Train Loss: 0.02313004295942958 \t|| Test Loss: 0.05399452976289622\n",
      "Epoch: 8328 \t|| Train Loss: 0.023095548213091593 \t|| Test Loss: 0.05395301976520598\n",
      "Epoch: 8329 \t|| Train Loss: 0.02306152296277485 \t|| Test Loss: 0.053842819767898974\n",
      "Epoch: 8330 \t|| Train Loss: 0.023026647216020282 \t|| Test Loss: 0.05380130977020871\n",
      "Epoch: 8331 \t|| Train Loss: 0.022993002966120113 \t|| Test Loss: 0.053691109772901716\n",
      "Epoch: 8332 \t|| Train Loss: 0.022958102967903786 \t|| Test Loss: 0.05358090977559471\n",
      "Epoch: 8333 \t|| Train Loss: 0.022924126220510578 \t|| Test Loss: 0.053539399777904464\n",
      "Epoch: 8334 \t|| Train Loss: 0.022889582971249055 \t|| Test Loss: 0.05342919978059746\n",
      "Epoch: 8335 \t|| Train Loss: 0.02285522522343925 \t|| Test Loss: 0.0533876897829072\n",
      "Epoch: 8336 \t|| Train Loss: 0.022821062974594335 \t|| Test Loss: 0.05327748978560021\n",
      "Epoch: 8337 \t|| Train Loss: 0.02278632422636794 \t|| Test Loss: 0.05323597978790995\n",
      "Epoch: 8338 \t|| Train Loss: 0.022752542977939605 \t|| Test Loss: 0.05312577979060297\n",
      "Epoch: 8339 \t|| Train Loss: 0.022717642979723274 \t|| Test Loss: 0.05301557979329598\n",
      "Epoch: 8340 \t|| Train Loss: 0.02268380323085822 \t|| Test Loss: 0.052974069795605726\n",
      "Epoch: 8341 \t|| Train Loss: 0.02264912298306856 \t|| Test Loss: 0.0528638697982987\n",
      "Epoch: 8342 \t|| Train Loss: 0.022614902233786906 \t|| Test Loss: 0.05282235980060847\n",
      "Epoch: 8343 \t|| Train Loss: 0.022580602986413824 \t|| Test Loss: 0.052712159803301496\n",
      "Epoch: 8344 \t|| Train Loss: 0.02254600123671559 \t|| Test Loss: 0.05267064980561122\n",
      "Epoch: 8345 \t|| Train Loss: 0.022512082989759097 \t|| Test Loss: 0.05256044980830421\n",
      "Epoch: 8346 \t|| Train Loss: 0.02247718299154277 \t|| Test Loss: 0.05245024981099722\n",
      "Epoch: 8347 \t|| Train Loss: 0.02244348024120587 \t|| Test Loss: 0.052408739813306945\n",
      "Epoch: 8348 \t|| Train Loss: 0.022408662994888036 \t|| Test Loss: 0.05229853981599997\n",
      "Epoch: 8349 \t|| Train Loss: 0.022374579244134555 \t|| Test Loss: 0.05225702981830973\n",
      "Epoch: 8350 \t|| Train Loss: 0.022340142998233312 \t|| Test Loss: 0.052146829821002716\n",
      "Epoch: 8351 \t|| Train Loss: 0.022305678247063226 \t|| Test Loss: 0.05210531982331246\n",
      "Epoch: 8352 \t|| Train Loss: 0.022271623001578582 \t|| Test Loss: 0.05199511982600545\n",
      "Epoch: 8353 \t|| Train Loss: 0.022236777249991922 \t|| Test Loss: 0.05195360982831522\n",
      "Epoch: 8354 \t|| Train Loss: 0.02220310300492386 \t|| Test Loss: 0.05184340983100823\n",
      "Epoch: 8355 \t|| Train Loss: 0.022168203006707528 \t|| Test Loss: 0.051733209833701235\n",
      "Epoch: 8356 \t|| Train Loss: 0.02213425625448221 \t|| Test Loss: 0.05169169983601095\n",
      "Epoch: 8357 \t|| Train Loss: 0.022099683010052808 \t|| Test Loss: 0.05158149983870399\n",
      "Epoch: 8358 \t|| Train Loss: 0.022065355257410896 \t|| Test Loss: 0.05153998984101371\n",
      "Epoch: 8359 \t|| Train Loss: 0.022031163013398074 \t|| Test Loss: 0.05142978984370673\n",
      "Epoch: 8360 \t|| Train Loss: 0.02199645426033957 \t|| Test Loss: 0.05138827984601646\n",
      "Epoch: 8361 \t|| Train Loss: 0.02196264301674335 \t|| Test Loss: 0.051278079848709454\n",
      "Epoch: 8362 \t|| Train Loss: 0.02192774301852702 \t|| Test Loss: 0.05116787985140246\n",
      "Epoch: 8363 \t|| Train Loss: 0.021893933264829864 \t|| Test Loss: 0.05112636985371222\n",
      "Epoch: 8364 \t|| Train Loss: 0.021859223021872293 \t|| Test Loss: 0.051016169856405225\n",
      "Epoch: 8365 \t|| Train Loss: 0.02182503226775855 \t|| Test Loss: 0.050974659858714966\n",
      "Epoch: 8366 \t|| Train Loss: 0.021790703025217566 \t|| Test Loss: 0.050864459861407973\n",
      "Epoch: 8367 \t|| Train Loss: 0.021756131270687228 \t|| Test Loss: 0.050822949863717735\n",
      "Epoch: 8368 \t|| Train Loss: 0.021722183028562835 \t|| Test Loss: 0.05071274986641072\n",
      "Epoch: 8369 \t|| Train Loss: 0.02168728303034651 \t|| Test Loss: 0.05060254986910373\n",
      "Epoch: 8370 \t|| Train Loss: 0.021653610275177516 \t|| Test Loss: 0.050561039871413485\n",
      "Epoch: 8371 \t|| Train Loss: 0.021618763033691802 \t|| Test Loss: 0.05045083987410648\n",
      "Epoch: 8372 \t|| Train Loss: 0.021584709278106202 \t|| Test Loss: 0.050409329876416206\n",
      "Epoch: 8373 \t|| Train Loss: 0.021550243037037058 \t|| Test Loss: 0.050299129879109214\n",
      "Epoch: 8374 \t|| Train Loss: 0.021515808281034894 \t|| Test Loss: 0.05025761988141897\n",
      "Epoch: 8375 \t|| Train Loss: 0.021481723040382334 \t|| Test Loss: 0.05014741988411198\n",
      "Epoch: 8376 \t|| Train Loss: 0.021446907283963555 \t|| Test Loss: 0.050105909886421704\n",
      "Epoch: 8377 \t|| Train Loss: 0.021413203043727604 \t|| Test Loss: 0.049995709889114726\n",
      "Epoch: 8378 \t|| Train Loss: 0.021378303045511277 \t|| Test Loss: 0.04988550989180775\n",
      "Epoch: 8379 \t|| Train Loss: 0.021344386288453858 \t|| Test Loss: 0.049843999894117474\n",
      "Epoch: 8380 \t|| Train Loss: 0.02130978304885655 \t|| Test Loss: 0.0497337998968105\n",
      "Epoch: 8381 \t|| Train Loss: 0.02127548529138254 \t|| Test Loss: 0.04969228989912024\n",
      "Epoch: 8382 \t|| Train Loss: 0.02124126305220183 \t|| Test Loss: 0.04958208990181322\n",
      "Epoch: 8383 \t|| Train Loss: 0.021206584294311222 \t|| Test Loss: 0.04954057990412297\n",
      "Epoch: 8384 \t|| Train Loss: 0.021172743055547106 \t|| Test Loss: 0.04943037990681598\n",
      "Epoch: 8385 \t|| Train Loss: 0.02113784305733076 \t|| Test Loss: 0.04932017990950899\n",
      "Epoch: 8386 \t|| Train Loss: 0.021104063298801518 \t|| Test Loss: 0.049278669911818715\n",
      "Epoch: 8387 \t|| Train Loss: 0.021069323060676038 \t|| Test Loss: 0.049168469914511716\n",
      "Epoch: 8388 \t|| Train Loss: 0.021035162301730203 \t|| Test Loss: 0.049126959916821464\n",
      "Epoch: 8389 \t|| Train Loss: 0.02100080306402131 \t|| Test Loss: 0.04901675991951447\n",
      "Epoch: 8390 \t|| Train Loss: 0.02096626130465888 \t|| Test Loss: 0.04897524992182423\n",
      "Epoch: 8391 \t|| Train Loss: 0.020932283067366598 \t|| Test Loss: 0.048865049924517234\n",
      "Epoch: 8392 \t|| Train Loss: 0.02089738306915026 \t|| Test Loss: 0.04875484992721023\n",
      "Epoch: 8393 \t|| Train Loss: 0.02086374030914916 \t|| Test Loss: 0.04871333992951997\n",
      "Epoch: 8394 \t|| Train Loss: 0.02082886307249553 \t|| Test Loss: 0.04860313993221299\n",
      "Epoch: 8395 \t|| Train Loss: 0.020794839312077838 \t|| Test Loss: 0.048561629934522746\n",
      "Epoch: 8396 \t|| Train Loss: 0.0207603430758408 \t|| Test Loss: 0.04845142993721572\n",
      "Epoch: 8397 \t|| Train Loss: 0.020725938315006534 \t|| Test Loss: 0.04840991993952547\n",
      "Epoch: 8398 \t|| Train Loss: 0.020691823079186083 \t|| Test Loss: 0.04829971994221849\n",
      "Epoch: 8399 \t|| Train Loss: 0.020657037317935213 \t|| Test Loss: 0.04825820994452823\n",
      "Epoch: 8400 \t|| Train Loss: 0.020623303082531352 \t|| Test Loss: 0.048148009947221224\n",
      "Epoch: 8401 \t|| Train Loss: 0.020588403084315025 \t|| Test Loss: 0.048037809949914245\n",
      "Epoch: 8402 \t|| Train Loss: 0.020554516322425494 \t|| Test Loss: 0.047996299952223986\n",
      "Epoch: 8403 \t|| Train Loss: 0.0205198830876603 \t|| Test Loss: 0.04788609995491698\n",
      "Epoch: 8404 \t|| Train Loss: 0.020485615325354183 \t|| Test Loss: 0.047844589957226756\n",
      "Epoch: 8405 \t|| Train Loss: 0.02045136309100557 \t|| Test Loss: 0.04773438995991974\n",
      "Epoch: 8406 \t|| Train Loss: 0.02041671432828287 \t|| Test Loss: 0.04769287996222947\n",
      "Epoch: 8407 \t|| Train Loss: 0.02038284309435084 \t|| Test Loss: 0.04758267996492248\n",
      "Epoch: 8408 \t|| Train Loss: 0.020347943096134514 \t|| Test Loss: 0.047472479967615486\n",
      "Epoch: 8409 \t|| Train Loss: 0.02031419333277315 \t|| Test Loss: 0.04743096996992523\n",
      "Epoch: 8410 \t|| Train Loss: 0.020279423099479797 \t|| Test Loss: 0.047320769972618235\n",
      "Epoch: 8411 \t|| Train Loss: 0.020245292335701832 \t|| Test Loss: 0.04727925997492797\n",
      "Epoch: 8412 \t|| Train Loss: 0.02021090310282506 \t|| Test Loss: 0.04716905997762101\n",
      "Epoch: 8413 \t|| Train Loss: 0.020176391338630514 \t|| Test Loss: 0.047127549979930725\n",
      "Epoch: 8414 \t|| Train Loss: 0.02014238310617033 \t|| Test Loss: 0.047017349982623746\n",
      "Epoch: 8415 \t|| Train Loss: 0.020107490341559203 \t|| Test Loss: 0.04697583998493349\n",
      "Epoch: 8416 \t|| Train Loss: 0.020073863109515613 \t|| Test Loss: 0.04686563998762648\n",
      "Epoch: 8417 \t|| Train Loss: 0.020038963111299275 \t|| Test Loss: 0.04675543999031949\n",
      "Epoch: 8418 \t|| Train Loss: 0.020004969346049485 \t|| Test Loss: 0.046713929992629244\n",
      "Epoch: 8419 \t|| Train Loss: 0.019970443114644555 \t|| Test Loss: 0.04660372999532224\n",
      "Epoch: 8420 \t|| Train Loss: 0.019936068348978174 \t|| Test Loss: 0.04656221999763197\n",
      "Epoch: 8421 \t|| Train Loss: 0.01990192311798981 \t|| Test Loss: 0.04645202000032498\n",
      "Epoch: 8422 \t|| Train Loss: 0.019867167351906863 \t|| Test Loss: 0.04641051000263474\n",
      "Epoch: 8423 \t|| Train Loss: 0.019833403121335098 \t|| Test Loss: 0.04630031000532773\n",
      "Epoch: 8424 \t|| Train Loss: 0.01979850312311877 \t|| Test Loss: 0.04619011000802074\n",
      "Epoch: 8425 \t|| Train Loss: 0.01976464635639714 \t|| Test Loss: 0.0461486000103305\n",
      "Epoch: 8426 \t|| Train Loss: 0.01972998312646405 \t|| Test Loss: 0.04603840001302349\n",
      "Epoch: 8427 \t|| Train Loss: 0.01969574535932584 \t|| Test Loss: 0.04599689001533323\n",
      "Epoch: 8428 \t|| Train Loss: 0.019661463129809313 \t|| Test Loss: 0.04588669001802624\n",
      "Epoch: 8429 \t|| Train Loss: 0.01962684436225451 \t|| Test Loss: 0.04584518002033598\n",
      "Epoch: 8430 \t|| Train Loss: 0.019592943133154593 \t|| Test Loss: 0.04573498002302899\n",
      "Epoch: 8431 \t|| Train Loss: 0.019558043134938266 \t|| Test Loss: 0.04562478002572199\n",
      "Epoch: 8432 \t|| Train Loss: 0.0195243233667448 \t|| Test Loss: 0.04558327002803174\n",
      "Epoch: 8433 \t|| Train Loss: 0.019489523138283525 \t|| Test Loss: 0.045473070030724747\n",
      "Epoch: 8434 \t|| Train Loss: 0.01945542236967348 \t|| Test Loss: 0.04543156003303449\n",
      "Epoch: 8435 \t|| Train Loss: 0.019421003141628812 \t|| Test Loss: 0.04532136003572751\n",
      "Epoch: 8436 \t|| Train Loss: 0.019386521372602168 \t|| Test Loss: 0.045279850038037264\n",
      "Epoch: 8437 \t|| Train Loss: 0.019352483144974085 \t|| Test Loss: 0.04516965004073024\n",
      "Epoch: 8438 \t|| Train Loss: 0.019317620375530854 \t|| Test Loss: 0.04512814004304\n",
      "Epoch: 8439 \t|| Train Loss: 0.01928396314831936 \t|| Test Loss: 0.04501794004573301\n",
      "Epoch: 8440 \t|| Train Loss: 0.019249063150103024 \t|| Test Loss: 0.044907740048426015\n",
      "Epoch: 8441 \t|| Train Loss: 0.019215099380021132 \t|| Test Loss: 0.04486623005073574\n",
      "Epoch: 8442 \t|| Train Loss: 0.019180543153448297 \t|| Test Loss: 0.04475603005342873\n",
      "Epoch: 8443 \t|| Train Loss: 0.01914619838294982 \t|| Test Loss: 0.04471452005573849\n",
      "Epoch: 8444 \t|| Train Loss: 0.019112023156793567 \t|| Test Loss: 0.04460432005843149\n",
      "Epoch: 8445 \t|| Train Loss: 0.01907729738587851 \t|| Test Loss: 0.04456281006074123\n",
      "Epoch: 8446 \t|| Train Loss: 0.01904350316013885 \t|| Test Loss: 0.04445261006343424\n",
      "Epoch: 8447 \t|| Train Loss: 0.019008603161922516 \t|| Test Loss: 0.04434241006612725\n",
      "Epoch: 8448 \t|| Train Loss: 0.018974776390368802 \t|| Test Loss: 0.044300900068436996\n",
      "Epoch: 8449 \t|| Train Loss: 0.018940083165267785 \t|| Test Loss: 0.044190700071130004\n",
      "Epoch: 8450 \t|| Train Loss: 0.018905875393297474 \t|| Test Loss: 0.044149190073439745\n",
      "Epoch: 8451 \t|| Train Loss: 0.01887156316861307 \t|| Test Loss: 0.044038990076132746\n",
      "Epoch: 8452 \t|| Train Loss: 0.018836974396226162 \t|| Test Loss: 0.043997480078442494\n",
      "Epoch: 8453 \t|| Train Loss: 0.01880304317195834 \t|| Test Loss: 0.04388728008113553\n",
      "Epoch: 8454 \t|| Train Loss: 0.018768143173742004 \t|| Test Loss: 0.04377708008382851\n",
      "Epoch: 8455 \t|| Train Loss: 0.01873445340071645 \t|| Test Loss: 0.043735570086138244\n",
      "Epoch: 8456 \t|| Train Loss: 0.01869962317708728 \t|| Test Loss: 0.04362537008883126\n",
      "Epoch: 8457 \t|| Train Loss: 0.018665552403645126 \t|| Test Loss: 0.04358386009114099\n",
      "Epoch: 8458 \t|| Train Loss: 0.018631103180432554 \t|| Test Loss: 0.043473660093833986\n",
      "Epoch: 8459 \t|| Train Loss: 0.018596651406573805 \t|| Test Loss: 0.04343215009614376\n",
      "Epoch: 8460 \t|| Train Loss: 0.018562583183777827 \t|| Test Loss: 0.04332195009883675\n",
      "Epoch: 8461 \t|| Train Loss: 0.01852775040950248 \t|| Test Loss: 0.04328044010114649\n",
      "Epoch: 8462 \t|| Train Loss: 0.0184940631871231 \t|| Test Loss: 0.0431702401038395\n",
      "Epoch: 8463 \t|| Train Loss: 0.018459163188906776 \t|| Test Loss: 0.043060040106532506\n",
      "Epoch: 8464 \t|| Train Loss: 0.01842522941399279 \t|| Test Loss: 0.043018530108842254\n",
      "Epoch: 8465 \t|| Train Loss: 0.01839064319225204 \t|| Test Loss: 0.04290833011153526\n",
      "Epoch: 8466 \t|| Train Loss: 0.018356328416921457 \t|| Test Loss: 0.04286682011384498\n",
      "Epoch: 8467 \t|| Train Loss: 0.01832212319559732 \t|| Test Loss: 0.042756620116538024\n",
      "Epoch: 8468 \t|| Train Loss: 0.018287427419850146 \t|| Test Loss: 0.042715110118847745\n",
      "Epoch: 8469 \t|| Train Loss: 0.018253603198942592 \t|| Test Loss: 0.04260491012154076\n",
      "Epoch: 8470 \t|| Train Loss: 0.018218703200726254 \t|| Test Loss: 0.04249471012423376\n",
      "Epoch: 8471 \t|| Train Loss: 0.01818490642434044 \t|| Test Loss: 0.04245320012654349\n",
      "Epoch: 8472 \t|| Train Loss: 0.018150183204071534 \t|| Test Loss: 0.04234300012923651\n",
      "Epoch: 8473 \t|| Train Loss: 0.01811600542726912 \t|| Test Loss: 0.04230149013154626\n",
      "Epoch: 8474 \t|| Train Loss: 0.01808166320741681 \t|| Test Loss: 0.04219129013423926\n",
      "Epoch: 8475 \t|| Train Loss: 0.018047104430197802 \t|| Test Loss: 0.042149780136548985\n",
      "Epoch: 8476 \t|| Train Loss: 0.018013143210762084 \t|| Test Loss: 0.04203958013924199\n",
      "Epoch: 8477 \t|| Train Loss: 0.01797824321254575 \t|| Test Loss: 0.041929380141935\n",
      "Epoch: 8478 \t|| Train Loss: 0.01794458343468809 \t|| Test Loss: 0.041887870144244756\n",
      "Epoch: 8479 \t|| Train Loss: 0.017909723215891033 \t|| Test Loss: 0.04177767014693776\n",
      "Epoch: 8480 \t|| Train Loss: 0.017875682437616773 \t|| Test Loss: 0.04173616014924751\n",
      "Epoch: 8481 \t|| Train Loss: 0.017841203219236296 \t|| Test Loss: 0.04162596015194051\n",
      "Epoch: 8482 \t|| Train Loss: 0.017806781440545455 \t|| Test Loss: 0.04158445015425024\n",
      "Epoch: 8483 \t|| Train Loss: 0.017772683222581576 \t|| Test Loss: 0.04147425015694325\n",
      "Epoch: 8484 \t|| Train Loss: 0.017737880443474144 \t|| Test Loss: 0.041432740159253\n",
      "Epoch: 8485 \t|| Train Loss: 0.01770416322592685 \t|| Test Loss: 0.04132254016194602\n",
      "Epoch: 8486 \t|| Train Loss: 0.0176692632277105 \t|| Test Loss: 0.041212340164639004\n",
      "Epoch: 8487 \t|| Train Loss: 0.017635359447964422 \t|| Test Loss: 0.04117083016694876\n",
      "Epoch: 8488 \t|| Train Loss: 0.01760074323105579 \t|| Test Loss: 0.04106063016964177\n",
      "Epoch: 8489 \t|| Train Loss: 0.017566458450893108 \t|| Test Loss: 0.041019120171951515\n",
      "Epoch: 8490 \t|| Train Loss: 0.017532223234401057 \t|| Test Loss: 0.040908920174644536\n",
      "Epoch: 8491 \t|| Train Loss: 0.017497557453821803 \t|| Test Loss: 0.04086741017695428\n",
      "Epoch: 8492 \t|| Train Loss: 0.017463703237746337 \t|| Test Loss: 0.04075721017964725\n",
      "Epoch: 8493 \t|| Train Loss: 0.017428803239530007 \t|| Test Loss: 0.04064701018234026\n",
      "Epoch: 8494 \t|| Train Loss: 0.017395036458312082 \t|| Test Loss: 0.04060550018465001\n",
      "Epoch: 8495 \t|| Train Loss: 0.01736028324287529 \t|| Test Loss: 0.04049530018734302\n",
      "Epoch: 8496 \t|| Train Loss: 0.017326135461240767 \t|| Test Loss: 0.04045379018965276\n",
      "Epoch: 8497 \t|| Train Loss: 0.017291763246220553 \t|| Test Loss: 0.04034359019234575\n",
      "Epoch: 8498 \t|| Train Loss: 0.01725723446416945 \t|| Test Loss: 0.04030208019465551\n",
      "Epoch: 8499 \t|| Train Loss: 0.01722324324956583 \t|| Test Loss: 0.040191880197348505\n",
      "Epoch: 8500 \t|| Train Loss: 0.017188343251349502 \t|| Test Loss: 0.040081680200041526\n",
      "Epoch: 8501 \t|| Train Loss: 0.01715471346865974 \t|| Test Loss: 0.040040170202351254\n",
      "Epoch: 8502 \t|| Train Loss: 0.017119823254694765 \t|| Test Loss: 0.039929970205044275\n",
      "Epoch: 8503 \t|| Train Loss: 0.01708581247158842 \t|| Test Loss: 0.039888460207354016\n",
      "Epoch: 8504 \t|| Train Loss: 0.017051303258040044 \t|| Test Loss: 0.039778260210047024\n",
      "Epoch: 8505 \t|| Train Loss: 0.0170169114745171 \t|| Test Loss: 0.039736750212356765\n",
      "Epoch: 8506 \t|| Train Loss: 0.016982783261385314 \t|| Test Loss: 0.03962655021504975\n",
      "Epoch: 8507 \t|| Train Loss: 0.016948010477445784 \t|| Test Loss: 0.039585040217359514\n",
      "Epoch: 8508 \t|| Train Loss: 0.016914263264730594 \t|| Test Loss: 0.03947484022005254\n",
      "Epoch: 8509 \t|| Train Loss: 0.016879363266514263 \t|| Test Loss: 0.03936464022274553\n",
      "Epoch: 8510 \t|| Train Loss: 0.016845489481936073 \t|| Test Loss: 0.03932313022505526\n",
      "Epoch: 8511 \t|| Train Loss: 0.016810843269859536 \t|| Test Loss: 0.03921293022774828\n",
      "Epoch: 8512 \t|| Train Loss: 0.016776588484864755 \t|| Test Loss: 0.03917142023005802\n",
      "Epoch: 8513 \t|| Train Loss: 0.016742323273204813 \t|| Test Loss: 0.039061220232751014\n",
      "Epoch: 8514 \t|| Train Loss: 0.016707687487793426 \t|| Test Loss: 0.03901971023506079\n",
      "Epoch: 8515 \t|| Train Loss: 0.016673803276550082 \t|| Test Loss: 0.038909510237753776\n",
      "Epoch: 8516 \t|| Train Loss: 0.016638903278333755 \t|| Test Loss: 0.03879931024044677\n",
      "Epoch: 8517 \t|| Train Loss: 0.01660516649228372 \t|| Test Loss: 0.03875780024275651\n",
      "Epoch: 8518 \t|| Train Loss: 0.01657038328167903 \t|| Test Loss: 0.03864760024544953\n",
      "Epoch: 8519 \t|| Train Loss: 0.016536265495212414 \t|| Test Loss: 0.038606090247759274\n",
      "Epoch: 8520 \t|| Train Loss: 0.016501863285024305 \t|| Test Loss: 0.03849589025045228\n",
      "Epoch: 8521 \t|| Train Loss: 0.016467364498141093 \t|| Test Loss: 0.038454380252762\n",
      "Epoch: 8522 \t|| Train Loss: 0.016433343288369574 \t|| Test Loss: 0.03834418025545504\n",
      "Epoch: 8523 \t|| Train Loss: 0.016398463501069774 \t|| Test Loss: 0.03830267025776476\n",
      "Epoch: 8524 \t|| Train Loss: 0.016364823291714847 \t|| Test Loss: 0.03819247026045778\n",
      "Epoch: 8525 \t|| Train Loss: 0.01632992329349852 \t|| Test Loss: 0.03808227026315077\n",
      "Epoch: 8526 \t|| Train Loss: 0.016295942505560053 \t|| Test Loss: 0.038040760265460515\n",
      "Epoch: 8527 \t|| Train Loss: 0.016261403296843793 \t|| Test Loss: 0.03793056026815352\n",
      "Epoch: 8528 \t|| Train Loss: 0.01622704150848873 \t|| Test Loss: 0.03788905027046328\n",
      "Epoch: 8529 \t|| Train Loss: 0.016192883300189066 \t|| Test Loss: 0.03777885027315627\n",
      "Epoch: 8530 \t|| Train Loss: 0.01615814051141743 \t|| Test Loss: 0.037737340275466005\n",
      "Epoch: 8531 \t|| Train Loss: 0.016124363303534332 \t|| Test Loss: 0.03762714027815901\n",
      "Epoch: 8532 \t|| Train Loss: 0.016089463305318012 \t|| Test Loss: 0.03751694028085202\n",
      "Epoch: 8533 \t|| Train Loss: 0.01605561951590772 \t|| Test Loss: 0.03747543028316177\n",
      "Epoch: 8534 \t|| Train Loss: 0.01602094330866329 \t|| Test Loss: 0.03736523028585478\n",
      "Epoch: 8535 \t|| Train Loss: 0.0159867185188364 \t|| Test Loss: 0.03732372028816453\n",
      "Epoch: 8536 \t|| Train Loss: 0.015952423312008558 \t|| Test Loss: 0.037213520290857525\n",
      "Epoch: 8537 \t|| Train Loss: 0.015917817521765083 \t|| Test Loss: 0.03717201029316726\n",
      "Epoch: 8538 \t|| Train Loss: 0.015883903315353824 \t|| Test Loss: 0.03706181029586027\n",
      "Epoch: 8539 \t|| Train Loss: 0.0158490033171375 \t|| Test Loss: 0.0369516102985533\n",
      "Epoch: 8540 \t|| Train Loss: 0.015815296526255358 \t|| Test Loss: 0.03691010030086304\n",
      "Epoch: 8541 \t|| Train Loss: 0.01578048332048277 \t|| Test Loss: 0.036799900303556024\n",
      "Epoch: 8542 \t|| Train Loss: 0.015746395529184054 \t|| Test Loss: 0.03675839030586577\n",
      "Epoch: 8543 \t|| Train Loss: 0.01571196332382805 \t|| Test Loss: 0.03664819030855878\n",
      "Epoch: 8544 \t|| Train Loss: 0.01567749453211274 \t|| Test Loss: 0.036606680310868535\n",
      "Epoch: 8545 \t|| Train Loss: 0.01564344332717332 \t|| Test Loss: 0.03649648031356155\n",
      "Epoch: 8546 \t|| Train Loss: 0.015608593535041423 \t|| Test Loss: 0.03645497031587129\n",
      "Epoch: 8547 \t|| Train Loss: 0.015574923330518603 \t|| Test Loss: 0.03634477031856427\n",
      "Epoch: 8548 \t|| Train Loss: 0.015540023332302266 \t|| Test Loss: 0.036234570321257285\n",
      "Epoch: 8549 \t|| Train Loss: 0.015506072539531707 \t|| Test Loss: 0.036193060323567026\n",
      "Epoch: 8550 \t|| Train Loss: 0.015471503335647544 \t|| Test Loss: 0.036082860326260034\n",
      "Epoch: 8551 \t|| Train Loss: 0.015437171542460392 \t|| Test Loss: 0.036041350328569775\n",
      "Epoch: 8552 \t|| Train Loss: 0.015402983338992815 \t|| Test Loss: 0.03593115033126276\n",
      "Epoch: 8553 \t|| Train Loss: 0.015368270545389081 \t|| Test Loss: 0.035889640333572524\n",
      "Epoch: 8554 \t|| Train Loss: 0.015334463342338085 \t|| Test Loss: 0.035779440336265525\n",
      "Epoch: 8555 \t|| Train Loss: 0.015299563344121758 \t|| Test Loss: 0.03566924033895854\n",
      "Epoch: 8556 \t|| Train Loss: 0.015265749549879365 \t|| Test Loss: 0.035627730341268274\n",
      "Epoch: 8557 \t|| Train Loss: 0.015231043347467037 \t|| Test Loss: 0.03551753034396129\n",
      "Epoch: 8558 \t|| Train Loss: 0.015196848552808048 \t|| Test Loss: 0.03547602034627103\n",
      "Epoch: 8559 \t|| Train Loss: 0.015162523350812302 \t|| Test Loss: 0.03536582034896405\n",
      "Epoch: 8560 \t|| Train Loss: 0.015127947555736723 \t|| Test Loss: 0.03532431035127378\n",
      "Epoch: 8561 \t|| Train Loss: 0.015094003354157587 \t|| Test Loss: 0.035214110353966786\n",
      "Epoch: 8562 \t|| Train Loss: 0.01505910335594125 \t|| Test Loss: 0.03510391035665976\n",
      "Epoch: 8563 \t|| Train Loss: 0.015025426560227031 \t|| Test Loss: 0.03506240035896955\n",
      "Epoch: 8564 \t|| Train Loss: 0.01499058335928653 \t|| Test Loss: 0.03495220036166256\n",
      "Epoch: 8565 \t|| Train Loss: 0.014956525563155697 \t|| Test Loss: 0.03491069036397228\n",
      "Epoch: 8566 \t|| Train Loss: 0.014922063362631799 \t|| Test Loss: 0.03480049036666529\n",
      "Epoch: 8567 \t|| Train Loss: 0.01488762456608438 \t|| Test Loss: 0.03475898036897503\n",
      "Epoch: 8568 \t|| Train Loss: 0.014853543365977068 \t|| Test Loss: 0.034648780371668034\n",
      "Epoch: 8569 \t|| Train Loss: 0.01481872356901306 \t|| Test Loss: 0.0346072703739778\n",
      "Epoch: 8570 \t|| Train Loss: 0.014785023369322341 \t|| Test Loss: 0.03449707037667079\n",
      "Epoch: 8571 \t|| Train Loss: 0.014750123371105994 \t|| Test Loss: 0.034386870379363776\n",
      "Epoch: 8572 \t|| Train Loss: 0.014716202573503354 \t|| Test Loss: 0.03434536038167353\n",
      "Epoch: 8573 \t|| Train Loss: 0.014681603374451286 \t|| Test Loss: 0.034235160384366546\n",
      "Epoch: 8574 \t|| Train Loss: 0.014647301576432032 \t|| Test Loss: 0.03419365038667629\n",
      "Epoch: 8575 \t|| Train Loss: 0.01461308337779656 \t|| Test Loss: 0.034083450389369295\n",
      "Epoch: 8576 \t|| Train Loss: 0.014578400579360717 \t|| Test Loss: 0.03404194039167903\n",
      "Epoch: 8577 \t|| Train Loss: 0.014544563381141828 \t|| Test Loss: 0.03393174039437206\n",
      "Epoch: 8578 \t|| Train Loss: 0.0145096633829255 \t|| Test Loss: 0.033821540397065045\n",
      "Epoch: 8579 \t|| Train Loss: 0.014475879583851006 \t|| Test Loss: 0.03378003039937479\n",
      "Epoch: 8580 \t|| Train Loss: 0.014441143386270772 \t|| Test Loss: 0.033669830402067794\n",
      "Epoch: 8581 \t|| Train Loss: 0.014406978586779685 \t|| Test Loss: 0.03362832040437754\n",
      "Epoch: 8582 \t|| Train Loss: 0.014372623389616052 \t|| Test Loss: 0.03351812040707054\n",
      "Epoch: 8583 \t|| Train Loss: 0.014338077589708365 \t|| Test Loss: 0.03347661040938029\n",
      "Epoch: 8584 \t|| Train Loss: 0.014304103392961324 \t|| Test Loss: 0.0333664104120733\n",
      "Epoch: 8585 \t|| Train Loss: 0.01426920339474499 \t|| Test Loss: 0.0332562104147663\n",
      "Epoch: 8586 \t|| Train Loss: 0.014235556594198655 \t|| Test Loss: 0.03321470041707601\n",
      "Epoch: 8587 \t|| Train Loss: 0.01420068339809027 \t|| Test Loss: 0.033104500419769034\n",
      "Epoch: 8588 \t|| Train Loss: 0.014166655597127337 \t|| Test Loss: 0.033062990422078796\n",
      "Epoch: 8589 \t|| Train Loss: 0.014132163401435544 \t|| Test Loss: 0.0329527904247718\n",
      "Epoch: 8590 \t|| Train Loss: 0.014097754600056026 \t|| Test Loss: 0.032911280427081545\n",
      "Epoch: 8591 \t|| Train Loss: 0.014063643404780817 \t|| Test Loss: 0.032801080429774546\n",
      "Epoch: 8592 \t|| Train Loss: 0.014028853602984706 \t|| Test Loss: 0.03275957043208427\n",
      "Epoch: 8593 \t|| Train Loss: 0.013995123408126081 \t|| Test Loss: 0.032649370434777295\n",
      "Epoch: 8594 \t|| Train Loss: 0.013960223409909761 \t|| Test Loss: 0.03253917043747031\n",
      "Epoch: 8595 \t|| Train Loss: 0.013926332607474995 \t|| Test Loss: 0.03249766043978003\n",
      "Epoch: 8596 \t|| Train Loss: 0.013891703413255015 \t|| Test Loss: 0.03238746044247304\n",
      "Epoch: 8597 \t|| Train Loss: 0.013857431610403675 \t|| Test Loss: 0.03234595044478279\n",
      "Epoch: 8598 \t|| Train Loss: 0.013823183416600304 \t|| Test Loss: 0.0322357504474758\n",
      "Epoch: 8599 \t|| Train Loss: 0.01378853061333236 \t|| Test Loss: 0.03219424044978555\n",
      "Epoch: 8600 \t|| Train Loss: 0.013754663419945579 \t|| Test Loss: 0.03208404045247857\n",
      "Epoch: 8601 \t|| Train Loss: 0.013719763421729255 \t|| Test Loss: 0.03197384045517156\n",
      "Epoch: 8602 \t|| Train Loss: 0.01368600961782265 \t|| Test Loss: 0.031932330457481284\n",
      "Epoch: 8603 \t|| Train Loss: 0.013651243425074525 \t|| Test Loss: 0.031822130460174305\n",
      "Epoch: 8604 \t|| Train Loss: 0.013617108620751339 \t|| Test Loss: 0.03178062046248405\n",
      "Epoch: 8605 \t|| Train Loss: 0.013582723428419796 \t|| Test Loss: 0.031670420465177054\n",
      "Epoch: 8606 \t|| Train Loss: 0.013548207623680022 \t|| Test Loss: 0.031628910467486795\n",
      "Epoch: 8607 \t|| Train Loss: 0.013514203431765065 \t|| Test Loss: 0.03151871047017979\n",
      "Epoch: 8608 \t|| Train Loss: 0.01347930662660871 \t|| Test Loss: 0.03147720047248955\n",
      "Epoch: 8609 \t|| Train Loss: 0.01344568343511034 \t|| Test Loss: 0.031367000475182524\n",
      "Epoch: 8610 \t|| Train Loss: 0.01341078343689402 \t|| Test Loss: 0.03125680047787556\n",
      "Epoch: 8611 \t|| Train Loss: 0.013376785631098986 \t|| Test Loss: 0.031215290480185287\n",
      "Epoch: 8612 \t|| Train Loss: 0.013342263440239293 \t|| Test Loss: 0.031105090482878305\n",
      "Epoch: 8613 \t|| Train Loss: 0.013307884634027666 \t|| Test Loss: 0.031063580485188046\n",
      "Epoch: 8614 \t|| Train Loss: 0.013273743443584562 \t|| Test Loss: 0.030953380487881064\n",
      "Epoch: 8615 \t|| Train Loss: 0.013238983636956341 \t|| Test Loss: 0.030911870490190795\n",
      "Epoch: 8616 \t|| Train Loss: 0.013205223446929825 \t|| Test Loss: 0.030801670492883803\n",
      "Epoch: 8617 \t|| Train Loss: 0.013170323448713512 \t|| Test Loss: 0.03069147049557678\n",
      "Epoch: 8618 \t|| Train Loss: 0.013136462641446644 \t|| Test Loss: 0.030649960497886562\n",
      "Epoch: 8619 \t|| Train Loss: 0.01310180345205878 \t|| Test Loss: 0.03053976050057957\n",
      "Epoch: 8620 \t|| Train Loss: 0.013067561644375331 \t|| Test Loss: 0.03049825050288929\n",
      "Epoch: 8621 \t|| Train Loss: 0.013033283455404065 \t|| Test Loss: 0.03038805050558231\n",
      "Epoch: 8622 \t|| Train Loss: 0.012998660647304 \t|| Test Loss: 0.03034654050789205\n",
      "Epoch: 8623 \t|| Train Loss: 0.012964763458749334 \t|| Test Loss: 0.030236340510585047\n",
      "Epoch: 8624 \t|| Train Loss: 0.01292986346053299 \t|| Test Loss: 0.030126140513278055\n",
      "Epoch: 8625 \t|| Train Loss: 0.0128961396517943 \t|| Test Loss: 0.030084630515587806\n",
      "Epoch: 8626 \t|| Train Loss: 0.012861343463878272 \t|| Test Loss: 0.029974430518280793\n",
      "Epoch: 8627 \t|| Train Loss: 0.012827238654722984 \t|| Test Loss: 0.029932920520590545\n",
      "Epoch: 8628 \t|| Train Loss: 0.012792823467223543 \t|| Test Loss: 0.029822720523283552\n",
      "Epoch: 8629 \t|| Train Loss: 0.012758337657651659 \t|| Test Loss: 0.029781210525593304\n",
      "Epoch: 8630 \t|| Train Loss: 0.012724303470568826 \t|| Test Loss: 0.02967101052828631\n",
      "Epoch: 8631 \t|| Train Loss: 0.012689436660580344 \t|| Test Loss: 0.029629500530596042\n",
      "Epoch: 8632 \t|| Train Loss: 0.012655783473914089 \t|| Test Loss: 0.02951930053328907\n",
      "Epoch: 8633 \t|| Train Loss: 0.012620883475697758 \t|| Test Loss: 0.029409100535982058\n",
      "Epoch: 8634 \t|| Train Loss: 0.012586915665070636 \t|| Test Loss: 0.02936759053829181\n",
      "Epoch: 8635 \t|| Train Loss: 0.012552363479043038 \t|| Test Loss: 0.029257390540984817\n",
      "Epoch: 8636 \t|| Train Loss: 0.012518014667999311 \t|| Test Loss: 0.02921588054329456\n",
      "Epoch: 8637 \t|| Train Loss: 0.012483843482388308 \t|| Test Loss: 0.029105680545987556\n",
      "Epoch: 8638 \t|| Train Loss: 0.012449113670927988 \t|| Test Loss: 0.029064170548297307\n",
      "Epoch: 8639 \t|| Train Loss: 0.012415323485733584 \t|| Test Loss: 0.028953970550990315\n",
      "Epoch: 8640 \t|| Train Loss: 0.012380423487517254 \t|| Test Loss: 0.028843770553683312\n",
      "Epoch: 8641 \t|| Train Loss: 0.012346592675418279 \t|| Test Loss: 0.028802260555993033\n",
      "Epoch: 8642 \t|| Train Loss: 0.01231190349086253 \t|| Test Loss: 0.02869206055868605\n",
      "Epoch: 8643 \t|| Train Loss: 0.012277691678346955 \t|| Test Loss: 0.028650550560995813\n",
      "Epoch: 8644 \t|| Train Loss: 0.0122433834942078 \t|| Test Loss: 0.02854035056368881\n",
      "Epoch: 8645 \t|| Train Loss: 0.012208790681275655 \t|| Test Loss: 0.02849884056599856\n",
      "Epoch: 8646 \t|| Train Loss: 0.012174863497553071 \t|| Test Loss: 0.02838864056869156\n",
      "Epoch: 8647 \t|| Train Loss: 0.012139963499336746 \t|| Test Loss: 0.028278440571384567\n",
      "Epoch: 8648 \t|| Train Loss: 0.012106269685765949 \t|| Test Loss: 0.028236930573694308\n",
      "Epoch: 8649 \t|| Train Loss: 0.012071443502682019 \t|| Test Loss: 0.028126730576387326\n",
      "Epoch: 8650 \t|| Train Loss: 0.012037368688694617 \t|| Test Loss: 0.028085220578697046\n",
      "Epoch: 8651 \t|| Train Loss: 0.012002923506027299 \t|| Test Loss: 0.027975020581390054\n",
      "Epoch: 8652 \t|| Train Loss: 0.011968467691623302 \t|| Test Loss: 0.027933510583699816\n",
      "Epoch: 8653 \t|| Train Loss: 0.011934403509372556 \t|| Test Loss: 0.027823310586392813\n",
      "Epoch: 8654 \t|| Train Loss: 0.011899566694551987 \t|| Test Loss: 0.027781800588702565\n",
      "Epoch: 8655 \t|| Train Loss: 0.011865883512717841 \t|| Test Loss: 0.027671600591395583\n",
      "Epoch: 8656 \t|| Train Loss: 0.011830983514501505 \t|| Test Loss: 0.02756140059408857\n",
      "Epoch: 8657 \t|| Train Loss: 0.01179704569904228 \t|| Test Loss: 0.02751989059639831\n",
      "Epoch: 8658 \t|| Train Loss: 0.011762463517846784 \t|| Test Loss: 0.02740969059909132\n",
      "Epoch: 8659 \t|| Train Loss: 0.011728144701970955 \t|| Test Loss: 0.02736818060140106\n",
      "Epoch: 8660 \t|| Train Loss: 0.011693943521192055 \t|| Test Loss: 0.027257980604094068\n",
      "Epoch: 8661 \t|| Train Loss: 0.011659243704899642 \t|| Test Loss: 0.02721647060640381\n",
      "Epoch: 8662 \t|| Train Loss: 0.011625423524537326 \t|| Test Loss: 0.027106270609096806\n",
      "Epoch: 8663 \t|| Train Loss: 0.011590523526321003 \t|| Test Loss: 0.026996070611789835\n",
      "Epoch: 8664 \t|| Train Loss: 0.011556722709389927 \t|| Test Loss: 0.026954560614099565\n",
      "Epoch: 8665 \t|| Train Loss: 0.011522003529666276 \t|| Test Loss: 0.026844360616792573\n",
      "Epoch: 8666 \t|| Train Loss: 0.011487821712318621 \t|| Test Loss: 0.026802850619102304\n",
      "Epoch: 8667 \t|| Train Loss: 0.011453483533011543 \t|| Test Loss: 0.026692650621795322\n",
      "Epoch: 8668 \t|| Train Loss: 0.011418920715247305 \t|| Test Loss: 0.026651140624105063\n",
      "Epoch: 8669 \t|| Train Loss: 0.011384963536356818 \t|| Test Loss: 0.02654094062679808\n",
      "Epoch: 8670 \t|| Train Loss: 0.01135006353814049 \t|| Test Loss: 0.02643074062949108\n",
      "Epoch: 8671 \t|| Train Loss: 0.01131639971973758 \t|| Test Loss: 0.02638923063180082\n",
      "Epoch: 8672 \t|| Train Loss: 0.01128154354148577 \t|| Test Loss: 0.026279030634493793\n",
      "Epoch: 8673 \t|| Train Loss: 0.011247498722666283 \t|| Test Loss: 0.02623752063680358\n",
      "Epoch: 8674 \t|| Train Loss: 0.011213023544831028 \t|| Test Loss: 0.026127320639496587\n",
      "Epoch: 8675 \t|| Train Loss: 0.011178597725594958 \t|| Test Loss: 0.026085810641806307\n",
      "Epoch: 8676 \t|| Train Loss: 0.011144503548176315 \t|| Test Loss: 0.025975610644499325\n",
      "Epoch: 8677 \t|| Train Loss: 0.01110969672852363 \t|| Test Loss: 0.025934100646809066\n",
      "Epoch: 8678 \t|| Train Loss: 0.011075983551521586 \t|| Test Loss: 0.025823900649502064\n",
      "Epoch: 8679 \t|| Train Loss: 0.011041083553305254 \t|| Test Loss: 0.02571370065219507\n",
      "Epoch: 8680 \t|| Train Loss: 0.011007175733013925 \t|| Test Loss: 0.025672190654504823\n",
      "Epoch: 8681 \t|| Train Loss: 0.010972563556650512 \t|| Test Loss: 0.02556199065719782\n",
      "Epoch: 8682 \t|| Train Loss: 0.010938274735942596 \t|| Test Loss: 0.02552048065950757\n",
      "Epoch: 8683 \t|| Train Loss: 0.0109040435599958 \t|| Test Loss: 0.02541028066220057\n",
      "Epoch: 8684 \t|| Train Loss: 0.010869373738871289 \t|| Test Loss: 0.02536877066451032\n",
      "Epoch: 8685 \t|| Train Loss: 0.010835523563341075 \t|| Test Loss: 0.02525857066720333\n",
      "Epoch: 8686 \t|| Train Loss: 0.010800623565124737 \t|| Test Loss: 0.025148370669896346\n",
      "Epoch: 8687 \t|| Train Loss: 0.01076685274336158 \t|| Test Loss: 0.025106860672206088\n",
      "Epoch: 8688 \t|| Train Loss: 0.010732103568470017 \t|| Test Loss: 0.024996660674899075\n",
      "Epoch: 8689 \t|| Train Loss: 0.010697951746290253 \t|| Test Loss: 0.024955150677208826\n",
      "Epoch: 8690 \t|| Train Loss: 0.01066358357181529 \t|| Test Loss: 0.024844950679901834\n",
      "Epoch: 8691 \t|| Train Loss: 0.010629050749218936 \t|| Test Loss: 0.024803440682211575\n",
      "Epoch: 8692 \t|| Train Loss: 0.010595063575160569 \t|| Test Loss: 0.024693240684904583\n",
      "Epoch: 8693 \t|| Train Loss: 0.010560163576944233 \t|| Test Loss: 0.02458304068759758\n",
      "Epoch: 8694 \t|| Train Loss: 0.010526529753709227 \t|| Test Loss: 0.02454153068990733\n",
      "Epoch: 8695 \t|| Train Loss: 0.010491643580289508 \t|| Test Loss: 0.02443133069260033\n",
      "Epoch: 8696 \t|| Train Loss: 0.010457628756637907 \t|| Test Loss: 0.024389820694910046\n",
      "Epoch: 8697 \t|| Train Loss: 0.01042312358363478 \t|| Test Loss: 0.024279620697603067\n",
      "Epoch: 8698 \t|| Train Loss: 0.010388727759566596 \t|| Test Loss: 0.02423811069991284\n",
      "Epoch: 8699 \t|| Train Loss: 0.010354603586980062 \t|| Test Loss: 0.024127910702605827\n",
      "Epoch: 8700 \t|| Train Loss: 0.010319826762495281 \t|| Test Loss: 0.024086400704915578\n",
      "Epoch: 8701 \t|| Train Loss: 0.010286083590325321 \t|| Test Loss: 0.023976200707608575\n",
      "Epoch: 8702 \t|| Train Loss: 0.010251183592109001 \t|| Test Loss: 0.023866000710301583\n",
      "Epoch: 8703 \t|| Train Loss: 0.01021730576698555 \t|| Test Loss: 0.023824490712611324\n",
      "Epoch: 8704 \t|| Train Loss: 0.010182663595454281 \t|| Test Loss: 0.023714290715304343\n",
      "Epoch: 8705 \t|| Train Loss: 0.010148404769914247 \t|| Test Loss: 0.023672780717614063\n",
      "Epoch: 8706 \t|| Train Loss: 0.010114143598799537 \t|| Test Loss: 0.02356258072030707\n",
      "Epoch: 8707 \t|| Train Loss: 0.010079503772842929 \t|| Test Loss: 0.023521070722616833\n",
      "Epoch: 8708 \t|| Train Loss: 0.01004562360214482 \t|| Test Loss: 0.02341087072530983\n",
      "Epoch: 8709 \t|| Train Loss: 0.010010723603928486 \t|| Test Loss: 0.023300670728002827\n",
      "Epoch: 8710 \t|| Train Loss: 0.009976982777333214 \t|| Test Loss: 0.0232591607303126\n",
      "Epoch: 8711 \t|| Train Loss: 0.009942203607273773 \t|| Test Loss: 0.023148960733005586\n",
      "Epoch: 8712 \t|| Train Loss: 0.0099080817802619 \t|| Test Loss: 0.023107450735315328\n",
      "Epoch: 8713 \t|| Train Loss: 0.009873683610619037 \t|| Test Loss: 0.022997250738008335\n",
      "Epoch: 8714 \t|| Train Loss: 0.009839180783190586 \t|| Test Loss: 0.022955740740318076\n",
      "Epoch: 8715 \t|| Train Loss: 0.00980516361396432 \t|| Test Loss: 0.022845540743011084\n",
      "Epoch: 8716 \t|| Train Loss: 0.009770279786119275 \t|| Test Loss: 0.022804030745320825\n",
      "Epoch: 8717 \t|| Train Loss: 0.009736643617309583 \t|| Test Loss: 0.022693830748013833\n",
      "Epoch: 8718 \t|| Train Loss: 0.009701743619093258 \t|| Test Loss: 0.02258363075070685\n",
      "Epoch: 8719 \t|| Train Loss: 0.009667758790609545 \t|| Test Loss: 0.022542120753016582\n",
      "Epoch: 8720 \t|| Train Loss: 0.00963322362243853 \t|| Test Loss: 0.02243192075570959\n",
      "Epoch: 8721 \t|| Train Loss: 0.009598857793538241 \t|| Test Loss: 0.02239041075801932\n",
      "Epoch: 8722 \t|| Train Loss: 0.009564703625783799 \t|| Test Loss: 0.02228021076071235\n",
      "Epoch: 8723 \t|| Train Loss: 0.009529956796466918 \t|| Test Loss: 0.02223870076302208\n",
      "Epoch: 8724 \t|| Train Loss: 0.00949618362912908 \t|| Test Loss: 0.022128500765715098\n",
      "Epoch: 8725 \t|| Train Loss: 0.00946128363091275 \t|| Test Loss: 0.022018300768408095\n",
      "Epoch: 8726 \t|| Train Loss: 0.009427435800957208 \t|| Test Loss: 0.021976790770717836\n",
      "Epoch: 8727 \t|| Train Loss: 0.009392763634258021 \t|| Test Loss: 0.02186659077341082\n",
      "Epoch: 8728 \t|| Train Loss: 0.009358534803885895 \t|| Test Loss: 0.021825080775720596\n",
      "Epoch: 8729 \t|| Train Loss: 0.00932424363760329 \t|| Test Loss: 0.021714880778413603\n",
      "Epoch: 8730 \t|| Train Loss: 0.009289633806814582 \t|| Test Loss: 0.021673370780723324\n",
      "Epoch: 8731 \t|| Train Loss: 0.00925572364094857 \t|| Test Loss: 0.021563170783416342\n",
      "Epoch: 8732 \t|| Train Loss: 0.009220823642732238 \t|| Test Loss: 0.02145297078610934\n",
      "Epoch: 8733 \t|| Train Loss: 0.009187112811304857 \t|| Test Loss: 0.02141146078841908\n",
      "Epoch: 8734 \t|| Train Loss: 0.009152303646077503 \t|| Test Loss: 0.021301260791112088\n",
      "Epoch: 8735 \t|| Train Loss: 0.009118211814233548 \t|| Test Loss: 0.02125975079342184\n",
      "Epoch: 8736 \t|| Train Loss: 0.009083783649422784 \t|| Test Loss: 0.021149550796114837\n",
      "Epoch: 8737 \t|| Train Loss: 0.009049310817162221 \t|| Test Loss: 0.02110804079842459\n",
      "Epoch: 8738 \t|| Train Loss: 0.009015263652768057 \t|| Test Loss: 0.020997840801117586\n",
      "Epoch: 8739 \t|| Train Loss: 0.008980409820090917 \t|| Test Loss: 0.020956330803427337\n",
      "Epoch: 8740 \t|| Train Loss: 0.00894674365611333 \t|| Test Loss: 0.020846130806120345\n",
      "Epoch: 8741 \t|| Train Loss: 0.008911843657897002 \t|| Test Loss: 0.020735930808813363\n",
      "Epoch: 8742 \t|| Train Loss: 0.008877888824581195 \t|| Test Loss: 0.020694420811123104\n",
      "Epoch: 8743 \t|| Train Loss: 0.008843323661242273 \t|| Test Loss: 0.02058422081381609\n",
      "Epoch: 8744 \t|| Train Loss: 0.008808987827509886 \t|| Test Loss: 0.020542710816125843\n",
      "Epoch: 8745 \t|| Train Loss: 0.008774803664587553 \t|| Test Loss: 0.02043251081881885\n",
      "Epoch: 8746 \t|| Train Loss: 0.008740086830438554 \t|| Test Loss: 0.02039100082112859\n",
      "Epoch: 8747 \t|| Train Loss: 0.008706283667932824 \t|| Test Loss: 0.0202808008238216\n",
      "Epoch: 8748 \t|| Train Loss: 0.008671383669716478 \t|| Test Loss: 0.020170600826514597\n",
      "Epoch: 8749 \t|| Train Loss: 0.008637565834928852 \t|| Test Loss: 0.020129090828824348\n",
      "Epoch: 8750 \t|| Train Loss: 0.008602863673061761 \t|| Test Loss: 0.020018890831517346\n",
      "Epoch: 8751 \t|| Train Loss: 0.008568664837857533 \t|| Test Loss: 0.019977380833827062\n",
      "Epoch: 8752 \t|| Train Loss: 0.008534343676407048 \t|| Test Loss: 0.01986718083652008\n",
      "Epoch: 8753 \t|| Train Loss: 0.008499763840786222 \t|| Test Loss: 0.019825670838829856\n",
      "Epoch: 8754 \t|| Train Loss: 0.00846582367975232 \t|| Test Loss: 0.019715470841522843\n",
      "Epoch: 8755 \t|| Train Loss: 0.008430923681535987 \t|| Test Loss: 0.01960527084421586\n",
      "Epoch: 8756 \t|| Train Loss: 0.008397242845276504 \t|| Test Loss: 0.019563760846525592\n",
      "Epoch: 8757 \t|| Train Loss: 0.008362403684881259 \t|| Test Loss: 0.01945356084921861\n",
      "Epoch: 8758 \t|| Train Loss: 0.008328341848205195 \t|| Test Loss: 0.01941205085152834\n",
      "Epoch: 8759 \t|| Train Loss: 0.008293883688226531 \t|| Test Loss: 0.01930185085422136\n",
      "Epoch: 8760 \t|| Train Loss: 0.00825944085113387 \t|| Test Loss: 0.019260340856531076\n",
      "Epoch: 8761 \t|| Train Loss: 0.008225363691571803 \t|| Test Loss: 0.019150140859224084\n",
      "Epoch: 8762 \t|| Train Loss: 0.008190539854062564 \t|| Test Loss: 0.01910863086153384\n",
      "Epoch: 8763 \t|| Train Loss: 0.008156843694917084 \t|| Test Loss: 0.018998430864226847\n",
      "Epoch: 8764 \t|| Train Loss: 0.008121943696700749 \t|| Test Loss: 0.01888823086691984\n",
      "Epoch: 8765 \t|| Train Loss: 0.008088018858552839 \t|| Test Loss: 0.018846720869229616\n",
      "Epoch: 8766 \t|| Train Loss: 0.008053423700046034 \t|| Test Loss: 0.018736520871922624\n",
      "Epoch: 8767 \t|| Train Loss: 0.008019117861481524 \t|| Test Loss: 0.018695010874232344\n",
      "Epoch: 8768 \t|| Train Loss: 0.007984903703391293 \t|| Test Loss: 0.018584810876925352\n",
      "Epoch: 8769 \t|| Train Loss: 0.007950216864410217 \t|| Test Loss: 0.018543300879235104\n",
      "Epoch: 8770 \t|| Train Loss: 0.007916383706736573 \t|| Test Loss: 0.0184331008819281\n",
      "Epoch: 8771 \t|| Train Loss: 0.007881483708520246 \t|| Test Loss: 0.018322900884621095\n",
      "Epoch: 8772 \t|| Train Loss: 0.007847695868900502 \t|| Test Loss: 0.01828139088693085\n",
      "Epoch: 8773 \t|| Train Loss: 0.007812963711865517 \t|| Test Loss: 0.018171190889623857\n",
      "Epoch: 8774 \t|| Train Loss: 0.007778794871829184 \t|| Test Loss: 0.0181296808919336\n",
      "Epoch: 8775 \t|| Train Loss: 0.007744443715210786 \t|| Test Loss: 0.018019480894626606\n",
      "Epoch: 8776 \t|| Train Loss: 0.0077098938747578675 \t|| Test Loss: 0.017977970896936334\n",
      "Epoch: 8777 \t|| Train Loss: 0.007675923718556066 \t|| Test Loss: 0.017867770899629366\n",
      "Epoch: 8778 \t|| Train Loss: 0.0076410237203397334 \t|| Test Loss: 0.017757570902322363\n",
      "Epoch: 8779 \t|| Train Loss: 0.007607372879248156 \t|| Test Loss: 0.017716060904632114\n",
      "Epoch: 8780 \t|| Train Loss: 0.007572503723684993 \t|| Test Loss: 0.017605860907325112\n",
      "Epoch: 8781 \t|| Train Loss: 0.007538471882176842 \t|| Test Loss: 0.017564350909634853\n",
      "Epoch: 8782 \t|| Train Loss: 0.007503983727030275 \t|| Test Loss: 0.017454150912327836\n",
      "Epoch: 8783 \t|| Train Loss: 0.007469570885105537 \t|| Test Loss: 0.017412640914637612\n",
      "Epoch: 8784 \t|| Train Loss: 0.00743546373037555 \t|| Test Loss: 0.01730244091733062\n",
      "Epoch: 8785 \t|| Train Loss: 0.007400669888034209 \t|| Test Loss: 0.017260930919640337\n",
      "Epoch: 8786 \t|| Train Loss: 0.007366943733720832 \t|| Test Loss: 0.01715073092233336\n",
      "Epoch: 8787 \t|| Train Loss: 0.007332043735504492 \t|| Test Loss: 0.017040530925026366\n",
      "Epoch: 8788 \t|| Train Loss: 0.007298148892524484 \t|| Test Loss: 0.016999020927336093\n",
      "Epoch: 8789 \t|| Train Loss: 0.007263523738849765 \t|| Test Loss: 0.0168888209300291\n",
      "Epoch: 8790 \t|| Train Loss: 0.007229247895453176 \t|| Test Loss: 0.016847310932338856\n",
      "Epoch: 8791 \t|| Train Loss: 0.007195003742195029 \t|| Test Loss: 0.01673711093503185\n",
      "Epoch: 8792 \t|| Train Loss: 0.007160346898381855 \t|| Test Loss: 0.016695600937341605\n",
      "Epoch: 8793 \t|| Train Loss: 0.007126483745540311 \t|| Test Loss: 0.0165854009400346\n",
      "Epoch: 8794 \t|| Train Loss: 0.007091583747323991 \t|| Test Loss: 0.01647520094272762\n",
      "Epoch: 8795 \t|| Train Loss: 0.007057825902872147 \t|| Test Loss: 0.016433690945037348\n",
      "Epoch: 8796 \t|| Train Loss: 0.007023063750669254 \t|| Test Loss: 0.01632349094773038\n",
      "Epoch: 8797 \t|| Train Loss: 0.0069889249058008255 \t|| Test Loss: 0.01628198095004012\n",
      "Epoch: 8798 \t|| Train Loss: 0.006954543754014522 \t|| Test Loss: 0.01617178095273312\n",
      "Epoch: 8799 \t|| Train Loss: 0.006920023908729513 \t|| Test Loss: 0.01613027095504286\n",
      "Epoch: 8800 \t|| Train Loss: 0.006886023757359803 \t|| Test Loss: 0.016020070957735867\n",
      "Epoch: 8801 \t|| Train Loss: 0.006851123759143469 \t|| Test Loss: 0.01590987096042886\n",
      "Epoch: 8802 \t|| Train Loss: 0.006817502913219792 \t|| Test Loss: 0.015868360962738616\n",
      "Epoch: 8803 \t|| Train Loss: 0.006782603762488749 \t|| Test Loss: 0.01575816096543161\n",
      "Epoch: 8804 \t|| Train Loss: 0.006748601916148475 \t|| Test Loss: 0.015716650967741365\n",
      "Epoch: 8805 \t|| Train Loss: 0.006714083765834015 \t|| Test Loss: 0.01560645097043436\n",
      "Epoch: 8806 \t|| Train Loss: 0.006679700919077157 \t|| Test Loss: 0.015564940972744079\n",
      "Epoch: 8807 \t|| Train Loss: 0.006645563769179297 \t|| Test Loss: 0.015454740975437097\n",
      "Epoch: 8808 \t|| Train Loss: 0.006610799922005838 \t|| Test Loss: 0.015413230977746873\n",
      "Epoch: 8809 \t|| Train Loss: 0.006577043772524575 \t|| Test Loss: 0.015303030980439858\n",
      "Epoch: 8810 \t|| Train Loss: 0.0065421437743082385 \t|| Test Loss: 0.015192830983132876\n",
      "Epoch: 8811 \t|| Train Loss: 0.006508278926496129 \t|| Test Loss: 0.015151320985442607\n",
      "Epoch: 8812 \t|| Train Loss: 0.0064736237776535106 \t|| Test Loss: 0.015041120988135625\n",
      "Epoch: 8813 \t|| Train Loss: 0.006439377929424816 \t|| Test Loss: 0.014999610990445356\n",
      "Epoch: 8814 \t|| Train Loss: 0.006405103780998791 \t|| Test Loss: 0.014889410993138374\n",
      "Epoch: 8815 \t|| Train Loss: 0.006370476932353495 \t|| Test Loss: 0.014847900995448105\n",
      "Epoch: 8816 \t|| Train Loss: 0.006336583784344052 \t|| Test Loss: 0.0147377009981411\n",
      "Epoch: 8817 \t|| Train Loss: 0.00630168378612773 \t|| Test Loss: 0.01462750100083412\n",
      "Epoch: 8818 \t|| Train Loss: 0.006267955936843789 \t|| Test Loss: 0.014585991003143872\n",
      "Epoch: 8819 \t|| Train Loss: 0.006233163789472998 \t|| Test Loss: 0.014475791005836857\n",
      "Epoch: 8820 \t|| Train Loss: 0.006199054939772466 \t|| Test Loss: 0.014434281008146633\n",
      "Epoch: 8821 \t|| Train Loss: 0.006164643792818285 \t|| Test Loss: 0.01432408101083964\n",
      "Epoch: 8822 \t|| Train Loss: 0.006130153942701154 \t|| Test Loss: 0.01428257101314936\n",
      "Epoch: 8823 \t|| Train Loss: 0.006096123796163551 \t|| Test Loss: 0.014172371015842367\n",
      "Epoch: 8824 \t|| Train Loss: 0.006061252945629841 \t|| Test Loss: 0.014130861018152118\n",
      "Epoch: 8825 \t|| Train Loss: 0.006027603799508825 \t|| Test Loss: 0.014020661020845116\n",
      "Epoch: 8826 \t|| Train Loss: 0.005992703801292507 \t|| Test Loss: 0.013910461023538111\n",
      "Epoch: 8827 \t|| Train Loss: 0.005958731950120123 \t|| Test Loss: 0.013868951025847865\n",
      "Epoch: 8828 \t|| Train Loss: 0.0059241838046377795 \t|| Test Loss: 0.013758751028540872\n",
      "Epoch: 8829 \t|| Train Loss: 0.005889830953048807 \t|| Test Loss: 0.013717241030850614\n",
      "Epoch: 8830 \t|| Train Loss: 0.005855663807983041 \t|| Test Loss: 0.013607041033543621\n",
      "Epoch: 8831 \t|| Train Loss: 0.00582092995597749 \t|| Test Loss: 0.01356553103585335\n",
      "Epoch: 8832 \t|| Train Loss: 0.005787143811328324 \t|| Test Loss: 0.01345533103854638\n",
      "Epoch: 8833 \t|| Train Loss: 0.005752243813111989 \t|| Test Loss: 0.013345131041239378\n",
      "Epoch: 8834 \t|| Train Loss: 0.00571840896046778 \t|| Test Loss: 0.01330362104354913\n",
      "Epoch: 8835 \t|| Train Loss: 0.005683723816457255 \t|| Test Loss: 0.013193421046242127\n",
      "Epoch: 8836 \t|| Train Loss: 0.005649507963396465 \t|| Test Loss: 0.013151911048551868\n",
      "Epoch: 8837 \t|| Train Loss: 0.005615203819802536 \t|| Test Loss: 0.013041711051244853\n",
      "Epoch: 8838 \t|| Train Loss: 0.005580606966325147 \t|| Test Loss: 0.013000201053554627\n",
      "Epoch: 8839 \t|| Train Loss: 0.005546683823147808 \t|| Test Loss: 0.012890001056247635\n",
      "Epoch: 8840 \t|| Train Loss: 0.005511783824931486 \t|| Test Loss: 0.012779801058940632\n",
      "Epoch: 8841 \t|| Train Loss: 0.00547808597081543 \t|| Test Loss: 0.012738291061250373\n",
      "Epoch: 8842 \t|| Train Loss: 0.005443263828276751 \t|| Test Loss: 0.012628091063943392\n",
      "Epoch: 8843 \t|| Train Loss: 0.005409184973744112 \t|| Test Loss: 0.01258658106625311\n",
      "Epoch: 8844 \t|| Train Loss: 0.005374743831622025 \t|| Test Loss: 0.012476381068946118\n",
      "Epoch: 8845 \t|| Train Loss: 0.005340283976672802 \t|| Test Loss: 0.012434871071255871\n",
      "Epoch: 8846 \t|| Train Loss: 0.005306223834967297 \t|| Test Loss: 0.012324671073948867\n",
      "Epoch: 8847 \t|| Train Loss: 0.0052713829796014865 \t|| Test Loss: 0.01228316107625862\n",
      "Epoch: 8848 \t|| Train Loss: 0.005237703838312574 \t|| Test Loss: 0.012172961078951616\n",
      "Epoch: 8849 \t|| Train Loss: 0.005202803840096239 \t|| Test Loss: 0.012062761081644635\n",
      "Epoch: 8850 \t|| Train Loss: 0.0051688619840917676 \t|| Test Loss: 0.012021251083954364\n",
      "Epoch: 8851 \t|| Train Loss: 0.00513428384344152 \t|| Test Loss: 0.011911051086647395\n",
      "Epoch: 8852 \t|| Train Loss: 0.005099960987020454 \t|| Test Loss: 0.011869541088957136\n",
      "Epoch: 8853 \t|| Train Loss: 0.005065763846786786 \t|| Test Loss: 0.011759341091650133\n",
      "Epoch: 8854 \t|| Train Loss: 0.005031059989949141 \t|| Test Loss: 0.011717831093959874\n",
      "Epoch: 8855 \t|| Train Loss: 0.004997243850132066 \t|| Test Loss: 0.011607631096652882\n",
      "Epoch: 8856 \t|| Train Loss: 0.004962343851915731 \t|| Test Loss: 0.011497431099345878\n",
      "Epoch: 8857 \t|| Train Loss: 0.00492853899443942 \t|| Test Loss: 0.011455921101655631\n",
      "Epoch: 8858 \t|| Train Loss: 0.004893823855261001 \t|| Test Loss: 0.011345721104348627\n",
      "Epoch: 8859 \t|| Train Loss: 0.004859637997368098 \t|| Test Loss: 0.01130421110665838\n",
      "Epoch: 8860 \t|| Train Loss: 0.004825303858606279 \t|| Test Loss: 0.011194011109351375\n",
      "Epoch: 8861 \t|| Train Loss: 0.004790737000296785 \t|| Test Loss: 0.011152501111661106\n",
      "Epoch: 8862 \t|| Train Loss: 0.004756783861951559 \t|| Test Loss: 0.011042301114354114\n",
      "Epoch: 8863 \t|| Train Loss: 0.0047218838637352115 \t|| Test Loss: 0.010932101117047144\n",
      "Epoch: 8864 \t|| Train Loss: 0.004688216004787074 \t|| Test Loss: 0.010890591119356873\n",
      "Epoch: 8865 \t|| Train Loss: 0.004653363867080497 \t|| Test Loss: 0.010780391122049893\n",
      "Epoch: 8866 \t|| Train Loss: 0.004619315007715762 \t|| Test Loss: 0.010738881124359622\n",
      "Epoch: 8867 \t|| Train Loss: 0.004584843870425776 \t|| Test Loss: 0.010628681127052642\n",
      "Epoch: 8868 \t|| Train Loss: 0.004550414010644453 \t|| Test Loss: 0.010587171129362371\n",
      "Epoch: 8869 \t|| Train Loss: 0.004516323873771049 \t|| Test Loss: 0.01047697113205539\n",
      "Epoch: 8870 \t|| Train Loss: 0.004481513013573119 \t|| Test Loss: 0.01043546113436512\n",
      "Epoch: 8871 \t|| Train Loss: 0.004447803877116327 \t|| Test Loss: 0.010325261137058128\n",
      "Epoch: 8872 \t|| Train Loss: 0.004412903878899995 \t|| Test Loss: 0.010215061139751135\n",
      "Epoch: 8873 \t|| Train Loss: 0.004378992018063415 \t|| Test Loss: 0.010173551142060889\n",
      "Epoch: 8874 \t|| Train Loss: 0.004344383882245256 \t|| Test Loss: 0.010063351144753874\n",
      "Epoch: 8875 \t|| Train Loss: 0.004310091020992087 \t|| Test Loss: 0.010021841147063648\n",
      "Epoch: 8876 \t|| Train Loss: 0.004275863885590539 \t|| Test Loss: 0.009911641149756656\n",
      "Epoch: 8877 \t|| Train Loss: 0.004241190023920783 \t|| Test Loss: 0.009870131152066374\n",
      "Epoch: 8878 \t|| Train Loss: 0.004207343888935814 \t|| Test Loss: 0.009759931154759382\n",
      "Epoch: 8879 \t|| Train Loss: 0.004172443890719487 \t|| Test Loss: 0.00964973115745238\n",
      "Epoch: 8880 \t|| Train Loss: 0.004138669028411074 \t|| Test Loss: 0.00960822115976213\n",
      "Epoch: 8881 \t|| Train Loss: 0.00410392389406476 \t|| Test Loss: 0.009498021162455128\n",
      "Epoch: 8882 \t|| Train Loss: 0.004069768031339752 \t|| Test Loss: 0.00945651116476488\n",
      "Epoch: 8883 \t|| Train Loss: 0.004035403897410026 \t|| Test Loss: 0.009346311167457887\n",
      "Epoch: 8884 \t|| Train Loss: 0.004000867034268436 \t|| Test Loss: 0.009304801169767628\n",
      "Epoch: 8885 \t|| Train Loss: 0.003966883900755304 \t|| Test Loss: 0.009194601172460636\n",
      "Epoch: 8886 \t|| Train Loss: 0.003931983902538979 \t|| Test Loss: 0.009084401175153644\n",
      "Epoch: 8887 \t|| Train Loss: 0.003898346038758725 \t|| Test Loss: 0.009042891177463397\n",
      "Epoch: 8888 \t|| Train Loss: 0.0038634639058842484 \t|| Test Loss: 0.008932691180156393\n",
      "Epoch: 8889 \t|| Train Loss: 0.003829445041687407 \t|| Test Loss: 0.008891181182466146\n",
      "Epoch: 8890 \t|| Train Loss: 0.0037949439092295175 \t|| Test Loss: 0.008780981185159142\n",
      "Epoch: 8891 \t|| Train Loss: 0.0037605440446160875 \t|| Test Loss: 0.008739471187468895\n",
      "Epoch: 8892 \t|| Train Loss: 0.0037264239125747974 \t|| Test Loss: 0.00862927119016187\n",
      "Epoch: 8893 \t|| Train Loss: 0.0036916430475447876 \t|| Test Loss: 0.008587761192471644\n",
      "Epoch: 8894 \t|| Train Loss: 0.0036579039159200617 \t|| Test Loss: 0.008477561195164652\n",
      "Epoch: 8895 \t|| Train Loss: 0.0036230039177037377 \t|| Test Loss: 0.008367361197857647\n",
      "Epoch: 8896 \t|| Train Loss: 0.0035891220520350583 \t|| Test Loss: 0.008325851200167388\n",
      "Epoch: 8897 \t|| Train Loss: 0.0035544839210490094 \t|| Test Loss: 0.008215651202860408\n",
      "Epoch: 8898 \t|| Train Loss: 0.0035202210549637363 \t|| Test Loss: 0.008174141205170127\n",
      "Epoch: 8899 \t|| Train Loss: 0.0034859639243942806 \t|| Test Loss: 0.008063941207863135\n",
      "Epoch: 8900 \t|| Train Loss: 0.0034513200578924277 \t|| Test Loss: 0.008022431210172909\n",
      "Epoch: 8901 \t|| Train Loss: 0.0034174439277395415 \t|| Test Loss: 0.007912231212865883\n",
      "Epoch: 8902 \t|| Train Loss: 0.0033825439295232214 \t|| Test Loss: 0.007802031215558902\n",
      "Epoch: 8903 \t|| Train Loss: 0.003348799062382718 \t|| Test Loss: 0.007760521217868632\n",
      "Epoch: 8904 \t|| Train Loss: 0.0033140239328685013 \t|| Test Loss: 0.007650321220561651\n",
      "Epoch: 8905 \t|| Train Loss: 0.0032798980653114 \t|| Test Loss: 0.007608811222871381\n",
      "Epoch: 8906 \t|| Train Loss: 0.0032455039362137674 \t|| Test Loss: 0.007498611225564411\n",
      "Epoch: 8907 \t|| Train Loss: 0.003210997068240083 \t|| Test Loss: 0.007457101227874141\n",
      "Epoch: 8908 \t|| Train Loss: 0.0031769839395590386 \t|| Test Loss: 0.007346901230567149\n",
      "Epoch: 8909 \t|| Train Loss: 0.003142096071168758 \t|| Test Loss: 0.00730539123287689\n",
      "Epoch: 8910 \t|| Train Loss: 0.0031084639429043203 \t|| Test Loss: 0.007195191235569898\n",
      "Epoch: 8911 \t|| Train Loss: 0.0030735639446879915 \t|| Test Loss: 0.007084991238262894\n",
      "Epoch: 8912 \t|| Train Loss: 0.003039575075659043 \t|| Test Loss: 0.007043481240572647\n",
      "Epoch: 8913 \t|| Train Loss: 0.0030050439480332576 \t|| Test Loss: 0.006933281243265643\n",
      "Epoch: 8914 \t|| Train Loss: 0.0029706740785877344 \t|| Test Loss: 0.006891771245575396\n",
      "Epoch: 8915 \t|| Train Loss: 0.0029365239513785323 \t|| Test Loss: 0.006781571248268403\n",
      "Epoch: 8916 \t|| Train Loss: 0.0029017730815164137 \t|| Test Loss: 0.006740061250578122\n",
      "Epoch: 8917 \t|| Train Loss: 0.0028680039547238135 \t|| Test Loss: 0.00662986125327113\n",
      "Epoch: 8918 \t|| Train Loss: 0.0028331039565074796 \t|| Test Loss: 0.006519661255964149\n",
      "Epoch: 8919 \t|| Train Loss: 0.0027992520860067025 \t|| Test Loss: 0.00647815125827389\n",
      "Epoch: 8920 \t|| Train Loss: 0.002764583959852762 \t|| Test Loss: 0.006367951260966909\n",
      "Epoch: 8921 \t|| Train Loss: 0.002730351088935383 \t|| Test Loss: 0.00632644126327665\n",
      "Epoch: 8922 \t|| Train Loss: 0.0026960639631980242 \t|| Test Loss: 0.006216241265969658\n",
      "Epoch: 8923 \t|| Train Loss: 0.002661450091864062 \t|| Test Loss: 0.0061747312682793876\n",
      "Epoch: 8924 \t|| Train Loss: 0.002627543966543311 \t|| Test Loss: 0.006064531270972407\n",
      "Epoch: 8925 \t|| Train Loss: 0.00259264396832697 \t|| Test Loss: 0.005954331273665414\n",
      "Epoch: 8926 \t|| Train Loss: 0.002558929096354361 \t|| Test Loss: 0.0059128212759751555\n",
      "Epoch: 8927 \t|| Train Loss: 0.002524123971672243 \t|| Test Loss: 0.005802621278668152\n",
      "Epoch: 8928 \t|| Train Loss: 0.0024900280992830428 \t|| Test Loss: 0.005761111280977904\n",
      "Epoch: 8929 \t|| Train Loss: 0.0024556039750175214 \t|| Test Loss: 0.0056509112836708895\n",
      "Epoch: 8930 \t|| Train Loss: 0.0024211271022117147 \t|| Test Loss: 0.0056094012859806645\n",
      "Epoch: 8931 \t|| Train Loss: 0.002387083978362807 \t|| Test Loss: 0.005499201288673683\n",
      "Epoch: 8932 \t|| Train Loss: 0.0023522261051403997 \t|| Test Loss: 0.005457691290983402\n",
      "Epoch: 8933 \t|| Train Loss: 0.0023185639817080717 \t|| Test Loss: 0.0053474912936763985\n",
      "Epoch: 8934 \t|| Train Loss: 0.002283663983491739 \t|| Test Loss: 0.005237291296369395\n",
      "Epoch: 8935 \t|| Train Loss: 0.0022497051096307024 \t|| Test Loss: 0.005195781298679147\n",
      "Epoch: 8936 \t|| Train Loss: 0.0022151439868370163 \t|| Test Loss: 0.005085581301372144\n",
      "Epoch: 8937 \t|| Train Loss: 0.0021808041125593826 \t|| Test Loss: 0.005044071303681896\n",
      "Epoch: 8938 \t|| Train Loss: 0.002146623990182288 \t|| Test Loss: 0.004933871306374904\n",
      "Epoch: 8939 \t|| Train Loss: 0.0021119031154880562 \t|| Test Loss: 0.004892361308684645\n",
      "Epoch: 8940 \t|| Train Loss: 0.0020781039935275553 \t|| Test Loss: 0.004782161311377642\n",
      "Epoch: 8941 \t|| Train Loss: 0.0020432039953112323 \t|| Test Loss: 0.004671961314070661\n",
      "Epoch: 8942 \t|| Train Loss: 0.0020093821199783494 \t|| Test Loss: 0.004630451316380413\n",
      "Epoch: 8943 \t|| Train Loss: 0.0019746839986564957 \t|| Test Loss: 0.0045202513190734095\n",
      "Epoch: 8944 \t|| Train Loss: 0.0019404811229070298 \t|| Test Loss: 0.004478741321383162\n",
      "Epoch: 8945 \t|| Train Loss: 0.0019061640020017825 \t|| Test Loss: 0.004368541324076158\n",
      "Epoch: 8946 \t|| Train Loss: 0.0018715801258357145 \t|| Test Loss: 0.004327031326385911\n",
      "Epoch: 8947 \t|| Train Loss: 0.0018376440053470486 \t|| Test Loss: 0.004216831329078907\n",
      "Epoch: 8948 \t|| Train Loss: 0.0018027440071307216 \t|| Test Loss: 0.004106631331771892\n",
      "Epoch: 8949 \t|| Train Loss: 0.0017690591303259951 \t|| Test Loss: 0.004065121334081667\n",
      "Epoch: 8950 \t|| Train Loss: 0.0017342240104759933 \t|| Test Loss: 0.003954921336774664\n",
      "Epoch: 8951 \t|| Train Loss: 0.0017001581332546825 \t|| Test Loss: 0.003913411339084405\n",
      "Epoch: 8952 \t|| Train Loss: 0.0016657040138212675 \t|| Test Loss: 0.003803211341777424\n",
      "Epoch: 8953 \t|| Train Loss: 0.0016312571361833616 \t|| Test Loss: 0.0037617013440871426\n",
      "Epoch: 8954 \t|| Train Loss: 0.0015971840171665323 \t|| Test Loss: 0.0036515013467801503\n",
      "Epoch: 8955 \t|| Train Loss: 0.0015623561391120532 \t|| Test Loss: 0.003609991349089925\n",
      "Epoch: 8956 \t|| Train Loss: 0.0015286640205118163 \t|| Test Loss: 0.0034997913517829105\n",
      "Epoch: 8957 \t|| Train Loss: 0.0014937640222954824 \t|| Test Loss: 0.003389591354475918\n",
      "Epoch: 8958 \t|| Train Loss: 0.001459835143602345 \t|| Test Loss: 0.003348081356785648\n",
      "Epoch: 8959 \t|| Train Loss: 0.0014252440256407608 \t|| Test Loss: 0.003237881359478667\n",
      "Epoch: 8960 \t|| Train Loss: 0.0013909341465310199 \t|| Test Loss: 0.003196371361788397\n",
      "Epoch: 8961 \t|| Train Loss: 0.0013567240289860311 \t|| Test Loss: 0.003086171364481427\n",
      "Epoch: 8962 \t|| Train Loss: 0.0013220331494597074 \t|| Test Loss: 0.003044661366791157\n",
      "Epoch: 8963 \t|| Train Loss: 0.0012882040323313067 \t|| Test Loss: 0.002934461369484165\n",
      "Epoch: 8964 \t|| Train Loss: 0.0012533040341149743 \t|| Test Loss: 0.0028242613721771613\n",
      "Epoch: 8965 \t|| Train Loss: 0.0012195121539499947 \t|| Test Loss: 0.0027827513744869137\n",
      "Epoch: 8966 \t|| Train Loss: 0.0011847840374602388 \t|| Test Loss: 0.0026725513771799214\n",
      "Epoch: 8967 \t|| Train Loss: 0.0011506111568786767 \t|| Test Loss: 0.0026310413794896625\n",
      "Epoch: 8968 \t|| Train Loss: 0.0011162640408055118 \t|| Test Loss: 0.002520841382182659\n",
      "Epoch: 8969 \t|| Train Loss: 0.0010817101598073573 \t|| Test Loss: 0.0024793313844924114\n",
      "Epoch: 8970 \t|| Train Loss: 0.0010477440441507863 \t|| Test Loss: 0.002369131387185419\n",
      "Epoch: 8971 \t|| Train Loss: 0.0010128440459344647 \t|| Test Loss: 0.0022589313898784048\n",
      "Epoch: 8972 \t|| Train Loss: 0.0009791891642976446 \t|| Test Loss: 0.002217421392188146\n",
      "Epoch: 8973 \t|| Train Loss: 0.000944324049279735 \t|| Test Loss: 0.0021072213948811645\n",
      "Epoch: 8974 \t|| Train Loss: 0.0009102881672263307 \t|| Test Loss: 0.002065711397190917\n",
      "Epoch: 8975 \t|| Train Loss: 0.0008758040526250135 \t|| Test Loss: 0.0019555113998839246\n",
      "Epoch: 8976 \t|| Train Loss: 0.0008413871701550113 \t|| Test Loss: 0.0019140014021936768\n",
      "Epoch: 8977 \t|| Train Loss: 0.0008072840559702879 \t|| Test Loss: 0.0018038014048866735\n",
      "Epoch: 8978 \t|| Train Loss: 0.0007724861730837058 \t|| Test Loss: 0.0017622914071964036\n",
      "Epoch: 8979 \t|| Train Loss: 0.0007387640593155582 \t|| Test Loss: 0.0016520914098894224\n",
      "Epoch: 8980 \t|| Train Loss: 0.0007038640610992255 \t|| Test Loss: 0.0015418914125824301\n",
      "Epoch: 8981 \t|| Train Loss: 0.000669965177573989 \t|| Test Loss: 0.0015003814148921713\n",
      "Epoch: 8982 \t|| Train Loss: 0.0006353440644445041 \t|| Test Loss: 0.001390181417585168\n",
      "Epoch: 8983 \t|| Train Loss: 0.0006010641805026667 \t|| Test Loss: 0.0013486714198949201\n",
      "Epoch: 8984 \t|| Train Loss: 0.0005668240677897798 \t|| Test Loss: 0.0012384714225879057\n",
      "Epoch: 8985 \t|| Train Loss: 0.0005321631834313445 \t|| Test Loss: 0.00119696142489768\n",
      "Epoch: 8986 \t|| Train Loss: 0.0004983040711350597 \t|| Test Loss: 0.0010867614275906988\n",
      "Epoch: 8987 \t|| Train Loss: 0.00046340407291871886 \t|| Test Loss: 0.0009765614302836734\n",
      "Epoch: 8988 \t|| Train Loss: 0.00042964218792162915 \t|| Test Loss: 0.0009350514325934145\n",
      "Epoch: 8989 \t|| Train Loss: 0.00039488407626400295 \t|| Test Loss: 0.0008248514352864111\n",
      "Epoch: 8990 \t|| Train Loss: 0.0003607411908503236 \t|| Test Loss: 0.0007833414375961634\n",
      "Epoch: 8991 \t|| Train Loss: 0.00032636407960926763 \t|| Test Loss: 0.0006731414402891712\n",
      "Epoch: 8992 \t|| Train Loss: 0.0002918401937790097 \t|| Test Loss: 0.0006316314425989123\n",
      "Epoch: 8993 \t|| Train Loss: 0.0002578440829545503 \t|| Test Loss: 0.00052143144529192\n",
      "Epoch: 8994 \t|| Train Loss: 0.00022294408473821497 \t|| Test Loss: 0.00041123144798494994\n",
      "Epoch: 8995 \t|| Train Loss: 0.00018966257732276476 \t|| Test Loss: 0.00043930145039841053\n",
      "Epoch: 8996 \t|| Train Loss: 0.00016135588847803335 \t|| Test Loss: 6.324145286314974e-05\n",
      "Epoch: 8997 \t|| Train Loss: 0.00020605683013462384 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 8998 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 8999 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9000 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9001 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9002 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9003 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9004 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9005 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9006 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9007 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9008 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9009 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9010 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9011 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9012 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9013 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9014 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9015 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9016 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9017 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9018 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9019 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9020 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9021 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9022 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9023 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9024 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9025 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9026 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9027 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9028 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9029 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9030 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9031 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9032 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9033 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9034 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9035 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9036 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9037 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9038 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9039 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9040 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9041 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9042 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9043 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9044 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9045 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9046 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9047 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9048 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9049 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9050 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9051 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9052 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9053 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9054 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9055 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9056 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9057 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9058 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9059 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9060 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9061 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9062 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9063 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9064 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9065 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9066 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9067 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9068 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9069 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9070 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9071 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9072 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9073 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9074 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9075 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9076 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9077 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9078 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9079 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9080 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9081 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9082 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9083 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9084 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9085 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9086 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9087 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9088 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9089 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9090 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9091 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9092 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9093 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9094 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9095 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9096 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9097 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9098 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9099 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9100 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9101 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9102 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9103 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9104 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9105 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9106 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9107 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9108 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9109 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9110 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9111 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9112 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9113 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9114 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9115 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9116 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9117 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9118 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9119 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9120 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9121 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9122 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9123 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9124 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9125 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9126 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9127 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9128 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9129 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9130 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9131 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9132 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9133 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9134 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9135 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9136 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9137 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9138 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9139 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9140 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9141 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9142 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9143 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9144 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9145 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9146 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9147 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9148 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9149 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9150 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9151 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9152 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9153 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9154 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9155 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9156 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9157 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9158 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9159 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9160 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9161 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9162 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9163 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9164 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9165 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9166 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9167 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9168 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9169 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9170 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9171 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9172 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9173 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9174 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9175 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9176 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9177 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9178 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9179 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9180 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9181 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9182 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9183 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9184 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9185 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9186 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9187 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9188 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9189 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9190 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9191 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9192 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9193 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9194 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9195 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9196 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9197 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9198 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9199 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9200 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9201 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9202 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9203 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9204 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9205 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9206 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9207 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9208 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9209 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9210 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9211 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9212 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9213 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9214 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9215 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9216 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9217 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9218 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9219 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9220 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9221 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9222 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9223 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9224 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9225 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9226 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9227 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9228 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9229 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9230 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9231 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9232 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9233 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9234 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9235 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9236 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9237 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9238 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9239 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9240 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9241 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9242 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9243 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9244 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9245 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9246 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9247 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9248 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9249 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9250 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9251 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9252 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9253 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9254 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9255 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9256 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9257 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9258 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9259 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9260 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9261 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9262 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9263 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9264 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9265 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9266 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9267 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9268 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9269 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9270 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9271 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9272 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9273 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9274 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9275 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9276 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9277 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9278 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9279 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9280 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9281 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9282 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9283 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9284 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9285 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9286 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9287 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9288 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9289 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9290 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9291 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9292 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9293 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9294 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9295 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9296 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9297 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9298 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9299 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9300 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9301 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9302 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9303 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9304 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9305 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9306 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9307 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9308 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9309 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9310 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9311 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9312 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9313 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9314 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9315 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9316 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9317 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9318 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9319 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9320 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9321 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9322 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9323 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9324 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9325 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9326 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9327 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9328 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9329 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9330 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9331 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9332 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9333 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9334 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9335 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9336 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9337 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9338 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9339 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9340 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9341 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9342 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9343 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9344 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9345 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9346 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9347 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9348 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9349 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9350 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9351 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9352 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9353 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9354 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9355 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9356 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9357 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9358 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9359 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9360 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9361 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9362 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9363 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9364 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9365 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9366 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9367 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9368 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9369 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9370 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9371 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9372 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9373 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9374 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9375 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9376 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9377 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9378 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9379 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9380 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9381 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9382 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9383 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9384 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9385 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9386 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9387 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9388 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9389 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9390 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9391 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9392 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9393 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9394 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9395 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9396 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9397 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9398 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9399 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9400 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9401 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9402 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9403 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9404 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9405 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9406 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9407 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9408 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9409 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9410 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9411 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9412 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9413 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9414 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9415 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9416 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9417 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9418 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9419 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9420 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9421 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9422 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9423 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9424 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9425 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9426 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9427 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9428 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9429 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9430 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9431 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9432 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9433 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9434 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9435 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9436 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9437 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9438 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9439 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9440 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9441 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9442 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9443 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9444 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9445 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9446 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9447 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9448 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9449 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9450 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9451 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9452 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9453 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9454 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9455 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9456 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9457 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9458 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9459 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9460 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9461 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9462 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9463 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9464 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9465 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9466 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9467 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9468 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9469 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9470 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9471 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9472 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9473 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9474 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9475 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9476 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9477 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9478 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9479 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9480 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9481 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9482 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9483 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9484 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9485 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9486 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9487 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9488 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9489 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9490 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9491 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9492 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9493 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9494 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9495 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9496 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9497 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9498 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9499 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9500 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9501 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9502 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9503 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9504 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9505 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9506 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9507 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9508 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9509 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9510 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9511 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9512 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9513 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9514 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9515 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9516 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9517 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9518 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9519 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9520 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9521 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9522 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9523 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9524 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9525 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9526 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9527 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9528 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9529 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9530 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9531 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9532 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9533 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9534 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9535 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9536 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9537 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9538 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9539 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9540 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9541 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9542 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9543 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9544 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9545 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9546 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9547 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9548 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9549 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9550 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9551 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9552 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9553 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9554 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9555 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9556 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9557 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9558 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9559 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9560 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9561 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9562 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9563 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9564 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9565 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9566 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9567 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9568 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9569 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9570 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9571 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9572 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9573 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9574 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9575 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9576 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9577 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9578 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9579 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9580 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9581 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9582 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9583 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9584 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9585 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9586 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9587 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9588 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9589 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9590 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9591 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9592 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9593 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9594 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9595 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9596 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9597 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9598 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9599 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9600 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9601 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9602 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9603 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9604 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9605 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9606 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9607 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9608 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9609 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9610 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9611 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9612 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9613 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9614 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9615 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9616 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9617 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9618 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9619 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9620 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9621 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9622 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9623 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9624 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9625 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9626 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9627 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9628 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9629 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9630 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9631 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9632 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9633 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9634 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9635 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9636 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9637 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9638 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9639 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9640 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9641 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9642 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9643 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9644 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9645 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9646 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9647 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9648 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9649 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9650 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9651 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9652 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9653 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9654 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9655 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9656 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9657 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9658 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9659 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9660 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9661 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9662 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9663 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9664 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9665 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9666 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9667 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9668 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9669 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9670 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9671 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9672 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9673 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9674 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9675 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9676 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9677 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9678 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9679 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9680 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9681 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9682 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9683 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9684 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9685 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9686 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9687 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9688 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9689 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9690 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9691 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9692 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9693 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9694 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9695 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9696 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9697 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9698 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9699 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9700 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9701 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9702 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9703 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9704 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9705 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9706 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9707 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9708 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9709 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9710 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9711 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9712 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9713 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9714 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9715 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9716 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9717 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9718 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9719 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9720 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9721 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9722 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9723 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9724 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9725 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9726 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9727 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9728 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9729 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9730 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9731 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9732 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9733 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9734 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9735 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9736 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9737 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9738 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9739 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9740 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9741 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9742 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9743 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9744 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9745 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9746 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9747 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9748 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9749 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9750 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9751 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9752 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9753 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9754 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9755 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9756 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9757 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9758 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9759 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9760 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9761 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9762 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9763 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9764 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9765 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9766 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9767 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9768 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9769 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9770 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9771 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9772 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9773 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9774 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9775 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9776 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9777 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9778 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9779 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9780 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9781 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9782 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9783 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9784 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9785 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9786 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9787 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9788 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9789 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9790 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9791 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9792 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9793 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9794 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9795 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9796 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9797 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9798 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9799 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9800 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9801 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9802 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9803 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9804 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9805 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9806 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9807 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9808 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9809 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9810 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9811 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9812 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9813 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9814 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9815 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9816 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9817 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9818 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9819 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9820 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9821 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9822 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9823 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9824 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9825 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9826 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9827 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9828 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9829 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9830 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9831 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9832 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9833 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9834 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9835 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9836 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9837 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9838 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9839 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9840 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9841 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9842 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9843 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9844 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9845 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9846 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9847 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9848 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9849 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9850 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9851 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9852 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9853 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9854 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9855 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9856 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9857 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9858 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9859 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9860 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9861 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9862 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9863 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9864 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9865 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9866 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9867 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9868 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9869 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9870 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9871 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9872 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9873 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9874 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9875 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9876 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9877 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9878 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9879 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9880 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9881 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9882 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9883 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9884 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9885 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9886 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9887 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9888 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9889 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9890 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9891 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9892 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9893 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9894 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9895 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9896 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9897 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9898 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9899 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9900 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9901 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9902 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9903 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9904 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9905 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9906 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9907 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9908 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9909 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9910 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9911 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9912 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9913 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9914 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9915 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9916 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9917 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9918 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9919 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9920 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9921 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9922 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9923 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9924 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9925 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9926 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9927 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9928 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9929 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9930 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9931 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9932 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9933 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9934 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9935 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9936 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9937 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9938 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9939 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9940 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9941 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9942 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9943 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9944 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9945 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9946 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9947 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9948 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9949 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9950 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9951 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9952 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9953 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9954 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9955 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9956 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9957 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9958 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9959 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9960 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9961 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9962 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9963 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9964 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9965 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9966 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9967 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9968 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9969 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9970 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9971 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9972 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9973 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9974 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9975 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9976 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9977 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9978 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9979 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9980 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9981 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9982 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9983 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9984 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9985 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9986 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9987 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9988 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9989 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9990 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9991 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9992 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9993 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9994 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9995 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9996 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9997 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 9998 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 9999 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10000 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10001 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10002 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10003 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10004 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10005 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10006 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10007 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10008 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10009 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10010 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10011 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10012 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10013 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10014 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10015 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10016 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10017 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10018 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10019 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10020 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10021 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10022 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10023 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10024 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10025 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10026 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10027 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10028 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10029 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10030 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10031 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10032 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10033 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10034 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10035 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10036 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10037 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10038 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10039 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10040 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10041 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10042 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10043 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10044 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10045 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10046 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10047 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10048 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10049 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10050 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10051 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10052 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10053 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10054 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10055 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10056 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10057 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10058 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10059 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10060 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10061 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10062 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10063 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10064 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10065 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10066 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10067 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10068 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10069 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10070 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10071 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10072 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10073 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10074 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10075 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10076 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10077 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10078 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10079 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10080 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10081 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10082 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10083 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10084 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10085 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10086 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10087 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10088 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10089 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10090 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10091 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10092 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10093 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10094 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10095 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10096 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10097 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10098 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10099 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10100 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10101 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10102 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10103 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10104 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10105 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10106 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10107 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10108 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10109 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10110 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10111 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10112 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10113 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10114 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10115 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10116 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10117 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10118 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10119 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10120 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10121 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10122 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10123 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10124 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10125 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10126 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10127 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10128 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10129 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10130 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10131 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10132 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10133 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10134 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10135 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10136 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10137 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10138 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10139 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10140 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10141 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10142 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10143 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10144 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10145 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10146 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10147 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10148 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10149 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10150 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10151 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10152 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10153 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10154 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10155 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10156 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10157 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10158 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10159 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10160 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10161 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10162 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10163 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10164 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10165 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10166 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10167 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10168 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10169 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10170 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10171 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10172 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10173 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10174 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10175 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10176 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10177 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10178 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10179 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10180 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10181 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10182 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10183 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10184 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10185 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10186 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10187 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10188 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10189 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10190 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10191 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10192 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10193 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10194 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10195 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10196 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10197 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10198 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10199 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10200 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10201 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10202 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10203 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10204 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10205 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10206 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10207 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10208 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10209 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10210 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10211 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10212 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10213 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10214 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10215 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10216 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10217 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10218 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10219 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10220 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10221 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10222 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10223 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10224 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10225 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10226 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10227 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10228 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10229 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10230 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10231 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10232 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10233 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10234 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10235 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10236 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10237 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10238 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10239 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10240 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10241 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10242 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10243 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10244 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10245 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10246 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10247 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10248 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10249 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10250 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10251 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10252 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10253 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10254 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10255 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10256 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10257 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10258 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10259 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10260 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10261 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10262 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10263 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10264 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10265 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10266 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10267 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10268 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10269 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10270 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10271 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10272 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10273 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10274 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10275 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10276 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10277 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10278 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10279 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10280 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10281 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10282 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10283 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10284 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10285 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10286 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10287 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10288 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10289 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10290 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10291 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10292 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10293 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10294 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10295 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10296 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10297 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10298 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10299 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10300 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10301 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10302 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10303 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10304 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10305 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10306 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10307 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10308 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10309 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10310 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10311 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10312 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10313 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10314 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10315 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10316 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10317 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10318 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10319 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10320 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10321 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10322 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10323 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10324 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10325 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10326 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10327 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10328 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10329 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10330 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10331 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10332 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10333 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10334 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10335 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10336 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10337 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10338 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10339 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10340 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10341 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10342 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10343 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10344 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10345 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10346 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10347 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10348 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10349 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10350 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10351 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10352 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10353 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10354 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10355 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10356 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10357 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10358 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10359 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10360 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10361 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10362 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10363 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10364 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10365 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10366 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10367 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10368 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10369 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10370 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10371 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10372 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10373 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10374 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10375 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10376 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10377 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10378 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10379 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10380 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10381 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10382 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10383 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10384 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10385 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10386 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10387 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10388 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10389 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10390 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10391 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10392 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10393 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10394 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10395 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10396 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10397 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10398 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10399 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10400 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10401 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10402 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10403 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10404 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10405 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10406 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10407 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10408 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10409 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10410 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10411 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10412 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10413 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10414 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10415 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10416 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10417 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10418 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10419 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10420 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10421 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10422 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10423 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10424 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10425 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10426 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10427 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10428 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10429 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10430 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10431 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10432 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10433 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10434 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10435 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10436 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10437 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10438 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10439 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10440 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10441 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10442 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10443 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10444 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10445 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10446 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10447 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10448 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10449 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10450 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10451 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10452 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10453 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10454 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10455 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10456 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10457 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10458 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10459 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10460 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10461 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10462 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10463 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10464 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10465 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10466 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10467 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10468 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10469 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10470 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10471 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10472 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10473 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10474 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10475 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10476 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10477 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10478 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10479 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10480 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10481 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10482 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10483 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10484 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10485 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10486 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10487 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10488 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10489 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10490 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10491 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10492 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10493 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10494 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10495 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10496 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10497 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10498 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10499 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10500 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10501 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10502 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10503 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10504 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10505 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10506 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10507 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10508 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10509 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10510 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10511 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10512 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10513 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10514 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10515 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10516 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10517 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10518 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10519 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10520 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10521 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10522 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10523 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10524 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10525 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10526 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10527 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10528 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10529 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10530 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10531 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10532 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10533 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10534 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10535 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10536 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10537 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10538 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10539 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10540 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10541 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10542 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10543 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10544 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10545 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10546 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10547 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10548 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10549 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10550 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10551 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10552 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10553 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10554 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10555 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10556 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10557 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10558 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10559 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10560 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10561 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10562 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10563 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10564 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10565 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10566 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10567 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10568 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10569 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10570 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10571 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10572 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10573 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10574 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10575 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10576 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10577 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10578 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10579 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10580 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10581 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10582 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10583 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10584 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10585 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10586 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10587 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10588 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10589 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10590 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10591 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10592 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10593 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10594 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10595 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10596 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10597 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10598 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10599 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10600 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10601 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10602 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10603 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10604 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10605 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10606 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10607 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10608 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10609 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10610 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10611 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10612 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10613 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10614 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10615 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10616 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10617 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10618 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10619 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10620 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10621 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10622 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10623 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10624 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10625 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10626 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10627 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10628 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10629 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10630 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10631 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10632 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10633 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10634 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10635 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10636 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10637 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10638 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10639 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10640 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10641 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10642 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10643 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10644 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10645 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10646 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10647 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10648 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10649 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10650 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10651 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10652 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10653 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10654 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10655 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10656 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10657 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10658 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10659 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10660 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10661 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10662 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10663 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10664 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10665 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10666 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10667 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10668 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10669 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10670 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10671 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10672 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10673 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10674 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10675 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10676 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10677 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10678 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10679 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10680 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10681 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10682 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10683 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10684 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10685 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10686 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10687 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10688 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10689 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10690 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10691 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10692 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10693 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10694 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10695 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10696 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10697 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10698 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10699 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10700 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10701 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10702 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10703 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10704 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10705 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10706 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10707 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10708 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10709 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10710 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10711 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10712 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10713 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10714 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10715 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10716 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10717 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10718 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10719 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10720 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10721 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10722 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10723 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10724 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10725 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10726 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10727 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10728 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10729 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10730 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10731 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10732 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10733 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10734 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10735 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10736 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10737 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10738 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10739 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10740 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10741 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10742 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10743 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10744 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10745 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10746 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10747 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10748 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10749 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10750 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10751 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10752 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10753 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10754 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10755 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10756 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10757 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10758 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10759 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10760 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10761 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10762 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10763 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10764 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10765 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10766 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10767 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10768 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10769 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10770 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10771 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10772 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10773 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10774 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10775 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10776 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10777 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10778 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10779 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10780 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10781 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10782 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10783 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10784 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10785 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10786 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10787 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10788 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10789 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10790 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10791 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10792 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10793 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10794 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10795 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10796 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10797 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10798 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10799 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10800 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10801 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10802 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10803 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10804 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10805 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10806 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10807 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10808 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10809 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10810 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10811 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10812 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10813 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10814 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10815 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10816 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10817 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10818 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10819 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10820 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10821 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10822 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10823 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10824 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10825 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10826 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10827 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10828 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10829 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10830 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10831 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10832 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10833 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10834 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10835 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10836 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10837 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10838 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10839 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10840 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10841 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10842 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10843 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10844 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10845 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10846 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10847 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10848 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10849 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10850 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10851 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10852 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10853 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10854 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10855 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10856 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10857 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10858 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10859 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10860 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10861 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10862 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10863 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10864 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10865 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10866 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10867 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10868 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10869 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10870 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10871 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10872 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10873 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10874 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10875 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10876 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10877 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10878 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10879 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10880 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10881 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10882 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10883 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10884 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10885 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10886 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10887 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10888 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10889 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10890 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10891 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10892 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10893 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10894 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10895 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10896 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10897 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10898 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10899 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10900 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10901 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10902 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10903 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10904 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10905 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10906 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10907 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10908 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10909 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10910 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10911 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10912 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10913 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10914 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10915 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10916 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10917 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10918 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10919 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10920 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10921 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10922 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10923 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10924 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10925 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10926 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10927 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10928 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10929 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10930 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10931 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10932 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10933 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10934 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10935 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10936 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10937 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10938 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10939 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10940 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10941 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10942 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10943 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10944 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10945 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10946 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10947 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10948 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10949 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10950 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10951 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10952 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10953 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10954 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10955 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10956 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10957 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10958 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10959 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10960 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10961 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10962 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10963 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10964 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10965 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10966 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10967 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10968 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10969 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10970 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10971 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10972 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10973 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10974 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10975 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10976 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10977 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10978 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10979 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10980 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10981 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10982 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10983 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10984 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10985 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10986 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10987 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10988 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10989 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10990 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10991 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10992 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10993 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10994 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10995 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10996 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10997 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 10998 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 10999 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11000 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11001 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11002 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11003 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11004 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11005 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11006 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11007 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11008 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11009 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11010 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11011 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11012 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11013 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11014 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11015 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11016 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11017 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11018 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11019 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11020 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11021 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11022 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11023 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11024 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11025 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11026 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11027 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11028 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11029 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11030 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11031 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11032 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11033 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11034 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11035 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11036 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11037 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11038 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11039 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11040 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11041 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11042 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11043 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11044 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11045 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11046 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11047 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11048 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11049 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11050 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11051 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11052 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11053 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11054 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11055 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11056 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11057 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11058 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11059 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11060 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11061 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11062 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11063 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11064 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11065 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11066 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11067 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11068 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11069 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11070 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11071 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11072 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11073 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11074 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11075 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11076 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11077 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11078 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11079 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11080 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11081 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11082 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11083 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11084 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11085 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11086 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11087 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11088 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11089 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11090 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11091 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11092 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11093 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11094 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11095 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11096 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11097 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11098 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11099 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11100 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11101 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11102 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11103 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11104 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11105 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11106 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11107 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11108 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11109 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11110 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11111 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11112 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11113 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11114 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11115 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11116 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11117 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11118 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11119 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11120 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11121 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11122 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11123 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11124 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11125 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11126 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11127 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11128 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11129 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11130 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11131 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11132 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11133 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11134 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11135 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11136 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11137 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11138 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11139 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11140 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11141 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11142 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11143 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11144 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11145 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11146 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11147 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11148 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11149 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11150 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11151 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11152 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11153 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11154 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11155 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11156 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11157 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11158 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11159 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11160 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11161 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11162 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11163 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11164 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11165 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11166 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11167 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11168 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11169 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11170 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11171 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11172 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11173 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11174 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11175 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11176 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11177 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11178 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11179 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11180 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11181 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11182 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11183 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11184 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11185 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11186 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11187 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11188 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11189 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11190 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11191 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11192 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11193 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11194 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11195 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11196 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11197 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11198 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11199 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11200 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11201 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11202 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11203 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11204 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11205 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11206 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11207 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11208 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11209 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11210 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11211 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11212 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11213 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11214 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11215 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11216 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11217 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11218 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11219 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11220 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11221 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11222 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11223 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11224 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11225 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11226 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11227 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11228 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11229 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11230 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11231 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11232 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11233 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11234 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11235 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11236 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11237 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11238 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11239 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11240 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11241 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11242 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11243 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11244 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11245 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11246 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11247 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11248 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11249 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11250 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11251 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11252 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11253 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11254 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11255 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11256 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11257 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11258 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11259 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11260 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11261 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11262 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11263 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11264 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11265 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11266 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11267 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11268 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11269 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11270 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11271 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11272 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11273 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11274 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11275 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11276 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11277 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11278 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11279 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11280 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11281 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11282 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11283 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11284 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11285 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11286 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11287 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11288 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11289 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11290 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11291 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11292 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11293 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11294 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11295 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11296 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11297 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11298 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11299 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11300 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11301 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11302 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11303 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11304 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11305 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11306 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11307 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11308 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11309 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11310 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11311 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11312 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11313 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11314 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11315 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11316 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11317 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11318 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11319 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11320 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11321 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11322 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11323 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11324 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11325 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11326 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11327 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11328 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11329 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11330 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11331 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11332 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11333 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11334 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11335 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11336 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11337 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11338 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11339 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11340 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11341 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11342 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11343 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11344 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11345 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11346 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11347 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11348 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11349 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11350 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11351 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11352 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11353 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11354 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11355 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11356 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11357 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11358 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11359 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11360 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11361 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11362 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11363 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11364 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11365 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11366 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11367 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11368 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11369 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11370 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11371 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11372 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11373 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11374 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11375 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11376 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11377 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11378 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11379 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11380 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11381 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11382 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11383 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11384 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11385 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11386 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11387 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11388 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11389 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11390 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11391 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11392 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11393 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11394 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11395 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11396 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11397 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11398 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11399 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11400 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11401 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11402 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11403 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11404 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11405 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11406 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11407 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11408 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11409 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11410 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11411 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11412 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11413 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11414 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11415 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11416 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11417 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11418 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11419 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11420 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11421 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11422 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11423 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11424 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11425 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11426 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11427 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11428 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11429 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11430 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11431 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11432 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11433 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11434 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11435 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11436 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11437 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11438 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11439 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11440 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11441 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11442 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11443 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11444 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11445 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11446 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11447 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11448 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11449 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11450 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11451 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11452 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11453 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11454 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11455 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11456 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11457 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11458 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11459 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11460 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11461 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11462 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11463 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11464 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11465 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11466 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11467 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11468 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11469 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11470 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11471 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11472 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11473 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11474 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11475 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11476 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11477 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11478 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11479 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11480 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11481 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11482 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11483 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11484 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11485 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11486 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11487 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11488 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11489 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11490 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11491 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11492 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11493 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11494 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11495 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11496 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11497 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11498 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11499 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11500 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11501 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11502 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11503 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11504 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11505 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11506 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11507 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11508 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11509 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11510 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11511 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11512 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11513 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11514 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11515 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11516 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11517 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11518 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11519 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11520 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11521 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11522 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11523 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11524 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11525 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11526 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11527 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11528 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11529 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11530 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11531 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11532 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11533 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11534 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11535 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11536 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11537 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11538 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11539 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11540 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11541 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11542 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11543 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11544 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11545 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11546 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11547 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11548 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11549 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11550 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11551 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11552 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11553 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11554 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11555 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11556 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11557 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11558 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11559 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11560 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11561 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11562 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11563 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11564 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11565 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11566 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11567 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11568 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11569 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11570 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11571 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11572 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11573 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11574 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11575 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11576 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11577 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11578 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11579 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11580 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11581 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11582 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11583 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11584 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11585 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11586 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11587 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11588 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11589 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11590 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11591 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11592 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11593 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11594 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11595 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11596 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11597 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11598 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11599 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11600 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11601 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11602 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11603 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11604 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11605 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11606 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11607 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11608 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11609 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11610 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11611 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11612 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11613 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11614 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11615 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11616 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11617 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11618 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11619 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11620 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11621 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11622 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11623 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11624 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11625 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11626 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11627 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11628 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11629 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11630 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11631 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11632 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11633 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11634 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11635 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11636 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11637 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11638 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11639 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11640 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11641 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11642 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11643 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11644 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11645 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11646 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11647 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11648 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11649 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11650 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11651 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11652 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11653 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11654 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11655 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11656 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11657 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11658 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11659 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11660 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11661 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11662 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11663 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11664 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11665 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11666 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11667 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11668 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11669 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11670 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11671 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11672 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11673 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11674 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11675 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11676 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11677 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11678 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11679 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11680 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11681 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11682 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11683 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11684 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11685 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11686 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11687 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11688 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11689 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11690 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11691 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11692 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11693 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11694 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11695 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11696 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11697 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11698 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11699 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11700 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11701 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11702 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11703 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11704 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11705 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11706 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11707 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11708 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11709 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11710 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11711 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11712 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11713 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11714 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11715 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11716 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11717 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11718 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11719 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11720 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11721 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11722 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11723 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11724 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11725 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11726 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11727 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11728 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11729 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11730 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11731 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11732 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11733 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11734 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11735 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11736 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11737 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11738 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11739 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11740 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11741 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11742 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11743 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11744 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11745 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11746 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11747 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11748 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11749 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11750 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11751 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11752 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11753 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11754 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11755 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11756 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11757 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11758 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11759 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11760 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11761 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11762 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11763 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11764 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11765 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11766 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11767 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11768 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11769 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11770 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11771 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11772 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11773 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11774 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11775 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11776 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11777 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11778 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11779 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11780 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11781 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11782 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11783 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11784 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11785 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11786 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11787 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11788 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11789 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11790 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11791 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11792 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11793 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11794 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11795 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11796 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11797 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11798 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11799 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11800 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11801 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11802 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11803 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11804 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11805 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11806 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11807 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11808 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11809 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11810 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11811 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11812 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11813 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11814 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11815 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11816 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11817 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11818 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11819 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11820 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11821 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11822 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11823 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11824 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11825 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11826 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11827 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11828 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11829 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11830 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11831 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11832 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11833 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11834 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11835 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11836 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11837 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11838 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11839 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11840 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11841 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11842 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11843 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11844 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11845 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11846 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11847 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11848 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11849 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11850 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11851 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11852 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11853 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11854 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11855 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11856 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11857 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11858 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11859 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11860 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11861 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11862 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11863 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11864 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11865 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11866 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11867 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11868 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11869 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11870 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11871 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11872 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11873 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11874 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11875 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11876 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11877 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11878 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11879 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11880 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11881 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11882 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11883 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11884 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11885 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11886 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11887 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11888 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11889 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11890 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11891 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11892 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11893 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11894 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11895 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11896 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11897 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11898 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11899 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11900 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11901 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11902 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11903 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11904 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11905 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11906 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11907 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11908 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11909 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11910 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11911 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11912 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11913 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11914 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11915 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11916 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11917 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11918 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11919 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11920 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11921 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11922 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11923 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11924 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11925 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11926 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11927 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11928 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11929 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11930 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11931 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11932 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11933 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11934 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11935 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11936 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11937 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11938 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11939 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11940 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11941 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11942 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11943 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11944 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11945 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11946 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11947 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11948 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11949 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11950 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11951 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11952 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11953 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11954 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11955 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11956 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11957 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11958 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11959 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11960 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11961 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11962 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11963 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11964 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11965 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11966 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11967 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11968 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11969 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11970 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11971 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11972 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11973 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11974 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11975 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11976 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11977 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11978 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11979 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11980 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11981 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11982 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11983 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11984 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11985 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11986 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11987 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11988 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11989 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11990 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11991 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11992 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11993 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11994 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11995 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11996 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11997 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 11998 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 11999 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12000 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12001 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12002 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12003 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12004 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12005 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12006 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12007 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12008 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12009 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12010 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12011 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12012 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12013 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12014 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12015 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12016 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12017 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12018 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12019 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12020 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12021 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12022 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12023 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12024 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12025 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12026 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12027 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12028 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12029 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12030 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12031 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12032 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12033 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12034 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12035 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12036 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12037 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12038 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12039 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12040 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12041 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12042 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12043 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12044 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12045 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12046 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12047 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12048 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12049 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12050 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12051 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12052 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12053 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12054 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12055 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12056 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12057 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12058 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12059 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12060 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12061 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12062 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12063 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12064 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12065 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12066 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12067 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12068 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12069 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12070 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12071 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12072 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12073 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12074 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12075 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12076 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12077 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12078 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12079 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12080 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12081 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12082 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12083 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12084 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12085 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12086 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12087 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12088 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12089 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12090 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12091 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12092 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12093 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12094 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12095 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12096 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12097 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12098 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12099 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12100 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12101 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12102 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12103 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12104 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12105 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12106 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12107 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12108 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12109 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12110 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12111 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12112 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12113 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12114 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12115 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12116 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12117 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12118 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12119 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12120 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12121 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12122 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12123 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12124 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12125 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12126 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12127 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12128 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12129 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12130 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12131 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12132 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12133 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12134 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12135 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12136 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12137 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12138 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12139 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12140 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12141 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12142 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12143 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12144 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12145 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12146 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12147 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12148 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12149 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12150 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12151 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12152 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12153 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12154 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12155 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12156 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12157 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12158 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12159 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12160 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12161 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12162 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12163 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12164 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12165 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12166 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12167 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12168 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12169 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12170 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12171 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12172 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12173 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12174 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12175 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12176 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12177 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12178 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12179 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12180 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12181 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12182 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12183 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12184 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12185 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12186 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12187 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12188 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12189 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12190 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12191 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12192 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12193 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12194 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12195 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12196 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12197 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12198 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12199 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12200 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12201 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12202 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12203 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12204 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12205 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12206 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12207 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12208 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12209 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12210 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12211 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12212 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12213 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12214 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12215 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12216 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12217 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12218 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12219 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12220 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12221 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12222 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12223 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12224 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12225 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12226 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12227 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12228 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12229 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12230 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12231 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12232 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12233 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12234 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12235 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12236 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12237 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12238 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12239 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12240 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12241 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12242 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12243 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12244 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12245 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12246 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12247 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12248 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12249 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12250 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12251 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12252 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12253 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12254 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12255 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12256 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12257 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12258 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12259 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12260 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12261 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12262 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12263 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12264 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12265 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12266 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12267 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12268 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12269 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12270 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12271 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12272 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12273 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12274 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12275 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12276 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12277 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12278 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12279 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12280 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12281 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12282 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12283 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12284 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12285 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12286 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12287 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12288 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12289 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12290 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12291 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12292 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12293 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12294 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12295 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12296 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12297 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12298 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12299 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12300 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12301 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12302 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12303 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12304 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12305 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12306 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12307 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12308 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12309 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12310 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12311 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12312 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12313 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12314 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12315 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12316 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12317 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12318 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12319 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12320 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12321 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12322 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12323 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12324 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12325 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12326 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12327 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12328 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12329 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12330 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12331 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12332 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12333 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12334 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12335 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12336 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12337 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12338 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12339 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12340 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12341 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12342 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12343 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12344 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12345 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12346 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12347 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12348 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12349 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12350 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12351 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12352 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12353 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12354 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12355 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12356 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12357 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12358 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12359 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12360 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12361 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12362 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12363 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12364 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12365 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12366 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12367 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12368 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12369 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12370 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12371 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12372 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12373 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12374 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12375 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12376 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12377 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12378 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12379 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12380 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12381 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12382 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12383 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12384 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12385 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12386 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12387 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12388 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12389 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12390 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12391 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12392 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12393 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12394 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12395 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12396 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12397 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12398 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12399 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12400 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12401 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12402 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12403 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12404 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12405 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12406 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12407 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12408 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12409 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12410 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12411 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12412 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12413 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12414 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12415 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12416 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12417 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12418 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12419 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12420 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12421 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12422 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12423 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12424 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12425 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12426 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12427 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12428 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12429 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12430 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12431 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12432 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12433 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12434 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12435 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12436 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12437 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12438 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12439 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12440 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12441 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12442 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12443 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12444 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12445 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12446 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12447 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12448 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12449 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12450 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12451 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12452 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12453 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12454 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12455 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12456 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12457 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12458 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12459 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12460 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12461 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12462 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12463 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12464 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12465 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12466 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12467 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12468 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12469 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12470 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12471 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12472 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12473 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12474 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12475 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12476 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12477 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12478 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12479 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12480 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12481 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12482 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12483 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12484 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12485 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12486 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12487 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12488 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12489 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12490 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12491 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12492 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12493 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12494 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12495 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12496 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12497 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12498 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12499 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12500 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12501 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12502 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12503 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12504 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12505 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12506 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12507 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12508 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12509 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12510 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12511 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12512 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12513 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12514 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12515 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12516 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12517 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12518 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12519 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12520 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12521 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12522 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12523 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12524 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12525 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12526 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12527 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12528 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12529 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12530 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12531 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12532 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12533 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12534 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12535 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12536 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12537 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12538 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12539 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12540 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12541 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12542 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12543 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12544 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12545 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12546 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12547 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12548 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12549 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12550 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12551 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12552 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12553 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12554 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12555 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12556 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12557 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12558 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12559 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12560 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12561 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12562 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12563 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12564 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12565 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12566 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12567 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12568 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12569 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12570 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12571 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12572 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12573 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12574 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12575 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12576 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12577 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12578 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12579 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12580 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12581 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12582 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12583 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12584 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12585 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12586 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12587 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12588 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12589 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12590 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12591 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12592 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12593 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12594 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12595 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12596 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12597 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12598 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12599 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12600 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12601 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12602 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12603 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12604 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12605 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12606 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12607 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12608 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12609 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12610 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12611 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12612 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12613 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12614 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12615 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12616 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12617 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12618 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12619 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12620 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12621 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12622 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12623 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12624 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12625 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12626 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12627 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12628 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12629 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12630 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12631 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12632 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12633 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12634 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12635 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12636 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12637 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12638 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12639 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12640 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12641 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12642 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12643 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12644 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12645 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12646 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12647 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12648 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12649 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12650 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12651 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12652 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12653 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12654 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12655 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12656 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12657 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12658 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12659 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12660 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12661 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12662 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12663 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12664 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12665 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12666 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12667 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12668 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12669 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12670 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12671 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12672 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12673 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12674 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12675 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12676 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12677 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12678 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12679 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12680 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12681 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12682 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12683 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12684 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12685 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12686 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12687 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12688 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12689 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12690 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12691 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12692 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12693 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12694 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12695 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12696 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12697 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12698 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12699 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12700 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12701 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12702 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12703 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12704 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12705 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12706 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12707 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12708 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12709 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12710 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12711 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12712 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12713 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12714 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12715 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12716 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12717 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12718 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12719 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12720 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12721 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12722 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12723 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12724 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12725 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12726 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12727 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12728 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12729 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12730 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12731 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12732 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12733 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12734 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12735 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12736 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12737 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12738 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12739 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12740 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12741 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12742 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12743 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12744 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12745 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12746 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12747 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12748 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12749 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12750 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12751 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12752 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12753 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12754 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12755 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12756 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12757 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12758 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12759 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12760 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12761 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12762 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12763 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12764 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12765 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12766 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12767 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12768 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12769 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12770 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12771 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12772 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12773 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12774 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12775 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12776 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12777 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12778 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12779 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12780 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12781 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12782 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12783 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12784 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12785 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12786 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12787 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12788 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12789 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12790 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12791 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12792 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12793 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12794 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12795 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12796 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12797 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12798 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12799 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12800 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12801 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12802 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12803 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12804 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12805 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12806 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12807 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12808 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12809 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12810 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12811 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12812 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12813 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12814 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12815 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12816 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12817 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12818 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12819 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12820 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12821 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12822 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12823 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12824 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12825 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12826 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12827 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12828 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12829 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12830 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12831 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12832 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12833 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12834 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12835 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12836 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12837 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12838 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12839 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12840 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12841 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12842 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12843 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12844 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12845 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12846 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12847 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12848 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12849 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12850 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12851 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12852 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12853 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12854 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12855 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12856 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12857 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12858 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12859 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12860 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12861 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12862 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12863 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12864 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12865 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12866 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12867 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12868 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12869 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12870 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12871 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12872 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12873 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12874 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12875 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12876 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12877 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12878 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12879 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12880 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12881 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12882 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12883 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12884 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12885 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12886 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12887 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12888 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12889 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12890 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12891 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12892 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12893 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12894 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12895 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12896 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12897 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12898 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12899 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12900 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12901 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12902 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12903 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12904 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12905 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12906 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12907 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12908 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12909 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12910 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12911 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12912 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12913 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12914 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12915 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12916 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12917 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12918 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12919 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12920 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12921 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12922 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12923 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12924 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12925 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12926 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12927 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12928 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12929 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12930 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12931 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12932 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12933 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12934 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12935 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12936 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12937 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12938 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12939 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12940 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12941 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12942 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12943 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12944 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12945 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12946 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12947 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12948 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12949 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12950 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12951 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12952 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12953 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12954 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12955 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12956 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12957 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12958 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12959 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12960 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12961 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12962 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12963 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12964 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12965 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12966 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12967 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12968 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12969 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12970 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12971 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12972 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12973 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12974 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12975 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12976 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12977 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12978 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12979 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12980 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12981 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12982 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12983 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12984 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12985 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12986 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12987 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12988 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12989 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12990 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12991 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12992 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12993 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12994 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12995 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12996 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12997 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 12998 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 12999 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13000 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13001 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13002 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13003 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13004 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13005 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13006 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13007 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13008 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13009 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13010 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13011 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13012 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13013 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13014 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13015 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13016 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13017 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13018 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13019 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13020 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13021 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13022 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13023 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13024 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13025 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13026 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13027 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13028 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13029 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13030 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13031 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13032 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13033 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13034 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13035 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13036 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13037 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13038 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13039 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13040 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13041 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13042 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13043 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13044 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13045 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13046 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13047 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13048 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13049 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13050 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13051 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13052 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13053 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13054 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13055 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13056 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13057 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13058 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13059 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13060 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13061 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13062 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13063 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13064 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13065 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13066 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13067 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13068 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13069 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13070 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13071 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13072 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13073 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13074 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13075 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13076 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13077 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13078 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13079 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13080 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13081 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13082 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13083 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13084 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13085 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13086 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13087 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13088 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13089 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13090 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13091 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13092 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13093 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13094 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13095 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13096 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13097 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13098 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13099 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13100 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13101 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13102 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13103 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13104 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13105 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13106 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13107 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13108 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13109 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13110 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13111 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13112 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13113 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13114 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13115 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13116 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13117 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13118 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13119 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13120 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13121 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13122 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13123 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13124 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13125 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13126 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13127 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13128 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13129 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13130 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13131 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13132 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13133 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13134 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13135 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13136 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13137 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13138 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13139 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13140 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13141 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13142 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13143 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13144 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13145 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13146 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13147 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13148 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13149 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13150 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13151 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13152 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13153 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13154 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13155 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13156 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13157 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13158 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13159 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13160 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13161 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13162 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13163 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13164 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13165 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13166 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13167 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13168 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13169 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13170 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13171 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13172 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13173 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13174 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13175 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13176 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13177 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13178 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13179 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13180 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13181 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13182 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13183 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13184 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13185 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13186 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13187 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13188 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13189 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13190 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13191 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13192 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13193 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13194 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13195 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13196 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13197 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13198 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13199 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13200 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13201 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13202 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13203 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13204 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13205 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13206 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13207 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13208 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13209 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13210 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13211 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13212 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13213 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13214 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13215 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13216 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13217 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13218 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13219 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13220 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13221 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13222 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13223 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13224 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13225 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13226 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13227 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13228 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13229 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13230 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13231 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13232 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13233 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13234 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13235 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13236 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13237 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13238 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13239 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13240 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13241 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13242 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13243 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13244 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13245 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13246 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13247 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13248 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13249 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13250 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13251 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13252 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13253 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13254 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13255 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13256 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13257 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13258 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13259 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13260 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13261 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13262 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13263 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13264 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13265 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13266 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13267 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13268 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13269 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13270 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13271 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13272 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13273 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13274 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13275 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13276 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13277 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13278 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13279 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13280 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13281 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13282 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13283 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13284 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13285 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13286 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13287 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13288 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13289 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13290 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13291 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13292 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13293 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13294 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13295 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13296 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13297 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13298 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13299 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13300 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13301 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13302 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13303 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13304 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13305 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13306 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13307 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13308 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13309 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13310 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13311 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13312 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13313 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13314 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13315 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13316 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13317 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13318 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13319 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13320 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13321 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13322 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13323 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13324 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13325 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13326 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13327 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13328 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13329 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13330 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13331 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13332 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13333 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13334 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13335 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13336 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13337 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13338 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13339 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13340 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13341 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13342 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13343 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13344 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13345 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13346 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13347 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13348 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13349 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13350 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13351 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13352 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13353 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13354 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13355 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13356 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13357 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13358 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13359 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13360 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13361 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13362 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13363 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13364 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13365 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13366 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13367 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13368 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13369 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13370 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13371 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13372 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13373 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13374 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13375 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13376 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13377 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13378 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13379 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13380 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13381 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13382 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13383 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13384 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13385 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13386 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13387 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13388 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13389 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13390 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13391 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13392 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13393 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13394 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13395 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13396 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13397 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13398 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13399 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13400 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13401 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13402 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13403 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13404 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13405 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13406 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13407 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13408 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13409 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13410 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13411 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13412 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13413 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13414 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13415 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13416 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13417 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13418 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13419 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13420 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13421 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13422 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13423 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13424 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13425 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13426 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13427 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13428 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13429 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13430 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13431 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13432 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13433 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13434 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13435 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13436 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13437 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13438 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13439 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13440 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13441 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13442 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13443 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13444 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13445 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13446 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13447 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13448 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13449 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13450 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13451 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13452 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13453 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13454 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13455 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13456 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13457 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13458 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13459 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13460 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13461 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13462 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13463 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13464 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13465 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13466 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13467 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13468 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13469 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13470 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13471 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13472 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13473 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13474 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13475 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13476 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13477 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13478 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13479 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13480 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13481 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13482 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13483 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13484 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13485 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13486 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13487 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13488 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13489 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13490 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13491 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13492 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13493 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13494 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13495 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13496 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13497 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13498 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13499 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13500 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13501 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13502 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13503 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13504 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13505 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13506 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13507 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13508 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13509 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13510 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13511 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13512 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13513 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13514 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13515 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13516 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13517 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13518 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13519 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13520 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13521 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13522 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13523 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13524 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13525 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13526 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13527 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13528 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13529 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13530 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13531 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13532 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13533 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13534 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13535 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13536 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13537 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13538 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13539 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13540 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13541 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13542 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13543 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13544 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13545 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13546 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13547 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13548 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13549 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13550 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13551 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13552 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13553 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13554 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13555 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13556 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13557 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13558 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13559 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13560 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13561 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13562 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13563 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13564 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13565 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13566 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13567 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13568 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13569 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13570 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13571 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13572 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13573 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13574 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13575 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13576 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13577 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13578 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13579 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13580 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13581 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13582 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13583 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13584 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13585 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13586 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13587 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13588 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13589 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13590 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13591 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13592 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13593 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13594 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13595 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13596 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13597 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13598 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13599 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13600 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13601 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13602 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13603 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13604 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13605 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13606 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13607 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13608 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13609 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13610 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13611 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13612 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13613 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13614 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13615 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13616 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13617 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13618 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13619 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13620 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13621 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13622 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13623 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13624 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13625 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13626 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13627 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13628 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13629 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13630 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13631 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13632 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13633 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13634 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13635 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13636 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13637 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13638 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13639 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13640 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13641 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13642 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13643 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13644 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13645 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13646 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13647 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13648 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13649 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13650 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13651 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13652 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13653 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13654 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13655 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13656 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13657 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13658 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13659 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13660 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13661 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13662 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13663 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13664 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13665 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13666 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13667 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13668 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13669 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13670 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13671 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13672 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13673 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13674 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13675 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13676 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13677 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13678 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13679 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13680 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13681 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13682 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13683 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13684 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13685 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13686 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13687 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13688 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13689 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13690 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13691 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13692 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13693 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13694 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13695 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13696 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13697 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13698 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13699 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13700 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13701 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13702 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13703 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13704 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13705 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13706 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13707 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13708 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13709 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13710 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13711 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13712 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13713 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13714 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13715 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13716 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13717 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13718 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13719 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13720 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13721 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13722 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13723 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13724 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13725 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13726 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13727 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13728 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13729 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13730 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13731 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13732 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13733 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13734 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13735 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13736 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13737 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13738 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13739 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13740 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13741 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13742 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13743 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13744 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13745 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13746 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13747 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13748 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13749 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13750 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13751 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13752 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13753 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13754 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13755 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13756 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13757 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13758 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13759 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13760 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13761 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13762 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13763 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13764 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13765 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13766 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13767 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13768 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13769 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13770 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13771 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13772 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13773 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13774 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13775 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13776 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13777 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13778 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13779 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13780 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13781 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13782 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13783 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13784 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13785 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13786 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13787 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13788 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13789 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13790 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13791 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13792 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13793 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13794 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13795 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13796 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13797 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13798 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13799 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13800 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13801 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13802 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13803 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13804 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13805 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13806 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13807 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13808 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13809 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13810 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13811 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13812 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13813 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13814 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13815 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13816 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13817 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13818 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13819 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13820 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13821 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13822 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13823 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13824 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13825 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13826 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13827 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13828 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13829 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13830 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13831 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13832 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13833 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13834 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13835 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13836 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13837 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13838 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13839 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13840 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13841 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13842 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13843 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13844 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13845 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13846 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13847 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13848 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13849 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13850 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13851 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13852 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13853 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13854 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13855 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13856 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13857 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13858 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13859 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13860 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13861 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13862 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13863 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13864 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13865 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13866 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13867 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13868 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13869 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13870 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13871 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13872 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13873 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13874 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13875 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13876 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13877 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13878 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13879 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13880 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13881 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13882 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13883 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13884 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13885 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13886 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13887 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13888 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13889 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13890 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13891 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13892 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13893 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13894 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13895 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13896 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13897 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13898 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13899 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13900 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13901 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13902 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13903 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13904 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13905 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13906 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13907 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13908 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13909 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13910 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13911 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13912 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13913 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13914 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13915 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13916 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13917 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13918 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13919 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13920 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13921 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13922 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13923 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13924 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13925 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13926 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13927 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13928 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13929 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13930 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13931 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13932 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13933 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13934 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13935 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13936 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13937 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13938 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13939 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13940 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13941 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13942 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13943 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13944 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13945 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13946 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13947 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13948 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13949 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13950 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13951 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13952 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13953 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13954 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13955 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13956 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13957 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13958 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13959 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13960 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13961 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13962 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13963 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13964 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13965 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13966 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13967 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13968 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13969 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13970 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13971 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13972 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13973 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13974 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13975 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13976 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13977 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13978 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13979 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13980 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13981 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13982 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13983 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13984 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13985 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13986 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13987 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13988 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13989 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13990 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13991 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13992 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13993 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13994 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13995 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13996 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13997 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 13998 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 13999 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14000 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14001 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14002 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14003 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14004 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14005 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14006 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14007 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14008 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14009 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14010 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14011 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14012 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14013 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14014 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14015 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14016 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14017 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14018 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14019 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14020 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14021 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14022 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14023 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14024 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14025 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14026 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14027 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14028 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14029 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14030 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14031 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14032 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14033 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14034 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14035 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14036 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14037 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14038 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14039 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14040 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14041 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14042 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14043 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14044 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14045 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14046 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14047 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14048 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14049 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14050 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14051 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14052 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14053 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14054 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14055 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14056 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14057 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14058 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14059 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14060 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14061 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14062 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14063 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14064 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14065 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14066 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14067 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14068 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14069 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14070 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14071 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14072 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14073 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14074 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14075 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14076 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14077 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14078 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14079 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14080 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14081 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14082 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14083 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14084 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14085 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14086 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14087 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14088 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14089 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14090 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14091 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14092 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14093 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14094 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14095 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14096 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14097 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14098 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14099 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14100 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14101 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14102 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14103 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14104 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14105 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14106 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14107 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14108 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14109 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14110 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14111 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14112 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14113 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14114 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14115 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14116 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14117 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14118 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14119 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14120 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14121 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14122 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14123 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14124 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14125 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14126 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14127 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14128 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14129 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14130 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14131 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14132 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14133 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14134 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14135 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14136 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14137 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14138 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14139 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14140 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14141 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14142 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14143 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14144 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14145 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14146 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14147 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14148 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14149 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14150 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14151 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14152 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14153 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14154 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14155 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14156 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14157 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14158 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14159 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14160 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14161 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14162 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14163 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14164 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14165 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14166 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14167 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14168 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14169 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14170 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14171 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14172 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14173 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14174 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14175 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14176 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14177 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14178 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14179 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14180 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14181 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14182 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14183 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14184 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14185 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14186 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14187 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14188 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14189 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14190 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14191 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14192 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14193 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14194 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14195 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14196 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14197 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14198 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14199 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14200 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14201 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14202 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14203 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14204 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14205 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14206 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14207 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14208 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14209 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14210 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14211 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14212 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14213 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14214 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14215 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14216 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14217 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14218 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14219 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14220 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14221 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14222 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14223 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14224 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14225 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14226 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14227 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14228 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14229 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14230 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14231 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14232 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14233 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14234 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14235 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14236 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14237 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14238 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14239 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14240 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14241 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14242 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14243 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14244 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14245 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14246 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14247 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14248 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14249 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14250 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14251 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14252 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14253 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14254 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14255 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14256 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14257 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14258 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14259 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14260 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14261 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14262 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14263 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14264 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14265 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14266 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14267 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14268 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14269 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14270 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14271 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14272 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14273 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14274 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14275 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14276 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14277 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14278 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14279 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14280 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14281 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14282 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14283 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14284 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14285 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14286 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14287 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14288 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14289 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14290 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14291 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14292 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14293 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14294 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14295 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14296 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14297 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14298 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14299 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14300 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14301 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14302 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14303 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14304 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14305 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14306 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14307 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14308 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14309 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14310 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14311 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14312 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14313 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14314 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14315 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14316 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14317 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14318 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14319 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14320 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14321 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14322 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14323 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14324 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14325 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14326 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14327 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14328 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14329 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14330 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14331 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14332 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14333 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14334 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14335 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14336 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14337 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14338 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14339 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14340 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14341 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14342 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14343 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14344 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14345 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14346 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14347 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14348 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14349 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14350 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14351 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14352 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14353 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14354 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14355 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14356 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14357 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14358 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14359 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14360 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14361 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14362 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14363 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14364 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14365 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14366 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14367 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14368 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14369 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14370 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14371 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14372 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14373 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14374 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14375 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14376 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14377 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14378 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14379 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14380 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14381 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14382 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14383 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14384 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14385 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14386 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14387 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14388 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14389 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14390 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14391 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14392 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14393 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14394 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14395 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14396 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14397 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14398 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14399 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14400 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14401 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14402 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14403 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14404 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14405 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14406 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14407 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14408 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14409 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14410 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14411 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14412 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14413 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14414 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14415 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14416 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14417 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14418 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14419 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14420 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14421 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14422 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14423 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14424 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14425 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14426 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14427 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14428 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14429 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14430 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14431 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14432 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14433 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14434 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14435 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14436 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14437 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14438 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14439 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14440 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14441 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14442 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14443 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14444 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14445 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14446 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14447 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14448 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14449 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14450 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14451 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14452 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14453 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14454 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14455 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14456 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14457 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14458 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14459 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14460 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14461 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14462 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14463 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14464 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14465 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14466 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14467 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14468 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14469 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14470 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14471 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14472 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14473 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14474 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14475 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14476 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14477 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14478 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14479 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14480 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14481 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14482 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14483 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14484 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14485 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14486 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14487 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14488 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14489 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14490 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14491 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14492 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14493 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14494 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14495 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14496 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14497 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14498 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14499 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14500 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14501 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14502 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14503 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14504 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14505 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14506 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14507 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14508 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14509 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14510 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14511 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14512 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14513 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14514 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14515 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14516 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14517 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14518 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14519 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14520 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14521 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14522 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14523 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14524 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14525 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14526 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14527 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14528 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14529 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14530 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14531 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14532 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14533 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14534 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14535 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14536 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14537 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14538 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14539 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14540 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14541 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14542 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14543 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14544 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14545 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14546 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14547 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14548 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14549 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14550 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14551 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14552 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14553 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14554 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14555 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14556 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14557 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14558 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14559 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14560 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14561 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14562 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14563 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14564 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14565 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14566 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14567 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14568 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14569 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14570 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14571 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14572 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14573 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14574 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14575 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14576 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14577 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14578 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14579 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14580 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14581 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14582 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14583 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14584 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14585 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14586 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14587 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14588 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14589 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14590 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14591 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14592 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14593 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14594 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14595 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14596 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14597 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14598 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14599 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14600 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14601 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14602 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14603 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14604 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14605 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14606 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14607 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14608 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14609 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14610 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14611 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14612 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14613 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14614 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14615 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14616 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14617 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14618 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14619 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14620 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14621 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14622 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14623 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14624 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14625 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14626 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14627 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14628 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14629 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14630 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14631 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14632 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14633 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14634 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14635 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14636 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14637 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14638 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14639 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14640 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14641 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14642 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14643 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14644 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14645 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14646 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14647 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14648 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14649 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14650 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14651 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14652 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14653 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14654 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14655 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14656 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14657 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14658 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14659 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14660 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14661 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14662 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14663 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14664 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14665 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14666 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14667 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14668 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14669 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14670 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14671 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14672 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14673 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14674 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14675 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14676 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14677 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14678 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14679 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14680 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14681 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14682 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14683 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14684 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14685 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14686 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14687 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14688 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14689 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14690 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14691 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14692 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14693 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14694 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14695 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14696 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14697 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14698 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14699 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14700 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14701 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14702 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14703 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14704 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14705 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14706 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14707 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14708 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14709 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14710 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14711 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14712 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14713 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14714 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14715 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14716 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14717 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14718 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14719 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14720 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14721 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14722 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14723 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14724 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14725 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14726 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14727 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14728 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14729 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14730 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14731 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14732 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14733 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14734 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14735 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14736 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14737 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14738 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14739 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14740 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14741 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14742 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14743 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14744 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14745 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14746 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14747 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14748 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14749 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14750 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14751 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14752 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14753 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14754 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14755 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14756 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14757 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14758 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14759 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14760 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14761 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14762 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14763 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14764 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14765 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14766 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14767 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14768 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14769 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14770 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14771 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14772 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14773 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14774 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14775 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14776 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14777 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14778 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14779 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14780 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14781 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14782 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14783 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14784 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14785 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14786 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14787 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14788 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14789 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14790 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14791 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14792 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14793 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14794 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14795 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14796 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14797 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14798 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14799 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14800 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14801 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14802 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14803 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14804 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14805 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14806 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14807 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14808 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14809 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14810 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14811 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14812 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14813 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14814 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14815 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14816 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14817 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14818 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14819 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14820 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14821 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14822 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14823 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14824 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14825 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14826 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14827 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14828 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14829 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14830 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14831 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14832 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14833 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14834 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14835 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14836 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14837 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14838 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14839 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14840 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14841 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14842 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14843 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14844 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14845 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14846 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14847 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14848 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14849 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14850 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14851 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14852 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14853 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14854 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14855 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14856 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14857 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14858 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14859 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14860 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14861 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14862 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14863 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14864 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14865 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14866 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14867 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14868 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14869 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14870 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14871 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14872 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14873 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14874 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14875 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14876 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14877 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14878 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14879 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14880 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14881 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14882 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14883 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14884 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14885 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14886 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14887 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14888 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14889 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14890 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14891 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14892 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14893 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14894 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14895 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14896 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14897 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14898 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14899 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14900 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14901 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14902 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14903 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14904 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14905 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14906 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14907 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14908 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14909 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14910 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14911 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14912 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14913 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14914 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14915 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14916 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14917 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14918 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14919 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14920 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14921 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14922 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14923 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14924 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14925 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14926 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14927 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14928 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14929 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14930 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14931 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14932 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14933 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14934 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14935 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14936 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14937 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14938 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14939 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14940 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14941 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14942 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14943 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14944 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14945 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14946 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14947 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14948 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14949 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14950 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14951 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14952 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14953 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14954 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14955 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14956 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14957 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14958 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14959 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14960 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14961 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14962 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14963 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14964 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14965 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14966 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14967 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14968 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14969 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14970 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14971 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14972 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14973 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14974 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14975 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14976 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14977 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14978 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14979 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14980 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14981 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14982 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14983 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14984 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14985 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14986 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14987 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14988 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14989 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14990 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14991 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14992 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14993 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14994 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14995 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14996 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14997 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 14998 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n",
      "Epoch: 14999 \t|| Train Loss: 0.0002710661038820894 \t|| Test Loss: 0.0013256314502227752\n",
      "Epoch: 15000 \t|| Train Loss: 0.0008810338897543818 \t|| Test Loss: 2.9555442477380288e-05\n"
     ]
    }
   ],
   "source": [
    "train_loss,test_loss = model.fit(X=X_train,y=y_train,epochs=15000,validate=True,x_val=X_test,y_val=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ad7bd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_metrics(train_loss_array=train_loss,test_loss_array=test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9976f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
